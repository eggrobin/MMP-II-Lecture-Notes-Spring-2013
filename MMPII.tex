%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%  Please follow ISO standards.  %%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  A copy of ISO 80000-2:2009 can be found at      %
%  <http://goo.gl/fVoiF>. Keep other applicable    %
%  standards in   mind, e.g. ISO 8601:2004, etc.   %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[10pt, a4paper, twoside]{lecturenotes}
\usepackage[Mathematics]{semtex}

%%%% Shorthands.

%% Frequently-used sets.
\newcommand{\Rn}{{\R^n}}
\newcommand{\Schwartz}{{\SchwartzSpace\of{\Rn}}}
\newcommand{\TemperedDistributions}{{\contdual\SchwartzSpace\of{\Rn}}}

%% Freqently-used expressions.
\newcommand{\sqftnrm}{\frac{1}{\pa{2\Pi}^n} }
\newcommand{\ftnrm}{\frac{1}{\pa{2\Pi}^{\frac{n}{2}}} }

\newcommand{\opAdag}{\adj{\opA}}

% Non-standard definitions.
% \SignDistrib is the distribution generated by 
% \Sign.
\DeclareMathOperator{\SignDistrib}{Sgn}
% \AbsDistrib is the distribution generated by the absolute value.
\DeclareMathOperator{\AbsDistrib}{Abs}

%%%%%

%%%% Title and authors.

\title{%
\textdisplay{%
Notes from the\\Methods of Mathematical Physics~\textsc{ii} \\lectures of 2013-02-19{\slash}05-30 by\\ Prof.~Dr.~Eugene~Trubowitz%
}%
}
\author{Robin~Leroy and David~Nadlinger}
\input{Figures.tex}
\begin{document}
  \maketitle
  % The first three lectures are not covered by this document.
  \setcounter{Lecture}{3} 
  \section{\emph{True Lies}: An introduction to distribution theory}
  \begin{lecture}[date={2013-02-28}]
  In many areas of physics, the Dirac delta ``function'' is an important tool, e.g., for describing localized phenomena. While it is usually described in vague terms as ``only making sense under an integral'', physicists tend to still write identities such as
  \begin{equation*}
    \Laplacian\frac{1}{\norm{\vx}} = -4 \Pi \DiracDelta\of\vx\text{,}
  \end{equation*}
  and rely on their intuition and previous mistakes to distinguish situations in which they can safely apply this calculus from the ones where that would lead to incorrect results.

  How can we formalize this? Note that the operation of taking the integral of the Dirac delta multiplied with another function $\gj$ can be considered as a map from that function to a scalar in $\C$. Thus, we could consider
  \begin{equation*}
    \DiracDelta[\vx]\of\gj = \int\Rn \DiracDelta\of{\vx - \vy} \gj\of\vy \diffd\vy
  \end{equation*}
  to be a functional, and more specifically an element of the dual space of whatever class of functions we can allow for $\gj$. But the question is: what restrictions do we need on this space of functions if we want to find objects like the Dirac delta in its dual space?
  
  Actually, we want these functionals to be in the continuous dual of our space, that is, we want them to be functionals $U$ such that $\conv {U\of{\gj_k}}{U\of\gj}$ for all sequence of functions $\tuplespec{\gj_k}{k\in\N}$  that converge to $\gj$ in an appropriate norm\footnote{This simply means that $U$ is a continuous map from the function space endowed with its norm topology to $\R$ endowed with the standard topology.} --- we want our space to be a Banach space.
Intuition tells us that the smaller, i.e., more restricted, a space of functions is, the stronger the conditions on $\conv{\gj_k}{\gj}$ must be in order for it to be complete, and so the bigger its continuous dual can be, as we only need $\conv {U\of{\gj_k}}{U\of\gj}$ for the sequences $\tuplespec{\gj_k}{k\in\N}$ that converge in that space. Thus, we will investigate a space of functions $\gj\of\vx \in \Continf\of\Rn$ that vanish quickly as $\conv {\norm\vx} \infty$ (``converge with a vengeance''), hoping to find the Dirac delta in its continuous dual.
  \begin{definition} First, a few definitions used throughout this course:
%TODO(eggrobin): the big partial derivative is nonsemantic.
  \begin{equation*}
    \Continf\of\Rn \DefineAs \setst{\FunctionSpec\gj\Rn\C}
    {\forall \miga \in \N^n, \pdop\miga\gj \in \Cont\of\Rn}
  \end{equation*} is the space of $\emph{smooth functions}$ from $\Rn$ to $\C$, where
  \begin{alignat*}{2}
    \N &\DefineAs \set{0, 1, 2, \dotsc}\text{,}\quad
    &\miga &= \tuple{\ga_1, \dotsc, \ga_n} \in \N^n \text{ is a \emph{multi-index},} \\
    \total{\miga} &\DefineAs \ga_1 + \dotsb + \ga_n\text{,}\quad
    &\pd\vx\miga &\DefineAs \frac{\partial^{\total{\miga}}}{\pd{x_1}{\ga_1} \dotsm \pd{x_n}{\ga_n}} \quad
    \commentbox{(we use $\pdop\miga$ when the variable is implicit),}\\
    \text{and }\vx^\migb &\DefineAs x_1^{\gb_1} \dotsm x_n^{\gb_n}
    \text{ for }\vx \in \Rn\text{, } \migb \in \N^n\text{.}
      \span\omit\span\omit\span\omit
  \end{alignat*}
  \end{definition}
  
  \begin{definition}[Schwartz space]  Now we define the \emph{Schwartz space}
  \begin{equation*}
    \Schwartz \DefineAs \setst{\gj \in \Continf\of\Rn}{\forall \miga, \migb \in \N^n, \SchwartzNorm{\miga}{\migb}{\gj} < \infty}
    \text{, where }\SchwartzNorm{\miga}{\migb}{\gj} \DefineAs \sup_{\vx\in\R^n}\abs{\vx^\miga\pd\vx\migb \gj\of\vx}\text{.}
  \end{equation*}
  \end{definition}
  Note that $\gj \in \Schwartz$ implies $\lim_{\conv {\norm\vx} \infty}\pa{1+\norm{\vx}^2}^k \pd\vx\miga \gj\of\vx = 0$, as
  \begin{align*}
    \pa{1+\norm{\vx}^2}^m \pd\vx\miga \gj\of\vx 
    &= \sum{j = 0}[m] \binom{m}{j} \norm{\vx}^{2j} \pd\vx\miga \gj\of\vx \\
    &= \sum{j = 0}[m] \binom{m}{j} \pa{\sum{i=1}[n]x_i^2}^j \pd\vx\miga \gj\of\vx \\
    &= \sum{j = 0}[m] \binom{m}{j} \sum{\total{\miga} = j}\pa{\prod{i = 1}[n] x_i^{2\ga_i}} \pd\vx\miga \gj\of\vx
  \end{align*}
  and $\sup_{\vx \in \Rn}\abs{\vx^{2 \miga} \gj\of\vx}<\infty$. It immediately follows:
  \begin{lemma}
    If $\gj \in \Schwartz$, then
    \begin{equation*}
      \forall k \geq 0\quad \forall \miga \in \N^n\quad \exists C_\miga \in \R \quad \abs{\pd\vx\miga\gj\of{x}} < \frac{C_\alpha}{\pa{1 + \norm{\vx}^2}^k}\text{.}
    \end{equation*}
  \end{lemma}
  \begin{definition}[Convergence in $\Schwartz$]
    The sequence $\tuplespec{\gj_k}{k \in \N} \in \Functions\N{\pa\Schwartz}$ \emph{converges to} $\gj \in \Schwartz$ if and only if 
    \begin{equation*}
    \forall \miga, \migb \in \N^n \quad \lim_{k \rightarrow \infty}\SchwartzNorm{\miga}{\migb}{\gj_k - \gj}=0\text{.}
    \end{equation*}
    We use the notation $\conv[\Schwartz]{\gj_k}\gj$.
  \end{definition}
  \begin{definition}[Tempered distributions]
    Note that $\Schwartz$ is a complex vector space. We define that $U$ is an element of its continuous dual $\TemperedDistributions$, the space of \emph{tempered distributions}, if and only if:
    \begin{enumerate}
      \item $U$ is a linear map from $\Schwartz$ to $\C$, i.e., $U\in\Dual\SchwartzSpace\of\Rn$
      \item $U$ is continuous on $\Schwartz$, i.e. if $\conv[\Schwartz]{\gj_k}\gj$ then $\lim_{\conv k \infty}U\of{\gj_k} = U\of\gj$.
    \end{enumerate}
  \end{definition}
  Let us now consider some examples. First, define the space of locally integrable functions as follows.
  \begin{definition}[Locally integrable functions]
    \begin{equation*}
      \LspaceLoc[1]\of\Rn \DefineAs \setst{\FunctionSpec f \Rn \C}{\textstyle{\forall \vx \in \Rn, \exists \ge > 0, \int{\OpenBall \ge \vx}\abs{f\of\vy} \diffd{\vy} < \infty}}
    \end{equation*}
  \end{definition}
  Now let $f \in {\LspaceLoc[1]}\of\Rn$.
  If additionally $\sup_{\vx \in \Rn}\pa{1 + \norm{\vx}^2}^{-s}\abs{f\of{\vx}} < \infty$
  for some $s > 0$ (e.g. $f\of\vx \sum{\total\migb\leq s} c_\migb \vx^\migb$),
  then 
  \begin{equation*}
    U_f\of\gj \DefineAs \int\Rn f\of{\vx} \gj\of{\vx} \diffd{x}
  \end{equation*}
  is a tempered distribution.
  \begin{definition}[Dirac delta distribution]
    Another example is the \emph{Dirac delta distribution} mentioned above, with 
    \begin{equation*}
      \DiracDelta\of\gj \DefineAs \gj\of{0}\text{,}\quad \DiracDelta[\vx]\of{\gj} \DefineAs \gj\of{\vx}\text{.}
    \end{equation*}
  \end{definition}  
  Distributions have the nice property that we can always differentiate them.
  \begin{definition}[Derivative of a distribution]
    Let $U\in\TemperedDistributions$. Then $\pdop\miga U$ is the distribution defined
    by 
    \begin{equation*}
      \pa{\pdop\miga U}\of{\gj} \DefineAs \pa{-1}^{\total{\miga}} U\of{\pdop\miga\gj}\quad
      \text{for }\gj \in \Schwartz\text{.}
    \end{equation*}
  \end{definition}
      This is natural, as
  \begin{align*}
    U_{\pdop\miga f}\of\gj 
    = \int\Rn\pa{\pdop\miga f\of\vx} \gj\of\vx \diffd{\vx} 
    = \pa{-1}^{\total{\miga}} \int\Rn f\of\vx \partial^\miga \gj\of\vx \diffd{\vx} 
  \end{align*}
  by integrating by parts.
  Let's find the derivative of $\abs{\placeholder}$ in the distribution world:
  \begin{align*}
    \AbsDistrib\of\gj &\DefineAs \int{-\infty}[\infty] \abs{x} \gj\of{x} \diffd{x}\\
    \pa{\derivop x \AbsDistrib}\of\gj &= - \AbsDistrib  \deriv x \gj\\
    &= - \int0[\infty] x \gj\der\of{x} \diffd{x} - \int{-\infty}[0]\pa{-x}\gj\der\of{x} \\
    &= - \diff 0 \infty {x \gj\of{x}} + \int0[\infty] \gj\of{x} \diffd{x} + 
    \diff {-\infty}{0} {x \gj\of{x}} - \int{-\infty}[0] \gj\of{x} \diffd{x} \\
    &= \int{-\infty}[\infty] \Sign\of{x} \gj\of{x} \diffd{x} = U_{\Sign}\of\gj\text{,}\\
    \text{where } \Sign\of{x} &\DefineAs
	\begin{cases}
		1  & x > 0 \\
		0  & x = 0 \\
		-1 & x < 0
	\end{cases}
    \quad\text{is the sign function.}
  \end{align*}
We define $\SignDistrib\of\gj \DefineAs \int{-\infty}[\infty] \Sign\of{x} \gj\of{x} \diffd{x}$ and write $\derivop x \mathrm{Abs} = \SignDistrib$. This corresponds to $\derivop x \abs{x} = \Sign\of{x}$ in physicist-speak.

  What about $\Laplacian\frac{1}{\norm{\vx}}$? By defining the \emph{Newtonian distribution}
  \begin{align*}
    N\of\gj &\DefineAs \int{\R^3} \frac{1}{\norm{\vx}} \gj\of\vx \diffd{\vx}\text{,}
  \intertext{we get}
    \Laplacian{N}\of\gj = \sum{j = 1}[3] \pderivop[2]{x_j}N\of\gj &= \sum{j = 1}[3]  N\of{\pderiv[2]{x_j}\gj} = \dotsb = -4 \pi \gj\of{0}\text{,}
  \intertext{in other words,}
    \Laplacian N &= -4 \pi \DiracDelta\text{.}
  \end{align*}
  \end{lecture}
  \begin{lecture}*[date=2013-03-05]
  \begin{definition}[Inner product on $\Schwartz$]\LectureStartsHere
  Let $\gj,\gy\in\Schwartz$. We define:
  \begin{equation*}
  \LTwoInner \gy \gj \DefineAs \int\Rn \conj\gy\of\vx \gj\of\vx \diffd\vx \text{.}
  \end{equation*}
  \end{definition}
\section{General properties of the Fourier transform}
  \begin{definition}[Fourier Transform]
  For $\gj\in\Schwartz$, we define the \emph{Fourier transform} $\ft\gj$ of $\gj$, also written $\FT\of\gj$, as:
  \[
    \FT\of\gj\of\vk\DefineAs\ft\gj\of\vk\DefineAs
    \ftnrm\int\Rn\gj\of\vx\E^{-\I\scal{\vk}{\vx}}\diffd\vx\text.
  \]
  \end{definition}
  \end{lecture}
  \begin{lecture}*[date=2013-03-07]
  \begin{lemma}[``Your life in Fourier land depends on it.'']\LectureStartsHere
    \begin{align}
      \label{FTLinearity}
      \begin{split}      
      \ft{\gj + \gy} &= \ft{\gj} + \ft{\gy}\text{,}\\
      \ft{\gl \gj} &= \gl \ft{\gj} 
      \end{split}
      \\
      \label{FTUpperBound}
      \abs{\ft{\gj}\of\vk} &\leq \ftnrm\Lnorm[1]{\gj}<\infty\text{,} &&
      \Lnorm[1]{\gj} \DefineAs \int\Rn \abs{\gj\of\vx} \diffd \vx
      \\
      \label{FTScaling}
      \ft{\gj\of{\gl\placeholder}}\of\vk &= \frac{1}{\abs{\gl}^n}\ft{\gj}\of{\frac\vk\gl}\text{,} &&
      \gl\neq 0
      \\
      \label{FTTranslation}
      \ft{T_\vy\gj}\of\vx &= \E^{\I\scal\vk\vy}\ft\gj\of\vk\text{,} &&
      \pa{T_\vy\gj}\of\vx \DefineAs \gj\of{\vx+\vy}
      \\
      \label{FTDerivative}
      \begin{split}
      \ft{\pderiv{x_j}\gj}\of\vk &= \I k_j \ft\gj\of\vk \text{,} \\ 
      \ft{x_j \gj}\of\vk &= \I \pderiv{k_j}{\ft\gj} \of\vk
      \end{split}
      \\
      \label{FTUnitary}
      \int\Rn\ft\gj\of\vy \gy\of\vy \diffd\vy &= \int\Rn \gj\of\vy \ft\gy\of\vy \diffd\vy
    \end{align}
    \begin{proof}
      Proof of (\ref{FTUpperBound}):
      \begin{align*}
       \abs{\ft\gj\of\vk} 
        &= \abs{\ftnrm\int\Rn \gj\of\vx \E^{-\I \scal\vk\vx}\diffd\vx} \\
        &\leq \ftnrm\int\Rn \abs{\gj\of\vx 
        \E^{-\I \scal\vk\vx}}\diffd\vx  = \ftnrm
        \underbrace{\int\Rn \abs{\gj\of\vx}}_{\Lnorm[1]{\gj}}
        \underbrace{\abs{\E^{-\I\scal\vk\vx}}}_{1}\diffd\vx \\
      \end{align*}
      Proof of existence of the integral $\Lnorm[1]\gj$:
      \begin{align*}
        \int\Rn\abs{\gj\of\vx}\diffd\vx 
        &= \int\Rn\pa{1+\norm{\vx}^2}^{\frac{s}{2}}\abs{\gj\of\vx}
        \frac{\diffd\vx}{\pa{1+\norm{\vx}^2} ^ {\frac{s}{2}}}  \\
        &\leq \SchwartzNorm s\nullmi\gj \int\Rn\frac{1}{\pa{1+\norm{\vx}^2}^{\frac{s}{2}}}\diffd\vx\\
        &= \SchwartzNorm s\nullmi\gj \int0[\infty]\frac{r^{n-1}}{\pa{1+r^2}^{\frac{s}{2}}}\diffd r
        < \infty &&\text{for } s\geq n+1\text{.}
      \end{align*}
      ``(\ref{FTLinearity}) I will not do!''
      Proof of (\ref{FTScaling}):
      \begin{align*}
        \ft{\gj\of{\gl\placeholder}} 
        &= \ftnrm\int\Rn\gj\of{\gl\vx}\E^{-\I\scal\vk\vx} \diffd\vx \\
        &= \ftnrm\int\Rn\gj\of\vy\E^{-\I\scal{\frac1\gl\vk}{\vy}} \diffd\of{\frac{\vy}{\gl}}
          && \text{where } \vx=\frac1\gl\vy\text{.} \\
        &= \frac{1}{\abs\gl^n} \underbrace{
          \ftnrm\int\Rn\gj\of\vy\E^{-\I\scal{\frac\vk\gl}\vy}
          \diffd\vy}_{\ft\gj\of{\frac\vk{\gl}}}
      \end{align*}
      Proof of the second part of (\ref{FTDerivative}):
      \begin{align*}
        \ft{x_j\gj}\of\vx 
        &= \ftnrm\int\Rn x_j\gj\of{\vx}\E^{-\I\scal\vk\vx}\diffd\vx \\
        &= \ftnrm
          \int\Rn\gj\of{\vx}\I\pderivop{k_j}\E^{-\I\scal\vk\vx} \diffd\vx 
          && \text{as }\pderivop{k_j}\E^{-\I\scal\vk\vx} =
            \pderivop{k_j}\E^{-\I\sum{r=0}[n] k_j x_j}=-\I x_j \E^{-\I\scal\vk\vx}\\
        &= \I\pderivop{k_j}\ftnrm
          \int\Rn\gj\of\vx\E^{-\I\scal\vk\vx} \diffd\vx \\
        &= \I\pderiv{k_j}{\ft\gj}\of\vk
      \end{align*}
      
      \emph{The professor starts whistling some elevator music while waiting for a student to write down the proof before he can wipe the board. When the student is done, the professor notices that there still is some room left on the blackboard and starts writing the rest there, without erasing anything.}
      
      Proof of (\ref{FTUnitary}):
      \begin{align*}
        &\int\Rn\ft\gj\of\vy \gy\of\vy \diffd\vy
        = \int\Rn\ftnrm\int\Rn\gj\of\vx
          \E^{-\I\scal\vy\vx}\diffd\vx\:\gy\of\vy\diffd\vy \\
        &= \int\Rn\gj\of\vx\ftnrm\int\Rn
          \E^{-\I\scal\vx\vy}\gy\of\vy\diffd\vy\diffd\vx
          \quad \parbox{.5\linewidth}{As $\int {\R^{2n}}\abs{\gj\of\vx
          \E^{-\I\scal\vy\vx}\gy\of\vy}\diffd\vx\diffd\vy<\infty$,
          we can use Fubini's theorem.} \\
        &=\int\Rn\gj\of\vy \ft\gy\of\vy \diffd\vy
      \end{align*}
    \end{proof}
  \end{lemma}
  \begin{proposition}
The Fourier transform $\ft{\placeholder}$ is a continuous bijective linear map from the Schwartz space $\Schwartz$ to itself. $\rft{\ft{\gj}}=\gj$, where $\rft\gy\of\vx\DefineAs\pa{2\Pi}^{-n/2} \int\Rn\gy\of\vk\E^{\I\scal\vk\vx}\diffd\vx$, read ``unhat'', ``bird'', ``seagull'', or whatever you like.
    \begin{proof}
      \begin{align*}
        \gj\in\Schwartz&\Rightarrow\vx^\miga\pd\vx\migb\in\Schwartz \\
        \FT\of{\vx^\miga\pd\vx\migb\gj}\of\vk
        &=\I^{\total\miga}\pd\vk\miga\ft{\pa{\pd\vx\migb\gj}}\of\vk 
          && \commentbox{from the second equality in (\ref{FTDerivative}), applied $\total\miga$ times.} \\
        &=\I^{\total\miga}\pd\vk\miga\of{\I^{\total\migb}\vk^\migb\ft\gj}\of\vk \\
        &=\I^{\total\miga+\total\migb}\pd\vk\miga\of{\vk^\migb\ft\gj}\of\vk
      \end{align*}
      ``I should have done it the other way around. [...] Let's try it the other way around.''
      We want:
      \begin{equation*}
        \forall\miga,\migb,\sup_{\vk\in\Rn}\abs{\vk^\miga\pd\vk\migb\ft\gj\of\vk}<\infty
      \end{equation*}
      As that implies $\ft\gj\in\Schwartz$.
      \begin{align*}
        \FT\of{\pd\vx\miga\of{\vk^\migb\gj}}\of\vk
        &=\I^{\total\miga+\total\migb}\vk^\miga\pd\vk\migb\ft\gj\of\vk
        && \commentbox{by applying the first part of (\ref{FTDerivative}) $\total\miga
        $ times and the second part $\total\migb$ times.}\\
        \forall\miga,\migb,\abs{\vk^\miga\pd\vk\migb\ft\gj\of\vk}
        &\leq\ftnrm\Lnorm[1]{\pd\vx\migb\of{\vx^\migb\gj}}<\infty
        && \text{from (\ref{FTUpperBound}).}
      \end{align*}
      We now know $\ft\gj\in\Schwartz$.
      Let us prove $\rft{\ft\gj}=\gj$. ``When you do the wrong thing I'm gonna scream loudly.''
      \begin{align*}
        \rft{\ft\gj}\of\vx&=\ftnrm\int\Rn\ft\gj\of\vk\E^{\I\scal\vk\vx}\diffd\vk \\
        &=\sqftnrm\int\Rn\int\Rn\gj\of\vy\E^{-\I\scal\vk\vy}\diffd\vy\:\E^{\I\scal\vk\vx}\diffd\vk
      \end{align*}
      We don't interchange the integrals here because that would lead to ugly calculations. ``If pou paint the walls before you start building, it's not a good idea.''
      At this point, somebody suggests replacing $\sqftnrm\int\Rn \E^{\I\scal\vk{\pascal{\vx-\vy}}}\diffd\vk$ by $\DiracDelta\of{\vx-\vy}$. ``It's plausible! --- \textsc{Why}? [...] Ah I said it, so it's plausible.'' However, we would need to do some nasty calculations in order to do this. ``How are we going to do it so fast that you don't get bored, and yet in enough detail that he's convinced? Be sneaky.''
      \begin{align*}
        \rft{\ft{\gj}}
        &=\ftnrm\int\Rn\ft\gj\of\vk\E^{\I\scal\vk\vx}
        \underbrace{\lim_{\ge\downarrow 0}\E^{-\ge\frac{\norm\vk^2}{2}}}_{\mathclap{\substack{1
        \text{ written in}\\\text{some other way}}}}\diffd\vk\\
        &=\lim_{\ge\downarrow 0}\ftnrm\int\Rn\ft\gj\of\vk\E^{\I\scal\vk\vx}
        \E^{-\ge\frac{\norm\vk^2}{2}}&&\parbox{.4\linewidth}{``Don't worry.'' This is actually a one-liner 
        using Lebesgue's dominated convergence theorem.}
      \end{align*}
      \emph{The 10-minute break ends with the loud noise of a metallic pointing stick hitting the desk.}
      \begin{align*}
        \ftnrm\int\Rn\ft\gj\of\vk\E^{\I\scal\vk\vx}\E^{-\ge\frac{\norm\vk^2}{2}}\diffd\vk
        &=\ftnrm\int\Rn\ftnrm\int\Rn\gj\of\vy\E^{-\I\scal\vy\vk}\diffd\vy\:
        \E^{\I\scal\vk\vx}\E^{-\ge\frac{\norm\vk^2}{2}}\diffd\vk\\
        &=\ftnrm\int\Rn\gj\of\vy\ftnrm\int\Rn\E^{\I\scal\vk{\pascal{\vx-\vy}}}
        \E^{-\ge\frac{\norm\vk^2}{2}}\diffd\vk\diffd\vy
      \end{align*}
      Recall: ``every path leads to Rome and every Gaussian is in $\Schwartz$.'' Also, the last foot of a dactylic hexameter is always a spondee. 
      \begin{equation*}
        \ftnrm\int\Rn\E^{-\ge\frac{\norm\vk^2}{2}}\E^{\I\scal{\pascal{\vy-\vx}}\vk}\diffd\vk
        =\frac{\E^{-\frac{1}{2\ge}\norm{\vx-\vy}^2}}{\ge^{\frac{n}{2}}}
      \end{equation*}
      We therefore get:
      \begin{align*}
         \ftnrm\int\Rn\ft\gj\of\vk\E^{\I\scal\vk\vx}\E^{-\ge\frac{\norm\vk^2}{2}}\diffd\vk
         =\ftnrm\int\Rn\gj\of\vy\ftnrm\int\Rn\E^{\I\scal\vk{\pascal{\vx-\vy}}} \E^{-\ge\frac{\norm\vk^2}{2}}\diffd\vk\diffd\vy
        \span\omit\span\omit\span\omit\\
        \qquad &=\ftnrm\int\Rn\gj\of\vy\frac{\E^{-\frac{1}{2\ge}\norm{\vx-\vy}^2}}
        {\ge^{\frac{n}{2}}}\diffd\vy \\
        &=\ftnrm\int\Rn\gj\of{\vy\sqrt\ge+\vx}\frac{\E^{-\frac{1}{2\ge}\norm{\vy\sqrt\ge}^2}}
        {\ge^{\frac{n}{2}}}\diffd\of{\vy\sqrt\ge} && \text{``Epsilons everywhere!''}\\    
        &=\ftnrm\int\Rn\gj\of{\vx+\vy\sqrt\ge}\E^{-\frac{1}{2}\norm\vy^2}\diffd\vy
        && \text{``Now we can paint.''}\\
        &\overset{\ge\downarrow 0}{\rightarrow}
        \ftnrm\gj\of\vx\int\Rn\E^{-\frac{1}{2}\norm\vy^2}\diffd\vy =\gj\of\vx\text{.}
      \end{align*}
    \end{proof}
  \end{proposition}
  \section{Seeking eigenfunctions of the Fourier transform}
  ``The main ideas of many things are right here.''
  We seek to find the solutions $\tuple{\gj,\gl}$ of
  \begin{equation*}
    \ft\gj=\gl\gj \text{.}
  \end{equation*}
  We shall find all of them. ``Not one is going to get away.'' Actually, we have already got one:
  \begin{equation*}
    \ft{\E^{-\frac{1}{2}\norm\vx^2}}=\E^{-\frac{1}{2}\norm\vx^2} \text{,}
  \end{equation*}
  so we know that 1 is an eigenvalue. How do we find the others? ``Here you actually have to have an idea.''
  \paragraph{Idea}
  Find an $H\in\Endomorphisms\of\Schwartz$ such that $\commutator H {\ft{\placeholder}} = 0$. Then they have the same eigenspaces, so we just need to find the eigenfunctions of $H$ (see \emph{Finite Dimensional Quantum Mechanics}).
  
  Let $n=1$.
  \begin{equation*}
    \left.
    \begin{aligned}
      \ft{\pa{x^2\gj}}\of k &= -\derivop[2]{k}\ft\gj\of k \\
      \ft{\pa{\derivop[2]{x}\gj}}\of k &= -k^2 \ft\gj\of k
    \end{aligned}
    \right\rbrace \qquad \commentbox{From the lemma your life depends on.}
  \end{equation*}
  Subtracting the second equality from the first one above:
  \begin{equation*}
    \FT\of{\pa{-\derivop[2]{x}+x^2}\gj}\of k = \pa{-\derivop[2]{k}+k^2}\FT\gj\of k\text{.}
  \end{equation*}
  We smell a harmonic potential. ``Don't get your fingers too close, they sometimes bite.''
  
  Let $\opH \DefineAs \frac{1}{2}\pa{-\derivop[2]{x}+x^2-1}$. Then $\commutator \opH {\ft{\placeholder}}=0$. We multiplied by $\frac{1}{2}$ so that it looks even more like a harmonic potential (it is the Hamiltonian of the quantum harmonic oscillator). Why did we add $-1$? Define
  \begin{align*}
    \opA &\DefineAs \frac{1}{\sqrt 2}\pa{\derivop{x} + x}\text{,}\\
    \opAdag &\DefineAs \frac{1}{\sqrt 2}\pa{-\derivop{x} + x}\text{.}
  \end{align*}
  We then have $\opH = \opAdag\opA$. If $a$ and $b$ commute, then we have $\pa{a-b}\pa{a+b}  = a^2-b^2$, but here $x$ and $\derivop{x}$ don't quite commute, hence the $-1$.
\end{lecture}
\begin{lecture}[date=2013-03-12]
Let us look at these operators in more detail: define $\pa{\opQ f}\of x \DefineAs x f\of x \in \SchwartzSpace\of{\R}$, $\pa{\opP f}\of x \DefineAs \derivop x f\of x \in \SchwartzSpace\of\R$ for $f\in\SchwartzSpace\of\R$. We saw that $\commutator \opQ \opP = 1$. We also have:
\begin{align*}
  \LTwoInner \gj {\opP\gy} 
  &= \int\R \conj\gj\of x \derivop x \gy \of x \diffd x \\
  &= \diff {-\infty}{\infty}{\conj\gj\gy} - 
    \int\R \derivop x \gj \of x \gy\of x \diffd x \\
  &= - \LTwoInner {\opP\gj} \gy \text{,}&&\commentbox{$\diff {-\infty}{\infty}{\conj\gj\gy} = 0$  as $\gj\in\SchwartzSpace\of\R$, $\gy\in\SchwartzSpace\of\R$, and therefore $\conj\gj\gy\in\SchwartzSpace\of\R$.}
\end{align*}
so $\opP$ is skew symmetric. What is better than skew symmetric? Self adjoint is better. How do we make a skew symmetric operator self adjoint? We add an $\I$. Namely, redefine $\pa{\opP f}\of x \DefineAs \frac{1}{\I}\derivop x f\of x$, we then have $\LTwoInner \gj {\opP\gy}  = \LTwoInner {\opP\gj} \gy$. $\opQ$ is obviously self adjoint. We now have $\commutator \opQ \opP = \I \Identity$. Multiplying by some $h\in\R$, redefine $\pa{\opP f}\of x \DefineAs \frac{h}{\I}\derivop x f\of x$, we get $\commutator \opQ \opP = h \I \Identity$.

If we let $h \DefineAs \ReducedPlanck$, the conditions verified by $\opP$ and $\opQ$ are the conditions for infinite matrices $\matP$ and $\matQ$ in the ``catechism of matrix mechanics'' in \emph{Finite Dimensional Quantum Mechanics}.
``Being something a little more, and something a lot less than cockroaches, we are curious.'' What we have been doing here is not the ideal way of doing this: we have to pick a vector space, in this case $\Schwartz$, last year the space of infinite matrices. As we learned in linear algebra, we gain a deeper understanding if we do not use coordinate systems. 
``We are going to study the pure essence'' of this. We are going to replace that by an abstract group, which lives in Plato's cave, and when we need to calculate, we will look at a representation. ``The group is as close to God as you can get.''

\begin{definition}[Heisenberg group]
We define the Heisenberg group, introduced by Hermann Weyl.
\begin{equation*}
\tuple{
\HeisenbergGroup \DefineAs \setst{
\begin{pmatrix}
1 & r & t\\
0 & 1 & s\\
0 & 0 & 1\\
\end{pmatrix}
}{r,s,t\in\R}, 
\text{ordinary matrix multiplication}}
\end{equation*}
\end{definition}

You know that if you take $\E^x\E^y$ on reals numbers, you get $\E^{x+y}$, but that doesn't work for matrices. Hermann Weyl's idea is the following: we look at the identities \begin{align*}
\pa{\E^{\I t P} f}\of x &= f\of{x+t}
&&\parbox{.5\linewidth}{ as $\pa{\E^{\I t \pa{\frac{1}{\I}\derivop x}}}f\of x \overset{\text{formally}}{=} \sum{l=0}[\infty]\frac{t^l}{l!}\derivop[l] x f \of x $, the Taylor series of $f\of{x + t}$ at $x$.}\\
\E^{\I t P}\E^{\I s Q} &= \E^{\I s t} \E^{\I s Q} \E^{\I t P}
\end{align*}

Write $X \DefineAs \E^{\I Q}$ and $Y \DefineAs \E^{\I P}$, $X^r Y^s = \E^{\I r s} Y^s X^r$, $Z \DefineAs \pa{\E^\I}^t \Identity$. We will generate a group with these three elements. ``I'll think up another way of doing this; it's something worth doing twice.'' How do we preserve all the ideas, and yet get rid of all the infinite dimensional vector spaces? We study a finite dimensional analog, the finite Heisenberg group $\FiniteHeisenbergGroup n$, in which we use $\IntegersModulo{n}$ instead of $\R$. And we'll use that little group to do all sorts of experiments. No one will complain about what we do to it. 

Let us start again. ``Those of you who have done the exercises, and those of you who have the determination to go to Prof. Willwacher's --- `Willwacher.' I like that name. --- lecture, we are going to do that again, but on a concrete example.''
We will find all the conjugacy classes. ``We're going to hunt them down!'' 

``I'll see you on Thursday, and I expect there'll be fewer people.''
\end{lecture}

\section{The finite Heisenberg group}
\begin{lecture}[date=2013-03-14, official=true]
\emph{The professor whistles ``Singin' in the Rain'' while waiting for the lecture to start.}

In its soul, the finite Heisenberg group has captured the essence of calculating something in quantum mechanics. The point of abstraction is to remove all that gets in the way of communing with the thing in and of itself. 
``We're gonna go deep into Plato's cave today.''

``You write commutators to see the level of how much it doesn't commute (\emph{sic}).''

 ``Conjugacy is where it's at, in the language of the 1960s.'' This is what the entire linear algebra lecture is about. For instance, the Jordan normal form gives you a unique representative of a conjugacy class. ``You can dig down here and spend years in this hole. Or you can go up. I'll go up, I'll let you dig down.''

$\matX\DefineAs\Helt 1 0 0$, $\matY\DefineAs\Helt 0 1 0$, $\matZ\DefineAs\Helt 0 0 1$ generate the finite Heisenberg group. With $\matZ=\matX\matY\matX^{-1}\matY^{-1}$, $\matX$, $\matY$ generate it. ``Jacobi? --- no, no, no. No Jacobi identity.'' 

The free group on one element is isomorphic to $\tuple{\Z,+}$. The normal subgroup of $\FreeGroup X$ generated by $W$ is the smallest normal subgroup which contains $W$. This is well-defined, as $\FreeGroup X$ is such a normal subgroup, and the intersection of normal subgroups is a normal subgroup, so the normal subgroup generated by $W$ is
\begin{equation*}
  \IntersectionOver {
    \LongDomainSpec{
      \mathllap{N\NormalSubgroup} \mathrlap{\FreeGroup X}\\
      \mathllap{W\Subset} \mathrlap{N}
    }
  } N\text{.}
\end{equation*}
%\begin{supplemental}
Note that this is also equal to the orbit of $W$ under the action of $G$ on its subsets by conjugation, $\LeftConjugationAction g S \DefineAs g S g^{-1}$, so we can write
\begin{equation*}
\GroupPresentation{X}{W} = \QuotientGroup{\FreeGroup X}{\LeftConjugationAction G W} = \QuotientGroup{\FreeGroup X}{
  \IntersectionOver {
    \LongDomainSpec{
      \mathllap{N\NormalSubgroup} \mathrlap{\FreeGroup X}\\
      \mathllap{W\Subset} \mathrlap{N}
    }
  } N\text{.}
}
\end{equation*}
%\end{supplemental}

``Out of this trivial stuff comes something deep.''

``A faithful representation of a group is just changing variables.''
$\gr$ is a faithful complex irreducible representation of the finite group $G$ if and only if $\sum{g\in G} \abs{\Character[\gr]\of g}^2 = \Cardinality G$.

``I will see those of you who actually do come back next Tuesday.''
\end{lecture}
\begin{lecture}[date=2013-03-19, official=true]
The dual group only works for abelian groups. 
The way a bourgeois mathematician thinks of the classical phase space $\Rn\Cartesian\PontryaginDual\Rn$ is as the cotangent bundle $\Rn\Cartesian\Dual{\pa\Rn}$.
``If you would have the energy to learn ancient Greek, and you talked about the real housewives of Atlanta, it would be a waste.'' It is not sufficient to have a fancy language, you actually have to say something.

We started this lecture with the finite Fourier transform. This is a special case of what we discuss here, with $\IntegersModulo{n}$ as the abelian group $A$. This is perfect pedagogy: after only eight months, we come back to the beginning. Everything I've done so far is just that for various groups. When the group is infinite, e.g., the unit circle for Fourier series, you have to do analysis. ``[Claude] doesn't go there. I do.''
\end{lecture}
\begin{supplemental}
\subsection{\textsc{Supplemental: }\emph{\textgreek{Ἡ φανταχτερή γλῶσσα τῶν ἀστικῶν μαθηματικῶν}}\footnote{The fancy language of bourgeois mathematicians.}}
\begingroup
\newcommand{\VelocitySpace}{\LieAlgebraSymbol{g}}
\newcommand{\MomentumSpace}{\Dual\VelocitySpace}
\newcommand{\Lagrangian}{\mathscr{L}}
\newcommand{\Hamiltonian}{\mathscr{H}}
\newcommand{\eqrel}{\sim}
Assume the space $G$ of generalised coordinates of a physical system is a Lie group, i.e., a group which is also a differentiable manifold, and in which composition and inversion are smooth maps. Then if $\FunctionSpec\vq \R G$ describes the evolution of the generalised coordinates of the system with time, the generalised velocity at time $t$, $\TimeDerivative\vq\of t$, lies in the tangent space $\TangentSpace{\vq\of t}G$ --- recall that the tangent space of $g$ at $\vx$ is defined as the set of derivatives\footnote{One has to be careful here: unless $G$ is a submanifold of $\Rn$, the derivative is not well-defined. In that case however, $\TimeDerivative \vgg\of 0$ can be defined as the equivalence class of $\vgg$ in the set of curves through $\vx$ with $\vgg\of 0 = \vx$ under the relation $\vgg_1\eqrel\vgg_2 \Equivalent \pa{f\Compose\vgg_1}\der\of 0 = \pa{f\Compose\vgg_2}\der\of 0$, where $f$ is a differentiable chart of a neighbourhood of $\vx$ onto $\Rn$. It can then be shown that this is independent of $f$.} $\TimeDerivative \vgg\of 0$ at $0$ of smooth curves $\FunctionSpec\vgg\R G$ with $\vgg\of 0 = \vx$.

The ordered pair $\tuple{\vq\of t,\TimeDerivative\vq\of t}$ lies in the \emph{tangent bundle} $\TangentBundle G$, that is,
\begin{equation*}
\tuple{\vq\of t,\TimeDerivative\vq\of t} \in \TangentBundle G \DefineAs \UnionOver{\vx \in G} \set{\vx} \Cartesian \TangentSpace{\vx}G\text{.}
\end{equation*}
Informally, $\TangentBundle G$ is the disjoint union of all the tangent spaces of the manifold $G$. An element $\tuple{\vx,\vv}\in \TangentBundle G$  represents a tangent vector $\vv$ to $G$ at $\vx$. Physically, this is a state of the system, namely the one with coordinates $\vx$ and velocity $\vv$. 

As $G$ is a Lie group, composition on the left --- or right --- by any element is a diffeomorphism. In particular, for $q\in G$,
$\FunctionSpec {\gj_\vq} G G, \FunctionBody \vx {\vq^{-1} \vx}$ is a diffeomorphism, and so it induces a canonical linear isomorphism $\diffd \gj_\vq \of\vq$ between the tangent spaces $\TangentSpace{\vq}G$ and $\TangentSpace{\gj_\vq\of{\vq}}=\TangentSpace{\Identity}G$, by
\begin{equation*}
\diffd \gj_\vq \of\vq\TimeDerivative\vgg\of 0 = \diffd \gj_\vq \of{\vgg\of 0}\TimeDerivative\vgg\of 0 = \pa{\gj_\vq\Compose\vgg}\der\of 0 = \TimeDerivative\vgg_0\of 0\text{,}
\end{equation*}
where $\TimeDerivative\vgg\of{0}$ is an element of $\TangentSpace{\vq}G$, so $\vgg\of 0 = \vq$ and $\vgg_0 \DefineAs \gj_\vq\Compose\vgg$ verifies $\vgg_0\of 0 = \Identity$, therefore $\TimeDerivative\vgg_0\of0\in\TangentSpace\Identity G \DefinitionOf \VelocitySpace$. By definition, $\VelocitySpace$ is the Lie algebra $\LieAlgebra\of G$ associated with $G$.
It follows from this isomorphism that the tangent bundle of $G$ is \emph{parallelisable}, i.e.,
\begin{equation*}
\TangentBundle G = \UnionOver{\vx \in G} \set{\vx} \Cartesian \TangentSpace{\vx}G \Isomorphic \UnionOver{\vx \in G} \set{\vx} \Cartesian \TangentSpace\Identity G = G \Cartesian \TangentSpace\Identity G = G \Cartesian \VelocitySpace\text{,}
\end{equation*}
where the isomorphism is canonical.
Recall from Hamiltonian mechanics that for a Lagrangian
$\FunctionSpec \Lagrangian {\TangentBundle G \Cartesian \R}{\R}, \FunctionBody {\tuple{\vq,\vv,t}} {\Lagrangian\of{\vq,\vv,t}}$,
the Hamiltonian is defined as the Legendre transform of $\Lagrangian$,
\begin{equation*}
\Hamiltonian \DefineAs \sum i v_i \pderiv{v_i}\Lagrangian - \Lagrangian = \sum i v_i p_i - \Lagrangian\text{,}
\end{equation*}
where $p_i \DefineAs \pderiv{v_i}\Lagrangian$. The \emph{generalised momentum} $\FunctionNamedBody\vp \vv {\sum i v_i p_i}$ is therefore a linear form on the space $\VelocitySpace\owns\vv$. Using $\vq\of t$ as the evolution of the system as above, $\TimeDerivative\vq\of t \in \TangentSpace{\vq\of t}G$, and so $\vp\of t\in \Dual{\pa{\TangentSpace{\vq\of t}G}} \DefinitionOf \CotangentSpace{\vq\of t}G$, the \emph{cotangent space} of $G$ at $\vq$, and the tuples $\tuple{\vq,\vp}$ lie in the \emph{cotangent bundle} $\CotangentBundle G$,
\begin{equation*}
\CotangentBundle G \DefineAs \UnionOver{\vq\in G}\set\vq \Cartesian \CotangentSpace\vq G\text{.}
\end{equation*}
As the tangent spaces of $G$ are canonically isomorphic to each other, so are their duals, so we can write
\begin{equation*}
\CotangentBundle G = G \Cartesian \MomentumSpace\text{.}
\end{equation*}
The dual of the Lie algebra, $\MomentumSpace$, is the momentum space. The Hamiltonian is a function from the cotangent bundle (and time) to the reals, $\FunctionSpec\Hamiltonian {\CotangentBundle G \Cartesian \R = G \Cartesian \MomentumSpace \Cartesian \Reals} \R$.

The Pontryagin dual\footnote{The Pontryagin dual is named after %
\textrussian{Лев Семёнович Понтря́гин} (1908{\slash}1988).} $\PontryaginDual G$ is the set of homomorphisms from $G$ to $\UnitaryGroup\of 1$. Compare this with the vector space dual $\Dual V$, which is the set of linear maps from $V$ to the underlying field. One can see $\PontryaginDual G$ as a sort of non-linear analogue to $\Dual V$. We will see that for $G=\Rn$, we have a meaningful isomorphism between $\PontryaginDual G$ and $\MomentumSpace$. This will also give a motivation for defining the Hamiltonian on the \emph{cotangent} bundle and not on the tangent bundle, answering the question: \emph{why should the generalised momenta not lie in the same space as the velocities?}
\endgroup
\end{supplemental}

\begin{supplemental}

\subsection{\textsc{Supplemental: }Examples of representations from Raisa Galimova's exercise class}
\begingroup
\newcommand{\LeftAction}[2]{f_{#1} \of{#2}}
Let $G$ be a group acting on a measure space\footnote{Here $X$ is a set, $\FunctionSpec \gm X {\intclos 0 \infty}$ is a measure on $X$ and $\gS\Subset\PowerSet X$ is the $\mathrm{\sigma}$-algebra of $\gm$-measurable subsets.} $\tuple{X, \gS, \gm}$ by the left action $\FunctionSpec f G {\Functions X X}, \FunctionBody g {f_g}$, that is, we denote by $\LeftAction g x$ the action of $g\in G$ on $x\in X$, and $f_g \Compose f_h = f_{gh}$. Define the space of square-integrable functions on $X\text{, }
  \Lspace[2]\of X \DefineAs \setst{\gj\in\Functions \C X}{\Lnorm[2] \gj < \infty}$, where $
  \Lnorm[2] \gj \DefineAs \LTwoInner \gj \gj \text{, }
  \LTwoInner \gy \gj \DefineAs \int X \conj\gy \gj \diffd \gm
  $. Note that if $X$ is finite, and $\gm$ is the counting measure,
\begin{align*} 
  \LTwoInner\gy\gj &= \int X \conj\gy\gj \diffd \gm = \sum {x\in X} \conj\gy\of x \gj\of x \text{,}\\
  \forall\gj\in\Functions X \C \quad  \Lnorm[2]\gj &= \int X \abs{\gj}^2 \diffd \gm = \sum {x\in X} \abs{\gj}^2  < \infty \text{,}\\
\intertext{so this is consistent with the definitions from the lecture for a finite group $A$, namely $\Lspace[2]\of A = \Functions A \C$, and $\LTwoInner\gy\gj = \sum {g\in A} \conj\gy\of g \gj\of g$. If $X=\Rn$, and $\gm$ is the Lebesgue measure,}
   \LTwoInner\gy\gj &= \int X \conj\gy\gj \diffd \gm = \int \Rn \conj\gy\of \vx \gj\of\vx\diffd\vx\text{,}\\
   \Lnorm[2]\gj&=\int X \abs{\gj}^2 \diffd \gm = \int \Rn \abs{\gj\of \vx}^2 \diffd\vx \text{,}\\
\end{align*}
and again we recover the definition we gave for the inner product on $\Schwartz\StrictSubset\Lspace[2]\of\Rn$.

Now consider the representation $\FunctionSpec \gr G \GeneralLinearGroup\of{\Lspace[2]\of X}, \FunctionBody g {\gr_g}$ of $G$ on $\Lspace[2]\of X$ defined by $\pa{\gr_g\of\gj}\of x =  \gj\of{\LeftAction g x}$.
\paragraph{Fourier Series and $\UnitSphere 1$.}
If $G=\UnitSphere 1=X$ is the circle group, acting on itself by addition (here we use the definition $\UnitSphere 1 = \QuotientGroup\R {2\Pi\Z}$), $\Lspace[2]\of X = \Lspace[2]\of{\UnitSphere 1}$ decomposes under $\gr$ in a way which is dictated by Fourier series. Indeed, we have for $\gj\in \Lspace[2]\of{\UnitSphere 1}$, $j\in\Z$ --- see \emph{The Discrete Fourier Transform}, page~8, though note that the inner product was antilinear in the second argument back then, whereas it is antilinear in the first one this semester:
\begin{equation*}
  \ft\gj\of j = \frac{1}{2\Pi}\int 0[2\Pi] \gj\of \gq \E^{-\I j \gq}\diffd \gq = \int {\UnitSphere 1} \gj \E^{-\I j \placeholder}\diffd \gm = \LTwoInner {\E^{\I j \placeholder}} \gj\text,
\end{equation*}
where $\E^{\I j \placeholder}\DefineAs \FunctionBody \gq {\E^{\I j \gq}}$, $\E^{\I j \placeholder} \in \PontryaginDual{\UnitSphere 1} \Isomorphic \Z$ and where we take a normed $\gm$, i.e., $\gm\of{\UnitSphere 1} = 1$. In other words, $\ft{\placeholder}\of j =  \LTwoInner {\E^{\I j \placeholder}} \placeholder$ is the projection of $\gj$ onto the subspace $V_j\DefineAs\LinearSpan\set{\E^{\I j \placeholder}}$ of $\Lspace[2]\of{\UnitSphere 1}$. Note that the $\E^{\I j \placeholder}$ are exactly the elements of $\PontryaginDual{\UnitSphere 1}$.
Moreover, $V_j$ is $\gr$-invariant, as $\gr$ acts by translation on $\gj\in \Lspace[2]\of{\UnitSphere 1}$, namely $\pa{\gr_\ga\of\gj}\of \gq = \gj\of{\ga+\gq}$, and hence
$
\forall \gl\E^{\I j \placeholder} \in V_j,\quad
\pa{\gr_\ga\of{\gl\E^{\I j \placeholder}}} =\gl\E^{\I j \pa{\ga+\placeholder}}=\gl\E^{\I j \ga} \E^{\I j \placeholder} \in V_j$.
As it is one-dimensional, $V_j$ is irreducible. We also have, with $\LTwoInner {\E^{\I j \placeholder}}{\E^{\I k \placeholder}}= \ft{\E^{\I k \placeholder}}\of j  = \KroneckerDelta j k$, $V_j \Orthogonal V_k$ for $k\neq j$.
By Parseval's identity, $\sum {j\in\Z}\ft\gj\of j \E^{\I j \placeholder}$ converges absolutely to $\gj$ for all $\gj\in\Lspace[2]\of{\UnitSphere 1}$, so
\begin{equation*} \Lspace[2]\of{\UnitSphere 1}=\DirectSum{j\in\Z}
V_j=\DirectSum{\gc\in\PontryaginDual{\UnitSphere 1}} \LinearSpan \gc \text{.}
\end{equation*}
To summarise, the translation action $\gr$ is a unitary representation of $\UnitSphere 1$ on $\Lspace[2]\of{\UnitSphere 1}$ with respect to the inner product $\LTwoInner \placeholder\placeholder$, it decomposes in the irreducible representations $V_j$ which are the spans of individual elements $\E^{\I j \placeholder}$ of $\PontryaginDual{\UnitSphere 1}$ and $\ft{\placeholder}\of{j}$ is the projector onto $V_j$. Since $\PontryaginDual {\UnitSphere 1} \Isomorphic \Z$, and the elements of $\PontryaginDual {\UnitSphere 1}$ are the basis onto which functions are projected by the Fourier transform, it makes sense to think of the projector as $\ft{\placeholder}\of{\E^{\I j \placeholder}}$ instead, with $\ft\gj\of{\gc}\DefineAs\LTwoInner\gc\gj$ for $\gc\in\PontryaginDual {\UnitSphere 1}$.
 
\paragraph{The Fourier transform and $\R$.}
If $G=\Rn=X$ acts by addition on itself, the decomposition of $\Lspace[2]\of X = \Lspace[2]\of{\Rn}$ under $\gr$ is described by the Fourier transform. As above, we have for $\gj\in\Lspace[2]\of\Rn, \vk\in\Rn,$
\begin{equation*}
 \ft\gj\of \vk = \ftnrm\int \Rn \gj\of \vx \E^{-\I \scal\vk \vx}\diffd \vx = \int \Rn \gj \E^{-\I \scal\vk{}}\diffd \gm = \LTwoInner {\E^{\I \scal\vk{}}} \gj
\end{equation*}
by taking $\gm = \ftnrm\LebesgueMeasure n$, where $\LebesgueMeasure n$ is the Lebesgue measure on $\Rn$. Again, we see that the $\E^{\I \scal\vk{}}\DefineAs \FunctionBody \vx \E^{\I \scal\vk{\vx}}$ are exactly the elements of $\PontryaginDual\Rn\Isomorphic\Rn$, and that the $V_\vk\DefineAs\LinearSpan\set{\E^{\I \scal\vk{}}}$ are $\gr$-invariant, where $\gr$ acts by translation and are orthogonal. The projector onto $V_\vk$ is  $\ft{\placeholder}\of \vk$. As for $\gj\in\Lspace[2]\of\Rn$,
\begin{equation*} 
\rft{\ft\gj}\of\vx = \ftnrm\int\Rn\ft\gj\of\vk\E^{\I\scal\vk\vx}\diffd\vk \gj\of\vx=
\int\Rn\ft\gj\E^{\I\scal\vx{}}\diffd \PontryaginDual\gm = \gj\of\vx
\end{equation*}
for almost all $x$ --- $\PontryaginDual\gm$ is the dual measure\footnote{The dual measure $\PontryaginDual\gm$ is the measure we need to get the
  expressions to look the same when written in terms of measures. We chose $\gm$ in such
  a way that $\gm = \PontryaginDual\gm$, but if we had defined a non-unitary version, e.g.
  $\ft\gj\of \vk \DefineAs \int \Rn \gj\of x \E^{-\I \scal\vk \vx}\diffd \vx$, we would  
  need to define the inverse Fourier transform accordingly as
  $\rft\gy\of\vx\DefineAs\sqftnrm \int\Rn\gy\of\vk\E^{\I\scal\vk\vx}\diffd\vx$, i.e., for
  $\gm=\LebesgueMeasure n$, $\PontryaginDual \gm = \sqftnrm \LebesgueMeasure n$.}
---, the representation $\gr$ of $\R$ on $\Lspace[2]\of\R$ by translation decomposes in irreducible representations as
\begin{equation*}
  \Lspace[2]\of{\UnitSphere 1}=\DirectSum{\vk\in\Rn}
  V_\vk=\DirectSum{\gc\in\PontryaginDual{\Rn}} \LinearSpan \gc \text{.}
\end{equation*}
Once again, it makes sense to think of the projectors as  $\ft{\placeholder}\of{\E^{\I \scal\vk{}}}$, with $\ft\gj\of\gc \DefineAs \LTwoInner \gc \gj$ for $\gc\in\PontryaginDual\Rn$, as the isomorphism $\PontryaginDual\Rn\Isomorphic\Rn$ is not unique. Actually, the isomorphism $\FunctionBody \vk {\E^{2 \Pi \I \scal{\vk}{} }}$ is sometimes used, as it allows for $\gm = \PontryaginDual\gm = \LebesgueMeasure n$ --- there are no constant factors before the integrals. Using this isomorphism, the Gaussian $\E^{-\frac{1}{2}\norm\vx^2}$ is no longer self-dual, i.e., the same as its Fourier transform, but $\E^{-\Pi\norm\vx^2}$ is instead.

This also gives us an isomorphism between the momentum space $\CotangentSpace 0 \Rn = \Dual{\pa\Rn}$ and the Pontryagin dual $\PontryaginDual \Rn$, namely $\FunctionBody \vgl {\E^{2 \Pi \I \vgl }}$, with $\PontryaginDual \Rn\owns\FunctionNamedBody{\E^{2 \Pi \I \vgl}} \vk {\E^{2 \Pi \I \vgl\vk}}$, which, in contrast to the isomorphism from $\Rn$ to $\PontryaginDual\Rn$, does not depend on the choice of an inner product. It therefore makes sense to think of momenta as lying in the dual of the tangent space, and not in the tangent space. Thus the phase space should indeed be $\CotangentBundle \Rn$, and not $\TangentBundle \Rn$.

\paragraph{Spherical harmonics and $\SpecialOrthogonalGroup\of 3$.} 
\newcommand{\degL}{\ell}
\newcommand{\Harmonic}[1]{\mathbf{H}_{#1}}
If $G=\SpecialOrthogonalGroup\of 3$ acts by rotations on $X=\UnitSphere 2$, the representation $\gr$ decomposes as 
\begin{equation*}
\Lspace[2]\of{\UnitSphere 2} = \DirectSum{\degL=1}[\infty] \Harmonic\degL\text{,}
\end{equation*}
where the irreducible $\Harmonic\degL$ are defined as follows: $\Harmonic\degL$ is the set of all homogeneous harmonic polynomials of degree $\degL$ on $\UnitSphere 2$, i.e.,
\begin{equation*}
\Harmonic\degL = \setst{\FunctionSpec f {\UnitSphere 2} \C, \FunctionBody {\Transpose{\tuple{x,y,z}}} {f\of{x,y,z}}}{f\of{x,y,z} = \sum {\ga+\gb+\gg = \degL} c_{\ga\,\gb\,\gg} x^{\ga}y^{\gb}z^{\gg} \And \Laplacian f = 0  }\text{.}
\end{equation*}
\endgroup
\end{supplemental}
\begin{lecture}[date=2013-03-21, official=true]
\begingroup
\newcommand{\vsqrtninv}{\frac{1}{\sqrt{\vn}}}
The dual of a group is a sort of nonlinear version of the dual of a vector space.

We switched from an additive to a multiplicative notation. ``Algebraists... you have to be careful. They're sensitive.''

``Representation theory is like a bubble on Wall Street.'' We have, obviously, $\matT  \vsqrtninv = \vsqrtninv$. ``Does nobody know about Elmo?''  Now, by taking this eigenvector and applying $\matM$ over and over again, --- in a perfectly legal manner --- we get all the other eigenvectors! All the representation theory, all the modules, all the things they talk about... it's just this. ``[Claude's] entire thesis is just a super-duper version of this.'' You don't have to do anything, you just talk. This is the French influence.

``We're going to come to the point where you see the difference between [Claude] and me.'' We will need not one, but \emph{two} proofs for the Stone-Von Neumann theorem.

``Why am I not happy [about Proof \textsc{i}] whereas he is happy? [...] I'm only going to talk about mathematical reasons.'' Man cannot live on algebra alone. What happens as $n$ goes to infinity? The proof doesn't converge. He doesn't care. 

The problem is phase transitions: the symmetries change, and you cannot see that with algebra. For instance, for superconductivity, you have to have enough stuff. You cannot just have an explicit $n$. 
``Suddenly, we're going to go back to the Fourier transform.'' 

\noindent
``That's what's good about these algebraists. They're tenacious.''

``We have a new Pope now. You cannot confuse the Father, the Son and the Holy Ghost. If you do, you are going to get in trouble. If you confuse eigenvectors and eigenvalues, you're also going to get into trouble. Not the same kind of trouble though.''

``If you don't come back next week I understand.''
\endgroup
\end{lecture}
\section{All the eigenfunctions of the Fourier transform}
\begin{lecture}[date=2013-03-26]
Recall: 
\begin{align*}
\FT\of\gj\of\vk\DefineAs\ft\gj\of\vk&\DefineAs\ftnrm\int\Rn\gj\of\vx\E^{-\I\scal{\vk}{\vx}}\diffd\vx\text{,} &&\gj\in\Schwartz\text,\\
\RFT\of\gy\of\vx\DefineAs\rft\gy\of\vx&\DefineAs\ftnrm\int\Rn\gy\of\vx\E^{\I\scal{\vk}{\vx}}\diffd\vk\text{,} &&\gy\in\Schwartz\text,\\
\FT\RFT &=\RFT\FT=\Identity\text,\\
\FT{\pa{\derivop[2]x + x^2}}\of{k}&=\pa{\derivop[2]x + x^2}\FT\of\gj\of{k}\text.
\end{align*}
So we have this very nice guy here: $\derivop[2]x + x^2$, and having gone to \foreign{\textgerman{Elementarschule}}, you'd think \[\derivop[2]x + x^2 \stackrel{?}= \pa{x-\derivop x}\pa{x+\derivop x}\text{.}\] But the \emph{real} identity they should teach you at school --- life is short, the sooner you get going, the better --- is \[\pa{a-b}\pa{a+b}=a^2-b^2+\commutator a b\text{.}\] That's the difference between childhood and adulthood: $ab$ is no longer equal to $ba$, and it causes a lot of problems.
Here $\commutator {\derivop x} x = \Identity$. We now know  $ \pa{x-\derivop x}\pa{x+\derivop x} = -\derivop[2]x + x^2 -1$.
This will be around long after strings are seen as a strange cult of the 21st century.

We defined:
\begin{align*}
\opA&\DefineAs\frac{1}{\sqrt{2}}\pa{x+\derivop x}\text,\\
\opAdag&\DefineAs\frac{1}{\sqrt{2}}\pa{x-\derivop x}\text.
\end{align*}
We wrote ``dagger''. This makes sense, as;
\begin{align*}
\LTwoInner \gy{\opA\gj} &= \frac{1}{\sqrt{2}}\int\R\conj{\gy}\of x \pa{\derivop x+x}\gj\of x \diffd x\\
&= \frac{1}{\sqrt{2}}\int\R\conj{\gy}\of x \derivop x\gj\of x \diffd x +
\frac{1}{\sqrt{2}}\int\R\conj{\pa{x\gy\of x}} \gj\of x \diffd x\\
&=\int\R\conj{\pa{\frac{1}{\sqrt{2}}\pa{-\derivop x + x}\gy\of x}} \gj\of x \diffd x \\
&=\LTwoInner {\opAdag \gy}\gj\text.
\end{align*}
Write 
\begin{align*}
\opA_x&\DefineAs\frac{1}{\sqrt{2}}\pa{x+\derivop x}\text,\\
\adj{\opA_x}&\DefineAs\frac{1}{\sqrt{2}}\pa{x-\derivop x}\text{,}
\end{align*}
and similarly for $k$ instead of $x$.
Observe:
\begin{align*}
\FT\of {\opA_x \gj}\of k &= \I \opA_k \ft\gj\of k\text,\\
\FT\of {\adj{\opA_x} \gj}\of k &= \I \adj{\opA_k} \ft\gj\of k\text,\\
\opA_x\E^{-\frac{1}{2}x^2}&=\pa{\derivop x+x}\E^{-\frac{1}{2}x^2}= 0\text{.}\\
\intertext{define}
h_0\of x &\DefineAs \frac{\E^{-\frac{1}{2}x^2}}{\sqrt[4]{\Pi}}\\
\end{align*}
so that $\LTwoInner {h_0}{h_0} = 1$.
We can now perform the following induction:
\begin{align*}
\opA h_0&=0\text, \\
\ft{h_0}&= h_0\text,\\
\ft{\adj {\opA_x} h_0}\of k &=\pa{-\I}\adj{\opA_k} h_0\of k\text{ using the vital lemma,}\\
\FT\of{\pa{\adj{\opA_x}}^n h_0}\of k &= \pa{-\I} \adj{\opA_k}\FT\of{\pa{\adj \opA_x}^{n-1}h_0}\of k
= \pa{-\I}^n \pa{\adj{\opA_k}}^n h_0\of k\text.
\end{align*}
In other words, the $\pa{\opAdag}^n h_0$ --- with an $\opAdag$, not an $\opA$; ``a dagger can be the difference between life and death!'' --- are eigenfunctions of the the Fourier transform. We will need to prove that these are orthogonal and form a complete set, and we will have an orthogonal basis of $\SchwartzSpace\of\R$.
``I'll do some coherent states. I like coherent states.'' Next time we shall prove:
\begin{align*}
\commutator A {\adj A} &= \Identity \text,\\
\commutator A {\pa{\adj A}^n} &= n \pa{\adj A}^{n-1} \text.
\end{align*}
This is the only real idea in group representations: you find some commutation relations, and you generate all the eigenvectors and eigenvalues.
\end{lecture}
\begin{lecture}[date=2013-03-28]
We also define $\opH \DefineAs \opAdag \opA$, $h_n\of  x \DefineAs \pa{\opAdag}^n h_0$ for $n\geq 1$. We have $h_n\in \SchwartzSpace\of\R$ for $n\geq 0$. $\opA$, $\opAdag$ and $\opH$ all map $\SchwartzSpace\of\R$ to itself.
Now we compute:
\begin{align*}
2\commutator \opA \opAdag \gj &= 2 \opA\opAdag\gj- 2 \opAdag\opA\gj\\
&=\pa{\derivop x + x}\pa{-\derivop x + x}\gj - \pa{-\derivop x + x}\pa{\derivop x + x}\gj\\
&=\pa{\derivop x + x}\pa{\gj\der + x\gj} - \pa{-\derivop x + x}\pa{\gj\der + x\gj}\\
&= \pa{\gj\dder + \pa{x\gj}\der + \pa{-x\gj\der}+x^2\gj}
 - \pa{\gj\dder - \pa{x\gj}\der + x\gj\der+x^2\gj}\\
 &= 2\pa{x\gj}\der - 2 x\gj\der = 2\gj
\end{align*}
This yields the following proposition.
\begin{proposition}
\begin{align}
\commutator \opA \opAdag &= \Identity \\
\label{comm2}
\commutator \opA {\pa{\opAdag}^n} &= n\pa{\opAdag}^{n-1}\text, &&n\geq 1\text,\\
\ft{h_n}&=\pa{-\I}^n h_n\text,&&n\geq 0\text,\\
\label{orth4}
\LTwoInner {h_m}{h_n} &= \KroneckerDelta m n\text,\\
\label{creation5}
\opA h_n &= \sqrt n h_{n-1}\text,\\
\opAdag h_n &= \sqrt{n+1} h_{n+1} \text, \\
\label{Hh07}
\opH h_n &= n h_n\text, && n\geq 0\text,\\
h_n&\in\SchwartzSpace\of\R\text, && n\geq 0\text,\\
\ft{\opA_x \gj}&= \I \opA_k \ft\gj\text,\\
\ft{\adj{\opA_x} \gj}&= \pa{-\I} \adj{\opA_k} \ft\gj\text.
\end{align}
\begin{proof}
Proof of (\ref{comm2}) by induction:
\begin{align*}
  \commutator \opA \opAdag &= \Identity\text, && (n=1)\text,\\
  \commutator \opA {\pa{\opAdag}^{n+1}} &= \opA \pa{\opAdag}^{n+1} - \pa{\opAdag}^{n+1} \opA\\
  &= \opA \pa{\opAdag}^{n+1} - \opAdag \pa{\opA \pa{\opAdag}^n - n \pa{\opAdag}^{n-1}} \\
  &= \opA \pa{\opAdag}^{n+1} - \opAdag \opA \pa{\opAdag}^n + n \pa{\opAdag}^n
\end{align*}
  ``Ah, there must be an easier way to show this, it can't be that hard. Let's just reboot this argument:''
\begin{alignat*}{2}
&& \opA\pa{\opAdag}^n - \pa{\opAdag}^n\opA &= n \pa{\opAdag}^{n-1}\\
\Implies&& \opA\pa{\opAdag}^{n+1} - \pa{\opAdag}^n \opA \opAdag &= n \pa{\opAdag}^n \\
\Implies&& \opA \pa{\opAdag}^{n+1} - \pa{\opAdag}^n \pa{\Identity + \opAdag\opA}&= n \pa{\opAdag}^n \\
\Implies&& \opA \pa{\opAdag}^{n+1} - \pa{\opAdag}^{n+1} \opA - \pa{\opAdag}^{n} &= n \pa{\opAdag}^n \text.
\end{alignat*}
Proof of (\ref{orth4}). Now we are going to do something quickly, where algebra really shines: you do nothing but apply the same basic identities over and over, yet get something useful out. Let us assume $m>n\geq 0$, because we already did the case where both are $0$.
\begin{align*}
\LTwoInner{h_m}{h_n} &= \frac{1}{\sqrt{\Factorial m}} \frac{1}{\sqrt{\Factorial n}} \LTwoInner{\pa{\opAdag}^{m} h_0}{\pa{\opAdag}^{n} h_0} \\
&= \frac{1}{\sqrt{\Factorial m}} \frac{1}{\sqrt{\Factorial n}}\LTwoInner{\pa{\opAdag}^{m-1} h_0}{\opA\pa{\opAdag}^{n} h_0}\\
&= \frac{1}{\sqrt{\Factorial m}} \frac{1}{\sqrt{\Factorial n}}\LTwoInner{\pa{\opAdag}^{m-1} h_0}{\pa{\pa{\opAdag}^{n}\opA + n\pa{\opAdag}^{n-1}} h_0 }\\
&= \frac{1}{\sqrt{\Factorial m}} \frac{n}{\sqrt{\Factorial n}} 
  \LTwoInner{\pa{\opAdag}^{m-1} h_0}{\pa{\opAdag}^{n-1} h_0}\\
&= \frac{1}{\sqrt{\Factorial m}} 
  \frac{n\pa{n-1}}{\sqrt{\Factorial n}} 
  \LTwoInner{\pa{\opAdag}^{m-2} h_0}{\pa{\opAdag}^{n-2} h_0}\\
  &= \dotsb = \frac{1}{\sqrt{\Factorial m}} 
  \frac{n\pa{n-1}\dotsm 1}{\sqrt{\Factorial n}} 
  \LTwoInner{\pa{\opAdag}^{m-n} h_0}{h_0}\text.
\end{align*}
As $\opAdag$ kills $h_0$ --- it is called the \emph{annihilation} operator, in German the \foreign{\textgerman{Vernichtungsoperator}} ---, we get $
  \LTwoInner{h_m}{h_n} =
  \frac{1}{\sqrt{\Factorial m}} 
  \frac{n\pa{n-1}\dotsm 1}{\sqrt{\Factorial n}} 
  \LTwoInner{0}{h_0}=0$ if $m>n$. If $m=n$, we have $
  \LTwoInner{h_n}{h_n} =
  \frac{1}{\sqrt{\Factorial n}} 
  \frac{n\pa{n-1}\dotsm 1}{\sqrt{\Factorial n}} 
  \LTwoInner{h_0}{h_0}=1$.  
  In other words, $\set{h_n}$ is an orthogonal family.

  Proof of (\ref{creation5}):
  \begin{align*}
  \opA h_n &= \frac{1}{\sqrt{\Factorial n}} \opA\pa{\opAdag}^n h_0 \\
  &= \frac{1}{\sqrt{\Factorial n}}\pa{\pa{\opAdag}^n\opA + n \pa{\opAdag}^{n-1}}h_0\\
  &= \frac{n}{\sqrt{\Factorial n}} \pa{\opAdag}^{n-1} h_0 = \sqrt{n} h_{n-1}\text.
  \end{align*}
  The other ones are even easier; I'm not doing them, you are getting bored.

  Now for something fun, the proof of (\ref{Hh07}). First, note that
  \begin{align*}
    \opH h_0 &= \opAdag \opA h_0 = 0\text.
  \end{align*}
  With what we already have, we can complete the proof elegantly --- but then, I have always maintained that elegance is for tailors:
 \begin{align*}
   \opH h_n &= \opAdag \pa{\opA h_n} = \opAdag \sqrt{n} h_{n-1} = \sqrt{n} \sqrt{n} h_n = n h_n\text.
 \end{align*}
\end{proof}
\end{proposition}
Now, ladies and gentlemen, your jaws should be dropping. We have these beautiful identities:
\begin{align*}
  \ft h_n &= \pa{-\I}^n h_n \text, && n \geq 0 \text,\\
  \opH h_n &= n\, h_n\text, && n \geq 0 \text.
\end{align*}
So, ``out of nowhere'' we now have eigenvectors with eigenvalues 0, 1, 2, etc.!

``Whenever I think of algebra, I think about Lawrence of Arabia. It's clean.'' \emph{The professor asks if anyone in the audience has actually heard about T. E. Lawrence, and goes on to discuss how the David Lean film features a scene widely known in the film world as an example for the jump cut technique.} ``If you are really interested, I can just tell you about film and skip this [points to the blackboard] boring crap. I know something about it.'' \emph{At this point the lecture has extended five minutes into the break, and people are beginning to leave.}
``You should leave the moment you feel lied to. That's a basic thing in all human relationships.''

Now, what would make this even better? Let us show that we are done, that we have all the eigenvectors --- in other words, that our orthogonal family is a basis. ``That, is, you can't get away from it. You can run, but you can't get away.''

\begin{proposition}
If $\gj\in\SchwartzSpace\of\R$ and $\LTwoInner \gj {h_n} = 0$, $n\geq 0$, then $\gj\Identically 0$.
\begin{proof}
We have $x=\frac{\opA + \opAdag}{\sqrt{2}}$.
We have:
\begin{align*}
\LTwoInner \gj {x h_0} 
&= \frac{1}{\sqrt{2}}\LTwoInner \gj {\opA h_0} + \frac{1}{\sqrt 2} \LTwoInner \gj {\opAdag h_0}\\
&=\frac{1}{\sqrt{2}}\LTwoInner \gj {h_1} = 0 \text.
\end{align*}
What do we do? We only need one word: Repeat.
\begin{align*}
\LTwoInner \gj {x^2 h_0} 
&= \frac{1}{2}\LTwoInner \gj {\pa{\opA+\opAdag}^2 h_0} \\
&= \frac{1}{2}\LTwoInner \gj {\pa{\opA \opA + \opAdag \opA + \opA  \opAdag + \opAdag \opAdag}h_0}\\
&= \frac{1}{2}\LTwoInner \gj {\opA \opAdag h_0 + \opAdag \opAdag h_0 }\\
&= \frac{1}{2}\LTwoInner \gj {\pa{\opAdag \opA +\Identity} h_0 + \opAdag h_1}\\
&= \frac{1}{2} \LTwoInner \gj {h_0 + \sqrt{2} h_2} = 0\text.
\end{align*}
And now? Again: Repeat.
\begin{align*}
\LTwoInner \gj {x^3 h_0} 
&= \pa{\frac{1}{\sqrt 2}}^3 \LTwoInner \gj {\pa{\opA+\opAdag}^3 h_0} \\
&= \dotsb = 0\text.
\end{align*}
By induction, we can show $\LTwoInner \gj {x^n h_0} = 0 $ for $n\geq0$. It follows 
\begin{align*}
\sum{n=0}[\infty] \frac{\pa{-\I k}^n}{\Factorial n} \LTwoInner\gj {x^n h_0} &=0\\
\Implies \LTwoInner \gj {\pa{\sum{n=0}[\infty]\frac{\pa{-\I k}^n}{\Factorial n}}h_0} &=0\text,&& \forall k\in\R\text.
\end{align*}
which, if we look carefully, we can recognize as a Fourier transform:
\begin{align*}
0 &= \LTwoInner\gj {\E^{-\I k x}h_0}\\
  &= \sqrt{2\Pi}\frac{1}{\sqrt{2\Pi}} \int\R \conj \gj \of x h_0\of x \E^{-\I k x}\diffd x\text, && \forall k\in\R\text.
\end{align*}
Note that $\conj \gj \of x h_0\of x\in \SchwartzSpace\of\R$. It follows:
\begin{alignat*}{3}
         & \forall k\in \R \quad & \ft{\conj \gj  h_0}\of k &= 0\\
\Implies & \forall x\in \R \quad &\conj \gj\of x  h_0\of x &= 0 && \quad \text{(as shown previously)}\\
\Implies & \forall x\in \R \quad & \gj \of x &=0 \text. && \quad \text{($h_0(x) \neq 0\text, \forall x\in\R$)}
\end{alignat*}
So $\gj\Identically 0$.
\end{proof}
\end{proposition}

We now know that the $h_n$ form a basis of $\SchwartzSpace\of\R$. But we are greedy. We really want to write
\[
\gy\in\SchwartzSpace\of\R \stackrel{?}{=} \sum{n=0}[\infty] \LTwoInner\gy {h_n} h_n\of x \DefineAs \lim_{\conv N \infty} \sum{n=0}[N] \LTwoInner\gy {h_n} h_n\of x\text.
\]
It is exceedingly boring to prove this, I'm sure you don't have the patience. But there is something I \emph{can} do:
\begin{align*}
\abs{\LTwoInner \gy {h_n}}
&= \abs{\int\R \conj\gy\of x h_n\of x \diffd x}\\
&\leq \int\R\abs{\conj\gy\of x} \abs{h_n\of x}\diffd x\\
&= \int\R\abs{\conj\gy\of x} h_n\of x \diffd x\\
&\leq \Lnorm[2]{\gy} \underbrace{\Lnorm[2]{h_n}}_{= 1}\text.
\end{align*}
But in fact, we can do better:
\begin{align*}
\LTwoInner\gy{h_n}
&= \frac{1}{n^p}\LTwoInner \gy {\opH^p h_n}\\
&= \frac{1}{n^p}\LTwoInner {\opH^p \gy} {h_n}\text.\\
\end{align*}
So,
\begin{align*}
\abs{\LTwoInner \gy {h_n}}
&\leq \frac{1}{n^p} \abs{\LTwoInner \gy {\opH^p h_n}}\\
&\leq \frac{1}{n^p} \Lnorm[2]{\opH^p \gy}\text.
\end{align*}

Concluding, we have the following
\begin{lemma}
  If $\gy \in \SchwartzSpace\of\R$, then $\abs{\LTwoInner \gy {h_n}} \leq \frac{\Lnorm[2]{\opH^p \gy}}{n^p}$, $n \geq 1$, $p \geq 0$.
\end{lemma}
\end{lecture}
\begin{lecture}*[date=2013-04-09]
\begin{proposition}\LectureStartsHere The $h_n\of x$, $n\geq 0$ form an orthonormal basis of $\SchwartzSpace\of\R$. More precisely, let \[\SchwartzSpace\of\N \DefineAs \setst{\tuple{s_0,s_1,s_2,\dotsc}}{s_j \in \C \text{ and } \forall p, \exists c_p, \abs{s_j} \leq c_p \frac{1}{\pa{1 + j}^p}}\text.\]
Then,
\begin{align*}
    \MapSpecBody
    {\SchwartzSpace\of \N}
    {\SchwartzSpace\of\R}
    {\vs}
    {\sum{n=0}[\infty] s_n h_n\of x}
\end{align*}
is a continuous, bijective linear map with a continuous inverse.
\end{proposition}
``Whatever somebody means by an orthonormal basis for an infinite dimensional vetor space, this is about as good as it gets.'' In proving the theorem, which is an exercise, we will have to do something like \[ \abs{\sum{n \geq 0} s_n h_n\of x} \leq \sum{n \geq 0} \abs{s_n} \abs{h_n\of x} \] --- and remember, them macho wait as long as possible before bringing the $\abs{\placeholder}$ in. Compare this to what we had for Fourier series:
\[
  \abs{\sum{j=-\infty}[\infty] \ft{\gj}\of j \E^{\I j x}} \leq \sum{j=-\infty}[\infty] \abs{\ft{\gj}\of j} \underbrace{\abs{\E^{\I j x}}}_{=1}
\]
For the $h_n$, we are not so lucky, and actually have to estimate $sup_{x\in \R} \abs{h_n\of x}$.
\begin{align*}
\abs{h_n\of x} &= \abs{\pa{-\I}^{-n} \ft{h_n}\of x} \\
           &\leq \frac{1}{\sqrt{2\Pi}}\int{-\infty}[\infty] 1 \abs{h_n\of y}\diffd  y \\
           &= \frac{1}{\sqrt{2\Pi}}\int{-\infty}[\infty] \frac{1}{\sqrt{1+y^2}} \sqrt{1+y^2} \abs{h_n\of y}\diffd  y \\
           &\leq \frac{1}{\sqrt{2\Pi}}\pa{\int{-\infty}[\infty]{\frac{1}{1+y^2}\diffd y}}^{\frac{1}{2}}\pa{\int{-\infty}[\infty]{\pa{1+y^2}h_n^2\of y\diffd y}}^{\frac{1}{2}} \\
           &= \frac{1}{\sqrt{2\Pi}}\pa{\int{-\infty}[\infty]{\frac{1}{1+y^2}\diffd y}}^{\frac{1}{2}}\pa{\LTwoInner{h_n}{\pa{1+y^2}h_n}}^{\frac{1}{2}} \\
           &=\frac{1}{\sqrt{2\Pi}}\pa{\int{-\infty}[\infty]{\frac{1}{1+y^2}\diffd y}}^{\frac{1}{2}}\pa{\LTwoInner{h_n}{h_n} + \frac{1}{2} \LTwoInner{h_n}{\pa{\opA+\opAdag}^2 h_n}}^{\frac{1}{2}}\\
           &\leq \frac{1}{\sqrt{2\Pi}}\underbrace{\pa{\int{-\infty}[\infty]{\frac{1}{1+y^2}\diffd y}}^{\frac{1}{2}}}_{\mathclap{< \infty \text{, indepedent of }n}} \sqrt{2\pa{n+1}}
\end{align*}
\section{Quantising the harmonic oscillator}
``Now, we come into the good stuff''. Once upon a time, there was a classical Hamiltonian:
\[
  h\of{q,p} \DefineAs \frac{1}{2} \pa{\frac{p^2}{m}+m\gw^2 q^2}
\]
Do you remember anything from classical mechanics?
``So, Thursday: from the classical to the quantum world and beyond.''
\end{lecture}
\subsection{Classical Mechanics}
\begin{lecture}[date=2013-04-11]
What's classical mechanics? We start with a number $n\geq 1$, the number of \emph{degrees of freedom}. It does not change while you keep discussing that system. Then, we have a set $\gW \Subset \Rn$, open and connected, called the \emph{classical configuration space}.
For instance, we can consider a pendulum, with $q\in\gW = \R$ the angle. Or we could have a joint in the pendulum, yielding $\tuple{q_1,q_2}\in\gW=\R^2$.
\marginfig[The configuration space $\gW=\R^2$ of a double pendulum, where $\vq=\tuple{q_1,q_2}\in\R^2$.]{\FigureSimplePendulumConfigurationSpace}
\marginpar{\vspace*{.5cm}}
\marginfig[The configuration space $\gW=\R$ of a simple pendulum. Here $q\in\R$, as the pendulum can do a full turn --- or several --- around its pivot.]{\FigureDoublePendulumConfigurationSpace}
\begin{definition}[Classical phase space] The \emph{classical phase space} is defined as $\gW \Cartesian \Rn$, where $\tuple{\vq,\vp}\in\gW \Cartesian \Rn$.
\end{definition}
``If you don't measure anything, then you're doing string theory, and your argument is: there must be supersymmetries, because it's so beautiful.'' 
\begin{definition}[Classical observables] The set of \emph{classical observables} is the algebra of smooth, real valued functions on $\gW\Cartesian\Rn$, namely $\Continf\of{\gW\Cartesian\Rn, \R} \text.$
\end{definition}
An example of a classical observable is
\begin{align*}
q_i\in\Continf\of{\gW\Cartesian\Rn, \R}\\
\FunctionBody{\gW\Cartesian\Rn\owns\tuple{\vq,\vp}}{q_i\in\R}\text.
\end{align*}
Somebody complained about complex-valued functions not being real enough, so we use real-valued functions.
``You could ask me: why smooth? --- Why smooth?'' You can't measure something that isn't regular. ``No human being can work with something other than a polynomial anyway.''
\begin{definition}[Poisson bracket]
Finally, define the \emph{Poisson bracket} of $\gy\text{, }\gj$ as
\[
\Poisson{\gy}{\gj}\of{\vq,\vp} \DefineAs \sum{j=1}[n]{\pderiv {q_j}\gy \pderiv {p_j} \gj-\pderiv {p_j}\gy \pderiv {q_j}\gj}
\text,
\]
for $\gy\text{, }\gj\in\Continf\of{\gW\Cartesian\Rn}$.
\end{definition}

\begin{proposition}[\foreign{\textgerman{Ur}}-something of classical mechanics]
$\tuple{\Continf\of{\gW,\Rn},\Poisson{\placeholder}{\placeholder}}$ is a Lie algebra,
\foreign{\textgerman{das heißt, }} $\Poisson{\placeholder}{\placeholder}$ is bilinear, skew-symmetric and satisfies the Jacobi identity.
\end{proposition}
\begin{definition}[Classical mechanical system]
A \emph{classical mechanical system} on $\gW \Cartesian \Rn$ is a (Hamiltonian) total energy function \[h\of{\vq,\vp}\in \Continf\of{\gW\Cartesian\Rn}\text.\]
\end{definition}
\begin{definition}[Evolution of a classical observable] $\derivop t f = \Poisson f h$  determines the evolution $\FunctionSpec f \R {\Continf\of{\gW\Cartesian\Rn}}$ of an observable $\gy\in\Continf\of{\gW\Cartesian\Rn}$ given at $t=0$ by $f\of{\vq,\vp,0}=\gy\of{\vq,\vp}$.
\end{definition}
The conservation of energy immediately follows:\[
\derivop t h = \Poisson h h = 0\text.
\]
This is all of classical mechanics. We can certainly ``pimp'' the theoretical structure behind it further by introducing symplectic manifolds, Poisson algebras, and so on. But so far, this could have all been done by mathematicians. Where does the physicist come in? The art and the genius lies in finding the correct $h$. In doing so, at least when describing a real system you see around you, you more or less establish a law of nature.

Starting from this theoretical perspective, as opposed to doing a history course and starting with Newton and $F = ma$, could be described as the modern view, because it brings the symmetries of the theory to the front. From what we have, we can now derive Hamilton's equations of motion:
\begin{align*}
\derivop t q_j &= \Poisson{q_j}{h} \\
&=\sum{k=1}[n]\pa{\pderivop{q_k} q_j}\pderiv {p_k} h - \pa{\pderivop{q_k} p_j}\pderiv {q_k} h \\
&=\pderiv {p_j} h\\
\pderivop t p_j &= - \pderiv {q_j} h\text.
\end{align*}

How can we now describe a ``natural'' transformation between two phase spaces? If the map
\begin{align*}
\FunctionActionSpecBody
{\vs}
{\gW\Cartesian\Rn}
{\Closure\gW\Cartesian\Rn}
{\pa{\Pullback \vs \gy}\of{\vq,\vp}}
{\gy\of{\vs\of{\vq,\vp}}\in\Continf\of{\gW\Cartesian\Rn}}
\end{align*} verifies
\begin{align*}
\gy\in\Continf\of{\Closure\gW\Cartesian\Rn}\\
\Poisson{\Pullback s \gy}{\Pullback s \gj}\of{\vq, \vp} = \Poisson{\gy}{\gj}\of{s\of{\vq, \vp}} = \Pullback s \Poisson{\gy}{\gj} && \forall \gy, \gj \in \Continf\of{\Closure\gW\Cartesian\Rn}\text,
\end{align*}
then it is called \emph{symplectic} --- or ``canonical'' in 19th century math slang. Why is this important? It allows us to define when two systems, i.e., two hamiltonians and the associated phase spaces, are physically equivalent.

Let me just show you one thing. $\vs$ is a symplectic isomorphism if and only if $\diffd \vs = \tuple{\pderiv {x_k} {s_j}}$ satisfies
\[
\scal{\vv\diffd \vs \of{\vq,\vp}}{\pascal{\matJ\diffd \vs \of{\vq,\vp}\vw }} = 
\scal{\vv}{\pascal{\matJ \vw}}\text{, where } % TODO: This seems off
\matJ\DefineAs\begin{pmatrix}
\nullmat & \Identity_n\\
-\Identity_n & \nullmat
\end{pmatrix}\text,
\]
for all $\vv\in\Rn$, $\vw\in\Rn$ and $\tuple{\vq,\vp}\in\gW\Cartesian\Rn$.
\begin{definition}[Symplectic group]
And if we really want to, we can now also define the group of symplectic matrices $\SymplecticGroup\of{n, \R} \DefineAs \setst{\matA \in \GeneralLinearGroup\of{2n, \R}}{\Transpose{\matA} = - \matJ \matA^{-1} \matJ}$.
\end{definition}
% The standard notation is Sp(2n, R). What do we do here?
Note that ``it's even in the Heisenberg stuff'', as $\SymplecticGroup\of{1, \R} = \SpecialLinearGroup\of{2, \R}$. % This is true, but what does it have to do with the Heisenberg group? There seems to be something in Wikipedia about Heisenberg groups and symplectic vector spaces.
With this definition, the above characterisation of a symplectic isomorphism can be reformulated to $\diffd \vs \in \SymplecticGroup\of{n, \R}$.

What is the simplest possible motion? Let's take an $h_0$ that only depends on $\vp$, not on $\vq$. In that case we have
\begin{alignat*}{3}
\TimeDerivative{q_j} &= \pderiv{p_j}{h_0}\of\vp &\Implies&& \vq &= \deriv{\vp}{h_0}t + \vq\of{0} \\
\TimeDerivative{p_j} &= \pderiv{q_j}{h_0}\of\vp = 0 &\Implies && \vp &= \vp\of{0} \\
\vv &\DefineAs \TimeDerivative \vq = \pderiv p h
\end{alignat*}
That's a straight line. There's nothing simpler than a straight line, except no line at all --- but I'm not going to get into discussions of nothingness, though it's fashionable these days. There are entire books written about nothingness, and how string theory explains everything about nothingness...

Now suppose somebody over there in Oberwinterthur has a fancy Hamiltonian $h\of{\vq,\vp}$. How do you know it's not a straight line, and that you are not being charged non-straight line prices for a straight line? You want to pull back, to find $\vs$ such that $\Pullback \vs h = h_0$. You heard about the Hamilton-Jacobi equation, right? And probably your mind went fuzzy at this time, and you heard about maps, and maybe generating functions, and this and that... 
There is an $\vs$ if and only if $\setst{\gj}{\Poisson\gj h = 0}$ has an Abelian subalgebra $\LieAlgebraSymbol A$ of dimension $n$ such that
\begin{align*}
\LieAlgebraSymbol A &= \set{I_1,\dotsc,I_n}\text, \\
\Poisson{I_j}{I_k}&= 0\text,\\
\Poisson{I_j}{h}&=0\text.
\end{align*}
\
Setting things up like this, the logical structure is crystal clear. This is something a mathematician could do. You may say, as a physicist, ``frankly, my dear, I don't give a damn,''\footnote{\emph{Gone with the Wind}, 1939, directed by Victor Fleming.} but the reason this structure is interesting is because we have beautiful examples. So, if mathematicians are coming up with all different kinds of beautiful structures, the question is --- and I am putting this in very low terms --- Are they not just playing with themselves? With this you can predict where Pluto or Neptune is. String theory has predicted nothing, and it may never.

In the case $h\of{q,p}=\frac{1}{2}\pa{\frac{p^2}{n} + m\gw^2 q^2}$, where $\tuple{q,p}\in\R\Cartesian\R$, we have the harmonic oscillator.
And that's classical mechanics. Now on to quantum mechanics.

\subsection{Quantum Mechanics}
\begingroup%QM
\newcommand{\QuantumObservables}{\StandardSymbol{QO}}
How do we start? There is a number $n\geq 1$ of \emph{quantum degrees of freedom}.
Then, $\gW\Subset \Rn$ is the \emph{quantum configuration space}, like the classical configuration space above. If you had fallen asleep at the beginning of the previous hour, and you thought \textsc{q.m.} was just a misspelling of classical mechanics, you might think the lecture had just started. Now it becomes different.
\begin{definition}[Quantum phase space]
The \emph{quantum phase space} is $\Schwartz$.
\end{definition}
Now we need an algebra of quantum observables.
\begin{definition}[Quantum observables] The \emph{quantum observables} are the elements of \[
\QuantumObservables\DefineAs\setst{\opA\in\Endomorphisms\of\Schwartz}{
\begin{array}{l}
\text{$A$ is a continuous linear map such that}\\
 \forall\gy, \gj \in \Schwartz, \LTwoInner{\opA\gy}{\gj} = \LTwoInner{\gy}{\opA\gj}
\end{array}
}\text.
\]
\end{definition}
Let me give you two examples of observables: 
\begin{align*}
\pa{\opQ_j\gy}\of\vq &\DefineAs q_j\gy\of\vq\text, \\
\pa{\opP_j\gy}\of\vq &\DefineAs \frac{\ReducedPlanck}{\I} \pderivop{q_j}\text.
\end{align*}
\begin{proposition}
$\tuple{\QuantumObservables, \frac  \I \ReducedPlanck \commutator \opA \opB}$ is a Lie algebra.
\begin{proof} We check that $\QuantumObservables$ is closed under the bracket $\frac \I \ReducedPlanck \commutator\placeholder\placeholder$: let $\opA${, }$\opB$ be self-adjoint. Then
\begin{align*}
\adj{\pa{\frac \I \ReducedPlanck \commutator\opA\opB}}
&= -\frac \I \ReducedPlanck\adj{\pa{\opA\opB-\opB\opA}}\\
&= -\frac \I \ReducedPlanck\pa{\adj\opB\adj\opA-\adj\opA\adj\opB}\\
&= -\frac \I \ReducedPlanck\pa{\opB\opA-\opA\opB} = \frac \I \ReducedPlanck \commutator\opA\opB\text.
\end{align*}
You can check the Jacobi identity and all that.
\end{proof}
\end{proposition}
\begin{definition}[Quantum system]
A \emph{quantum system} is an $\opH\in\QuantumObservables$.
\end{definition}
\begin{definition}[Evolution of a quantum observable] The evolution of the observable $\opA$ is determined by \[\TimeDerivative\opA= \frac \I \ReducedPlanck \commutator\opA\opH\text.\]
\end{definition}
``That's it. That's quantum mechanics.''
\end{lecture}
\begin{lecture}[date=2013-04-16]
Okay! So where were we, ladies and gentlemen? When I last saw you, we talked about quantum mechanics. We have: EQ1

We're doing this the French way. This is a 5-tuple, the way Bourbaki would write it.
Such a logical structure is of no interest if you can't find $\opH$, unless there is a way of picking $\opH$s that describe quantum systems, not just because you believe they do, but because they do actually predict things that you can verify. It's like finding Waldo.\footnote{This is a reference to Martin Handford's \emph{Where's Wally?}, a series of children's books in which a character (Wally) is hidden on every page, published in the United Kingdom from 1987 onwards. It was published as \emph{Where's Waldo?} in the United States, and as \emph{\textgerman{Wo ist Walter?}} is Germany.} Unless you can find Waldo, it's just a meaningless complicated picture. Finding Waldo gives it its meaning. Actually let's call the Hamiltonian $\opW$. Finding Waldo is \emph{not} the Bourbaki way of doing things. I'll show you two books any physicist will tell you about: \emph{Find classical Waldo}, and \emph{Find quantum Waldo}. \emph{Find classical Waldo} you already had a lecture about last semester.
EQ2
Now we do something \emph{really} weird. We find Waldo in the classical case --- this may be hard, but we find Waldo hiding amidst the electric and magnetic fields ---, and then we quantise him: EQ3
For instance, EQ4
%%% REMOVED UNTIL SENSE IS MADE OUT OF THIS %%% Why does this make sense? Do you remember the Weierstrass approximation theorem? [...] EXPAND You could do all of Analysis~\textsc{i} by just defining the derivative for $x^k$ and saying it is linear. % WHAT? This only works for analytic functions, and then it has little to do with the Weierstrass theorem anymore...

This works because we have a strict Apartheid between the $\opP$s and $\opQ$s. They have to sit on different sides of the bus.
For instance, look at the harmonic oscillator:
EQ5

But if we do not have this Apartheid in the classical Hamiltonian, if Waldo lives in the United States after the civil rights movement --- or even after Obama was elected ---, we may have $q_i p_j$ in the Hamiltonian --- say with $\vq \Exterior \vp$. And then this commutes in the classical case, but not in the quantum case, so we have a problem: do we write $\opQ_i \opP_j$ or $\opP_j \opQ_i$? Somehow, if you have a physically interesting Hamiltonian, it just happens that you can fiddle with it to get rid of those $q_i p_j$. This may sound like a ridiculous story like \foreign{\textgerman{Samichlaus}}, but it works. That's all I can tell you. Do you know about Pauling? He got the Nobel Peace Prize --- and also the Nobel Prize in Chemistry for something he did on the side. Well Pauling used that sort of bullshit to calculate the angle in the water molecule, and he got the right answer.

So do you understand the point? There is a logical structure which is neat, and there is this weird recipe to quantise Waldo, and it works. I can't tell you more, and nobody can. Actually, there is a slightly more satisfying recipe, using these $\opW$ --- hey, more Waldos! --- these Weyl operators we talked about. We'll come back to this later. It's a messy story, and it doesn't even have a good end. Somehow or other, quantum mechanics and gravity don't mix. There was a time when string theory was supposed to quantise gravity. Twenty years ago there was a long list of all the thing quantum mechanics was supposed to do. Now the list looks very different, and it's much shorter; none of the things that used to be on that list are on it anymore. String theory has changed its \foreign{\textlatin{curriculum vitae}}.
\end{lecture}
\endgroup%QM

\end{document}
