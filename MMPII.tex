%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%  Please follow ISO standards.  %%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  A copy of ISO 80000-2:2009 can be found at      %
%  <http://goo.gl/fVoiF>. Keep other applicable    %
%  standards in   mind, e.g. ISO 8601:2004, etc.   %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[10pt, a4paper, twoside]{lecturenotes}
\usepackage[Mathematics]{semtex}

%%%% Shorthands.

%% Frequently-used sets.
\newcommand{\Rn}{{\R^n}}
\newcommand{\Schwartz}{{\SchwartzSpace\of{\Rn}}}
\newcommand{\TemperedDistributions}{{\contdual\SchwartzSpace\of{\Rn}}}

%% Freqently-used expressions.
% Nonstandard, but seen as the application of an informal / to the Pauli matrices.
\newcommand{\ReducedPauli}[1]{\slashed{\StandardMatrixSymbol{\sigma}}_{#1}}
% Nonstandard.
\newcommand{\PauliTensor}{\tensgs}
\newcommand{\sqftnrm}{\frac{1}{\pa{2\Pi}^n} }
\newcommand{\ftnrm}{\frac{1}{\pa{2\Pi}^{\frac{n}{2}}} }
%TODO: I think this makes sense. It has to be bold somehow in any case.
\DeclareMathOperator{\AdjointRep}{\StandardTensorSymbol{Ad}}

\newcommand{\cz}{\mathscr{Z}}

\newcommand{\opAdag}{\adj{\opA}}

% Not exactly the notations from the lecture, but this is both
% less ambiguous and more readable.
\newcommand{\InnerVn}[2]{\lParenCommaRParen{#1}{#2}}
\newcommand{\InvariantInnerVn}[2]{\lAngleCommaRAngle{#1}{#2}}

% Non-standard definitions.
% \SignDistrib is the distribution generated by 
% \Sign.
\DeclareMathOperator{\SignDistrib}{Sgn}
% \AbsDistrib is the distribution generated by the absolute value.
\DeclareMathOperator{\AbsDistrib}{Abs}

%%%%%

%%%% Title and authors.

\title{%
\textdisplay{%
Notes from the\\Methods of Mathematical Physics~\textsc{ii} \\lectures of 2013-02-19{\slash}05-30 by\\ Prof.~Dr.~Eugene~Trubowitz%
}%
}
\author{Robin~Leroy and David~Nadlinger}
\input{Figures.tex}
\begin{document}
  \maketitle
  % The first three lectures are not covered by this document.
  \setcounter{Lecture}{3} 
  \section{\emph{True Lies}: An introduction to distribution theory}
  \NewLecture[date={2013-02-28}]
  In many areas of physics, the Dirac delta ``function'' is an important tool, e.g., for describing localized phenomena. While it is usually described in vague terms as ``only making sense under an integral'', physicists tend to still write identities such as
  \begin{equation*}
    \Laplacian\frac{1}{\norm{\vx}} = -4 \Pi \DiracDelta\of\vx\text{,}
  \end{equation*}
  and rely on their intuition and previous mistakes to distinguish situations in which they can safely apply this calculus from the ones where that would lead to incorrect results.

  How can we formalize this? Note that the operation of taking the integral of the Dirac delta multiplied with another function $\gj$ can be considered as a map from that function to a scalar in $\C$. Thus, we could consider
  \begin{equation*}
    \DiracDelta[\vx]\of\gj = \int\Rn \DiracDelta\of{\vx - \vy} \gj\of\vy \diffd\vy
  \end{equation*}
  to be a functional, and more specifically an element of the dual space of whatever class of functions we can allow for $\gj$. But the question is: what restrictions do we need on this space of functions if we want to find objects like the Dirac delta in its dual space?
  
  Actually, we want these functionals to be in the continuous dual of our space, that is, we want them to be functionals $U$ such that $\conv {U\of{\gj_k}}{U\of\gj}$ for all sequence of functions $\tuplespec{\gj_k}{k\in\N}$  that converge to $\gj$ in an appropriate norm\footnote{This simply means that $U$ is a continuous map from the function space endowed with its norm topology to $\R$ endowed with the standard topology.}---we want our space to be a Banach space.
Intuition tells us that the smaller, i.e., more restricted, a space of functions is, the stronger the conditions on $\conv{\gj_k}{\gj}$ must be in order for it to be complete, and so the bigger its continuous dual can be, as we only need $\conv {U\of{\gj_k}}{U\of\gj}$ for the sequences $\tuplespec{\gj_k}{k\in\N}$ that converge in that space. Thus, we will investigate a space of functions $\gj\of\vx \in \Continf\of\Rn$ that vanish quickly as $\conv {\norm\vx} \infty$ (``converge with a vengeance''), hoping to find the Dirac delta in its continuous dual.
  \begin{definition} First, a few definitions used throughout this course:
%TODO(eggrobin): the big partial derivative is nonsemantic.
  \begin{equation*}
    \Continf\of\Rn \DefineAs \setst{\FunctionSpec\gj\Rn\C}
    {\forall \miga \in \N^n, \pdop\miga\gj \in \Cont\of\Rn}
  \end{equation*} is the space of $\emph{smooth functions}$ from $\Rn$ to $\C$, where
  \begin{alignat*}{2}
    \N &\DefineAs \set{0, 1, 2, \dotsc}\text{,}\quad
    &\miga &= \tuple{\ga_1, \dotsc, \ga_n} \in \N^n \text{ is a \emph{multi-index},} \\
    \total{\miga} &\DefineAs \ga_1 + \dotsb + \ga_n\text{,}\quad
    &\pd\vx\miga &\DefineAs \frac{\partial^{\total{\miga}}}{\pd{x_1}{\ga_1} \dotsm \pd{x_n}{\ga_n}} \quad
    \commentbox{(we use $\pdop\miga$ when the variable is implicit),}\\
    \text{and }\vx^\migb &\DefineAs x_1^{\gb_1} \dotsm x_n^{\gb_n}
    \text{ for }\vx \in \Rn\text{, } \migb \in \N^n\text{.}
      \span\omit\span\omit\span\omit
  \end{alignat*}
  \end{definition}
  
  \begin{definition}[Schwartz space]  Now we define the \emph{Schwartz space}
  \begin{equation*}
    \Schwartz \DefineAs \setst{\gj \in \Continf\of\Rn}{\forall \miga, \migb \in \N^n, \SchwartzNorm{\miga}{\migb}{\gj} < \infty}
    \text{, where }\SchwartzNorm{\miga}{\migb}{\gj} \DefineAs \sup_{\vx\in\R^n}\abs{\vx^\miga\pd\vx\migb \gj\of\vx}\text{.}
  \end{equation*}
  \end{definition}
  Note that $\gj \in \Schwartz$ implies $\lim_{\conv {\norm\vx} \infty}\pa{1+\norm{\vx}^2}^k \pd\vx\miga \gj\of\vx = 0$, as
  \begin{align*}
    \pa{1+\norm{\vx}^2}^m \pd\vx\miga \gj\of\vx 
    &= \sum{j = 0}[m] \binom{m}{j} \norm{\vx}^{2j} \pd\vx\miga \gj\of\vx \\
    &= \sum{j = 0}[m] \binom{m}{j} \pa{\sum{i=1}[n]x_i^2}^j \pd\vx\miga \gj\of\vx \\
    &= \sum{j = 0}[m] \binom{m}{j} \sum{\total{\miga} = j}\pa{\prod{i = 1}[n] x_i^{2\ga_i}} \pd\vx\miga \gj\of\vx
  \end{align*}
  and $\sup_{\vx \in \Rn}\abs{\vx^{2 \miga} \gj\of\vx}<\infty$. It immediately follows:
  \begin{lemma}
    If $\gj \in \Schwartz$, then
    \begin{equation*}
      \forall k \geq 0\quad \forall \miga \in \N^n\quad \exists C_\miga \in \R \quad \abs{\pd\vx\miga\gj\of{x}} < \frac{C_\alpha}{\pa{1 + \norm{\vx}^2}^k}\text{.}
    \end{equation*}
  \end{lemma}
  \begin{definition}[Convergence in $\Schwartz$]
    The sequence $\tuplespec{\gj_k}{k \in \N} \in \Functions\N{\pa\Schwartz}$ \emph{converges to} $\gj \in \Schwartz$ if and only if 
    \begin{equation*}
    \forall \miga, \migb \in \N^n \quad \lim_{k \rightarrow \infty}\SchwartzNorm{\miga}{\migb}{\gj_k - \gj}=0\text{.}
    \end{equation*}
    We use the notation $\conv[\Schwartz]{\gj_k}\gj$.
  \end{definition}
  \begin{definition}[Tempered distributions]
    Note that $\Schwartz$ is a complex vector space. We define that $U$ is an element of its continuous dual $\TemperedDistributions$, the space of \emph{tempered distributions}, if and only if:
    \begin{enumerate}
      \item $U$ is a linear map from $\Schwartz$ to $\C$, i.e., $U\in\Dual\SchwartzSpace\of\Rn$
      \item $U$ is continuous on $\Schwartz$, i.e. if $\conv[\Schwartz]{\gj_k}\gj$ then $\lim_{\conv k \infty}U\of{\gj_k} = U\of\gj$.
    \end{enumerate}
  \end{definition}
  Let us now consider some examples. First, define the space of locally integrable functions as follows.
  \begin{definition}[Locally integrable functions]
    \begin{equation*}
      \LspaceLoc[1]\of\Rn \DefineAs \setst{\FunctionSpec f \Rn \C}{\textstyle{\forall \vx \in \Rn, \exists \ge > 0, \int{\OpenBall \ge \vx}\abs{f\of\vy} \diffd{\vy} < \infty}}
    \end{equation*}
  \end{definition}
  Now let $f \in {\LspaceLoc[1]}\of\Rn$.
  If additionally $\sup_{\vx \in \Rn}\pa{1 + \norm{\vx}^2}^{-s}\abs{f\of{\vx}} < \infty$
  for some $s > 0$ (e.g. $f\of\vx \sum{\total\migb\leq s} c_\migb \vx^\migb$),
  then 
  \begin{equation*}
    U_f\of\gj \DefineAs \int\Rn f\of{\vx} \gj\of{\vx} \diffd{x}
  \end{equation*}
  is a tempered distribution.
  \begin{definition}[Dirac delta distribution]
    Another example is the \emph{Dirac delta distribution} mentioned above, with 
    \begin{equation*}
      \DiracDelta\of\gj \DefineAs \gj\of{0}\text{,}\quad \DiracDelta[\vx]\of{\gj} \DefineAs \gj\of{\vx}\text{.}
    \end{equation*}
  \end{definition}  
  Distributions have the nice property that we can always differentiate them.
  \begin{definition}[Derivative of a distribution]
    Let $U\in\TemperedDistributions$. Then $\pdop\miga U$ is the distribution defined
    by 
    \begin{equation*}
      \pa{\pdop\miga U}\of{\gj} \DefineAs \pa{-1}^{\total{\miga}} U\of{\pdop\miga\gj}\quad
      \text{for }\gj \in \Schwartz\text{.}
    \end{equation*}
  \end{definition}
      This is natural, as
  \begin{align*}
    U_{\pdop\miga f}\of\gj 
    = \int\Rn\pa{\pdop\miga f\of\vx} \gj\of\vx \diffd{\vx} 
    = \pa{-1}^{\total{\miga}} \int\Rn f\of\vx \partial^\miga \gj\of\vx \diffd{\vx} 
  \end{align*}
  by integrating by parts.
  Let's find the derivative of $\abs{\placeholder}$ in the distribution world:
  \begin{align*}
    \AbsDistrib\of\gj &\DefineAs \int{-\infty}[\infty] \abs{x} \gj\of{x} \diffd{x}\\
    \pa{\derivop x \AbsDistrib}\of\gj &= - \AbsDistrib  \deriv x \gj\\
    &= - \int0[\infty] x \gj\der\of{x} \diffd{x} - \int{-\infty}[0]\pa{-x}\gj\der\of{x} \\
    &= - \diff 0 \infty {x \gj\of{x}} + \int0[\infty] \gj\of{x} \diffd{x} + 
    \diff {-\infty}{0} {x \gj\of{x}} - \int{-\infty}[0] \gj\of{x} \diffd{x} \\
    &= \int{-\infty}[\infty] \Sign\of{x} \gj\of{x} \diffd{x} = U_{\Sign}\of\gj\text{,}\\
    \text{where } \Sign\of{x} &\DefineAs
	\begin{cases}
		1  & x > 0 \\
		0  & x = 0 \\
		-1 & x < 0
	\end{cases}
    \quad\text{is the sign function.}
  \end{align*}
We define $\SignDistrib\of\gj \DefineAs \int{-\infty}[\infty] \Sign\of{x} \gj\of{x} \diffd{x}$ and write $\derivop x \mathrm{Abs} = \SignDistrib$. This corresponds to $\derivop x \abs{x} = \Sign\of{x}$ in physicist-speak.

  What about $\Laplacian\frac{1}{\norm{\vx}}$? By defining the \emph{Newtonian distribution}
  \begin{align*}
    N\of\gj &\DefineAs \int{\R^3} \frac{1}{\norm{\vx}} \gj\of\vx \diffd{\vx}\text{,}
  \intertext{we get}
    \Laplacian{N}\of\gj = \sum{j = 1}[3] \pderivop[2]{x_j}N\of\gj &= \sum{j = 1}[3]  N\of{\pderiv[2]{x_j}\gj} = \dotsb = -4 \pi \gj\of{0}\text{,}
  \intertext{in other words,}
    \Laplacian N &= -4 \pi \DiracDelta\text{.}
  \end{align*}
  
  \NewLecture*[date=2013-03-05]
  \begin{definition}[Inner product on $\Schwartz$]\LectureStartsHere
  Let $\gj,\gy\in\Schwartz$. We define:
  \begin{equation*}
  \LTwoInner \gy \gj \DefineAs \int\Rn \conj\gy\of\vx \gj\of\vx \diffd\vx \text{.}
  \end{equation*}
  \end{definition}
\section{General properties of the Fourier transform}
  \begin{definition}[Fourier Transform]
  For $\gj\in\Schwartz$, we define the \emph{Fourier transform} $\ft\gj$ of $\gj$, also written $\FT\of\gj$, as:
  \[
    \FT\of\gj\of\vk\DefineAs\ft\gj\of\vk\DefineAs
    \ftnrm\int\Rn\gj\of\vx\E^{-\I\scal{\vk}{\vx}}\diffd\vx\text.
  \]
  \end{definition}
  
  \NewLecture*[date=2013-03-07]
  \begin{lemma}[``Your life in Fourier land depends on it.'']\LectureStartsHere
    \begin{align}
      \label{FTLinearity}
      \begin{split}      
      \ft{\gj + \gy} &= \ft{\gj} + \ft{\gy}\text{,}\\
      \ft{\gl \gj} &= \gl \ft{\gj} 
      \end{split}
      \\
      \label{FTUpperBound}
      \abs{\ft{\gj}\of\vk} &\leq \ftnrm\Lnorm[1]{\gj}<\infty\text{,} &&
      \Lnorm[1]{\gj} \DefineAs \int\Rn \abs{\gj\of\vx} \diffd \vx
      \\
      \label{FTScaling}
      \ft{\gj\of{\gl\placeholder}}\of\vk &= \frac{1}{\abs{\gl}^n}\ft{\gj}\of{\frac\vk\gl}\text{,} &&
      \gl\neq 0
      \\
      \label{FTTranslation}
      \ft{T_\vy\gj}\of\vx &= \E^{\I\scal\vk\vy}\ft\gj\of\vk\text{,} &&
      \pa{T_\vy\gj}\of\vx \DefineAs \gj\of{\vx+\vy}
      \\
      \label{FTDerivative}
      \begin{split}
      \ft{\pderiv{x_j}\gj}\of\vk &= \I k_j \ft\gj\of\vk \text{,} \\ 
      \ft{x_j \gj}\of\vk &= \I \pderiv{k_j}{\ft\gj} \of\vk
      \end{split}
      \\
      \label{FTUnitary}
      \int\Rn\ft\gj\of\vy \gy\of\vy \diffd\vy &= \int\Rn \gj\of\vy \ft\gy\of\vy \diffd\vy
    \end{align}
    \begin{proof}
      Proof of (\ref{FTUpperBound}):
      \begin{align*}
       \abs{\ft\gj\of\vk} 
        &= \abs{\ftnrm\int\Rn \gj\of\vx \E^{-\I \scal\vk\vx}\diffd\vx} \\
        &\leq \ftnrm\int\Rn \abs{\gj\of\vx 
        \E^{-\I \scal\vk\vx}}\diffd\vx  = \ftnrm
        \underbrace{\int\Rn \abs{\gj\of\vx}}_{\Lnorm[1]{\gj}}
        \underbrace{\abs{\E^{-\I\scal\vk\vx}}}_{1}\diffd\vx \\
      \end{align*}
      Proof of existence of the integral $\Lnorm[1]\gj$:
      \begin{align*}
        \int\Rn\abs{\gj\of\vx}\diffd\vx 
        &= \int\Rn\pa{1+\norm{\vx}^2}^{\frac{s}{2}}\abs{\gj\of\vx}
        \frac{\diffd\vx}{\pa{1+\norm{\vx}^2} ^ {\frac{s}{2}}}  \\
        &\leq \SchwartzNorm s\nullmi\gj \int\Rn\frac{1}{\pa{1+\norm{\vx}^2}^{\frac{s}{2}}}\diffd\vx\\
        &= \SchwartzNorm s\nullmi\gj \int0[\infty]\frac{r^{n-1}}{\pa{1+r^2}^{\frac{s}{2}}}\diffd r
        < \infty &&\text{for } s\geq n+1\text{.}
      \end{align*}
      ``(\ref{FTLinearity}) I will not do!''
      Proof of (\ref{FTScaling}):
      \begin{align*}
        \ft{\gj\of{\gl\placeholder}} 
        &= \ftnrm\int\Rn\gj\of{\gl\vx}\E^{-\I\scal\vk\vx} \diffd\vx \\
        &= \ftnrm\int\Rn\gj\of\vy\E^{-\I\scal{\frac1\gl\vk}{\vy}} \diffd\of{\frac{\vy}{\gl}}
          && \text{where } \vx=\frac1\gl\vy\text{.} \\
        &= \frac{1}{\abs\gl^n} \underbrace{
          \ftnrm\int\Rn\gj\of\vy\E^{-\I\scal{\frac\vk\gl}\vy}
          \diffd\vy}_{\ft\gj\of{\frac\vk{\gl}}}
      \end{align*}
      Proof of the second part of (\ref{FTDerivative}):
      \begin{align*}
        \ft{x_j\gj}\of\vx 
        &= \ftnrm\int\Rn x_j\gj\of{\vx}\E^{-\I\scal\vk\vx}\diffd\vx \\
        &= \ftnrm
          \int\Rn\gj\of{\vx}\I\pderivop{k_j}\E^{-\I\scal\vk\vx} \diffd\vx 
          && \text{as }\pderivop{k_j}\E^{-\I\scal\vk\vx} =
            \pderivop{k_j}\E^{-\I\sum{r=0}[n] k_j x_j}=-\I x_j \E^{-\I\scal\vk\vx}\\
        &= \I\pderivop{k_j}\ftnrm
          \int\Rn\gj\of\vx\E^{-\I\scal\vk\vx} \diffd\vx \\
        &= \I\pderiv{k_j}{\ft\gj}\of\vk
      \end{align*}
      
      \emph{The professor starts whistling some elevator music while waiting for a student to write down the proof before he can wipe the board. When the student is done, the professor notices that there still is some room left on the blackboard and starts writing the rest there, without erasing anything.}
      
      Proof of (\ref{FTUnitary}):
      \begin{align*}
        &\int\Rn\ft\gj\of\vy \gy\of\vy \diffd\vy
        = \int\Rn\ftnrm\int\Rn\gj\of\vx
          \E^{-\I\scal\vy\vx}\diffd\vx\:\gy\of\vy\diffd\vy \\
        &= \int\Rn\gj\of\vx\ftnrm\int\Rn
          \E^{-\I\scal\vx\vy}\gy\of\vy\diffd\vy\diffd\vx
          \quad \parbox{.5\linewidth}{As $\int {\R^{2n}}\abs{\gj\of\vx
          \E^{-\I\scal\vy\vx}\gy\of\vy}\diffd\vx\diffd\vy<\infty$,
          we can use Fubini's theorem.} \\
        &=\int\Rn\gj\of\vy \ft\gy\of\vy \diffd\vy
      \end{align*}
    \end{proof}
  \end{lemma}
  \begin{proposition}
The Fourier transform $\ft{\placeholder}$ is a continuous bijective linear map from the Schwartz space $\Schwartz$ to itself. $\rft{\ft{\gj}}=\gj$, where $\rft\gy\of\vx\DefineAs\pa{2\Pi}^{-n/2} \int\Rn\gy\of\vk\E^{\I\scal\vk\vx}\diffd\vx$, read ``unhat'', ``bird'', ``seagull'', or whatever you like.
    \begin{proof}
      \begin{align*}
        \gj\in\Schwartz&\Rightarrow\vx^\miga\pd\vx\migb\in\Schwartz \\
        \FT\of{\vx^\miga\pd\vx\migb\gj}\of\vk
        &=\I^{\total\miga}\pd\vk\miga\ft{\pa{\pd\vx\migb\gj}}\of\vk 
          && \commentbox{from the second equality in (\ref{FTDerivative}), applied $\total\miga$ times.} \\
        &=\I^{\total\miga}\pd\vk\miga\of{\I^{\total\migb}\vk^\migb\ft\gj}\of\vk \\
        &=\I^{\total\miga+\total\migb}\pd\vk\miga\of{\vk^\migb\ft\gj}\of\vk
      \end{align*}
      ``I should have done it the other way around. [...] Let's try it the other way around.''
      We want:
      \begin{equation*}
        \forall\miga,\migb,\sup_{\vk\in\Rn}\abs{\vk^\miga\pd\vk\migb\ft\gj\of\vk}<\infty
      \end{equation*}
      As that implies $\ft\gj\in\Schwartz$.
      \begin{align*}
        \FT\of{\pd\vx\miga\of{\vk^\migb\gj}}\of\vk
        &=\I^{\total\miga+\total\migb}\vk^\miga\pd\vk\migb\ft\gj\of\vk
        && \commentbox{by applying the first part of (\ref{FTDerivative}) $\total\miga
        $ times and the second part $\total\migb$ times.}\\
        \forall\miga,\migb,\abs{\vk^\miga\pd\vk\migb\ft\gj\of\vk}
        &\leq\ftnrm\Lnorm[1]{\pd\vx\migb\of{\vx^\migb\gj}}<\infty
        && \text{from (\ref{FTUpperBound}).}
      \end{align*}
      We now know $\ft\gj\in\Schwartz$.
      Let us prove $\rft{\ft\gj}=\gj$. ``When you do the wrong thing I'm gonna scream loudly.''
      \begin{align*}
        \rft{\ft\gj}\of\vx&=\ftnrm\int\Rn\ft\gj\of\vk\E^{\I\scal\vk\vx}\diffd\vk \\
        &=\sqftnrm\int\Rn\int\Rn\gj\of\vy\E^{-\I\scal\vk\vy}\diffd\vy\:\E^{\I\scal\vk\vx}\diffd\vk
      \end{align*}
      We don't interchange the integrals here because that would lead to ugly calculations. ``If you paint the walls before you start building, it's not a good idea.''
      At this point, somebody suggests replacing $\pa{2\Pi}^{-n}\int\Rn \E^{\I\scal\vk{\pascal{\vx-\vy}}}\diffd\vk$ by $\DiracDelta\of{\vx-\vy}$. ``It's plausible!---\textsc{Why}? [...] Ah I said it, so it's plausible.'' However, we would need to do some nasty calculations in order to do this. ``How are we going to do it so fast that you don't get bored, and yet in enough detail that he's convinced? Be sneaky.''
      \begin{align*}
        \rft{\ft{\gj}}
        &=\ftnrm\int\Rn\ft\gj\of\vk\E^{\I\scal\vk\vx}
        \underbrace{\lim_{\ge\downarrow 0}\E^{-\ge\frac{\norm\vk^2}{2}}}_{\mathclap{\substack{1
        \text{ written in}\\\text{some other way}}}}\diffd\vk\\
        &=\lim_{\ge\downarrow 0}\ftnrm\int\Rn\ft\gj\of\vk\E^{\I\scal\vk\vx}
        \E^{-\ge\frac{\norm\vk^2}{2}}&&\parbox{.4\linewidth}{``Don't worry.'' This is actually a one-liner 
        using Lebesgue's dominated convergence theorem.}
      \end{align*}
      \emph{The 10-minute break ends with the loud noise of a metallic pointing stick hitting the desk.}
      \begin{align*}
        \ftnrm\int\Rn\ft\gj\of\vk\E^{\I\scal\vk\vx}\E^{-\ge\frac{\norm\vk^2}{2}}\diffd\vk
        &=\ftnrm\int\Rn\ftnrm\int\Rn\gj\of\vy\E^{-\I\scal\vy\vk}\diffd\vy\:
        \E^{\I\scal\vk\vx}\E^{-\ge\frac{\norm\vk^2}{2}}\diffd\vk\\
        &=\ftnrm\int\Rn\gj\of\vy\ftnrm\int\Rn\E^{\I\scal\vk{\pascal{\vx-\vy}}}
        \E^{-\ge\frac{\norm\vk^2}{2}}\diffd\vk\diffd\vy
      \end{align*}
      Recall: ``every path leads to Rome and every Gaussian is in $\Schwartz$''. Also, the last foot of a dactylic hexameter is always a spondee. 
      \begin{equation*}
        \ftnrm\int\Rn\E^{-\ge\frac{\norm\vk^2}{2}}\E^{\I\scal{\pascal{\vy-\vx}}\vk}\diffd\vk
        =\frac{\E^{-\frac{1}{2\ge}\norm{\vx-\vy}^2}}{\ge^{\frac{n}{2}}}
      \end{equation*}
      We therefore get:
      \begin{align*}
         \ftnrm\int\Rn\ft\gj\of\vk\E^{\I\scal\vk\vx}\E^{-\ge\frac{\norm\vk^2}{2}}\diffd\vk
         =\ftnrm\int\Rn\gj\of\vy\ftnrm\int\Rn\E^{\I\scal\vk{\pascal{\vx-\vy}}} \E^{-\ge\frac{\norm\vk^2}{2}}\diffd\vk\diffd\vy
        \span\omit\span\omit\span\omit\\
        \qquad &=\ftnrm\int\Rn\gj\of\vy\frac{\E^{-\frac{1}{2\ge}\norm{\vx-\vy}^2}}
        {\ge^{\frac{n}{2}}}\diffd\vy \\
        &=\ftnrm\int\Rn\gj\of{\vy\sqrt\ge+\vx}\frac{\E^{-\frac{1}{2\ge}\norm{\vy\sqrt\ge}^2}}
        {\ge^{\frac{n}{2}}}\diffd\of{\vy\sqrt\ge} && \text{``Epsilons everywhere!''}\\    
        &=\ftnrm\int\Rn\gj\of{\vx+\vy\sqrt\ge}\E^{-\frac{1}{2}\norm\vy^2}\diffd\vy
        && \text{``Now we can paint.''}\\
        &\overset{\ge\downarrow 0}{\rightarrow}
        \ftnrm\gj\of\vx\int\Rn\E^{-\frac{1}{2}\norm\vy^2}\diffd\vy =\gj\of\vx\text{.}
      \end{align*}
    \end{proof}
  \end{proposition}
  \section{Seeking eigenfunctions of the Fourier transform}
  ``The main ideas of many things are right here.''
  We seek to find the solutions $\tuple{\gj,\gl}$ of
  \begin{equation*}
    \ft\gj=\gl\gj \text{.}
  \end{equation*}
  We shall find all of them. ``Not one is going to get away.'' Actually, we have already got one:
  \begin{equation*}
    \ft{\E^{-\frac{1}{2}\norm\vx^2}}=\E^{-\frac{1}{2}\norm\vx^2} \text{,}
  \end{equation*}
  so we know that 1 is an eigenvalue. How do we find the others? ``Here you actually have to have an idea.''
  \paragraph{Idea}
  Find an $H\in\Endomorphisms\of\Schwartz$ such that $\commutator H {\ft{\placeholder}} = 0$. Then they have the same eigenspaces, so we just need to find the eigenfunctions of $H$ (see \emph{Finite Dimensional Quantum Mechanics}).
  
  Let $n=1$.
  \begin{equation*}
    \left.
    \begin{aligned}
      \ft{\pa{x^2\gj}}\of k &= -\derivop[2]{k}\ft\gj\of k \\
      \ft{\pa{\derivop[2]{x}\gj}}\of k &= -k^2 \ft\gj\of k
    \end{aligned}
    \right\rbrace \qquad \commentbox{From the lemma your life depends on.}
  \end{equation*}
  Subtracting the second equality from the first one above:
  \begin{equation*}
    \FT\of{\pa{-\derivop[2]{x}+x^2}\gj}\of k = \pa{-\derivop[2]{k}+k^2}\FT\gj\of k\text{.}
  \end{equation*}
  We smell a harmonic potential. ``Don't get your fingers too close, they sometimes bite.''
  
  Let $\opH \DefineAs \frac{1}{2}\pa{-\derivop[2]{x}+x^2-1}$. Then $\commutator \opH {\ft{\placeholder}}=0$. We multiplied by $\frac{1}{2}$ so that it looks even more like a harmonic potential (it is the Hamiltonian of the quantum harmonic oscillator). Why did we add $-1$? Define
  \begin{align*}
    \opA &\DefineAs \frac{1}{\sqrt 2}\pa{\derivop{x} + x}\text{,}\\
    \opAdag &\DefineAs \frac{1}{\sqrt 2}\pa{-\derivop{x} + x}\text{.}
  \end{align*}
  We then have $\opH = \opAdag\opA$. If $a$ and $b$ commute, then we have $\pa{a-b}\pa{a+b}  = a^2-b^2$, but here $x$ and $\derivop{x}$ don't quite commute, hence the $-1$.

\NewLecture[date=2013-03-12]
Let us look at these operators in more detail: define $\pa{\opQ f}\of x \DefineAs x f\of x \in \SchwartzSpace\of{\R}$, $\pa{\opP f}\of x \DefineAs \derivop x f\of x \in \SchwartzSpace\of\R$ for $f\in\SchwartzSpace\of\R$. We saw that $\commutator \opQ \opP = 1$. We also have:
\begin{align*}
  \LTwoInner \gj {\opP\gy} 
  &= \int\R \conj\gj\of x \derivop x \gy \of x \diffd x \\
  &= \diff {-\infty}{\infty}{\conj\gj\gy} - 
    \int\R \derivop x \gj \of x \gy\of x \diffd x \\
  &= - \LTwoInner {\opP\gj} \gy \text{,}&&\commentbox{$\diff {-\infty}{\infty}{\conj\gj\gy} = 0$  as $\gj\in\SchwartzSpace\of\R$, $\gy\in\SchwartzSpace\of\R$, and therefore $\conj\gj\gy\in\SchwartzSpace\of\R$.}
\end{align*}
so $\opP$ is skew symmetric. What is better than skew symmetric? Self adjoint is better. How do we make a skew symmetric operator self adjoint? We add an $\I$. Namely, redefine $\pa{\opP f}\of x \DefineAs \frac{1}{\I}\derivop x f\of x$, we then have $\LTwoInner \gj {\opP\gy}  = \LTwoInner {\opP\gj} \gy$. $\opQ$ is obviously self adjoint. We now have $\commutator \opQ \opP = \I \Identity$. Multiplying by some $h\in\R$, redefine $\pa{\opP f}\of x \DefineAs \frac{h}{\I}\derivop x f\of x$, we get $\commutator \opQ \opP = h \I \Identity$.

If we let $h \DefineAs \ReducedPlanck$, the conditions verified by $\opP$ and $\opQ$ are the conditions for infinite matrices $\matP$ and $\matQ$ in the ``catechism of matrix mechanics'' in \emph{Finite Dimensional Quantum Mechanics}.
``Being something a little more, and something a lot less than cockroaches, we are curious.'' What we have been doing here is not the ideal way of doing this: we have to pick a vector space, in this case $\Schwartz$, last year the space of infinite matrices. As we learned in linear algebra, we gain a deeper understanding if we do not use coordinate systems. 
``We are going to study the pure essence'' of this. We are going to replace that by an abstract group, which lives in Plato's cave, and when we need to calculate, we will look at a representation. ``The group is as close to God as you can get.''

\begin{definition}[Heisenberg group]
We define the \emph{Heisenberg group}, introduced by Hermann Weyl.
\begin{equation*}
\tuple{
\HeisenbergGroup \DefineAs \setst{
\begin{pmatrix}
1 & r & t\\
0 & 1 & s\\
0 & 0 & 1\\
\end{pmatrix}
}{r,s,t\in\R}, 
\text{ordinary matrix multiplication}}
\end{equation*}
\end{definition}

You know that if you take $\E^x\E^y$ on reals numbers, you get $\E^{x+y}$, but that doesn't work for matrices. Hermann Weyl's idea is the following: we look at the identities \begin{align*}
\pa{\E^{\I t P} f}\of x &= f\of{x+t}
&&\parbox{.5\linewidth}{ as $\pa{\E^{\I t \pa{\frac{1}{\I}\derivop x}}}f\of x \overset{\text{formally}}{=} \sum{l=0}[\infty]\frac{t^l}{l!}\derivop[l] x f \of x $, the Taylor series of $f\of{x + t}$ at $x$.}\\
\E^{\I t P}\E^{\I s Q} &= \E^{\I s t} \E^{\I s Q} \E^{\I t P}
\end{align*}

Write $X \DefineAs \E^{\I Q}$ and $Y \DefineAs \E^{\I P}$, $X^r Y^s = \E^{\I r s} Y^s X^r$, $Z \DefineAs \pa{\E^\I}^t \Identity$. We will generate a group with these three elements. ``I'll think up another way of doing this; it's something worth doing twice.'' How do we preserve all the ideas, and yet get rid of all the infinite dimensional vector spaces? We study a finite dimensional analog, the finite Heisenberg group $\FiniteHeisenbergGroup n$, in which we use $\IntegersModulo{n}$ instead of $\R$. And we'll use that little group to do all sorts of experiments. No one will complain about what we do to it. 

Let us start again. ``Those of you who have done the exercises, and those of you who have the determination to go to Prof. Willwacher's---`Willwacher.' I like that name.---lecture, we are going to do that again, but on a concrete example.''
We will find all the conjugacy classes. ``We're going to hunt them down!'' 

``I'll see you on Thursday, and I expect there'll be fewer people.''


\section{The finite Heisenberg group}
\NewLecture[date=2013-03-14, official=true]
\emph{The professor whistles ``Singin' in the Rain'' while waiting for the lecture to start.}

In its soul, the finite Heisenberg group has captured the essence of calculating something in quantum mechanics. The point of abstraction is to remove all that gets in the way of communing with the thing in and of itself. 
``We're gonna go deep into Plato's cave today.''

``You write commutators to see the level of how much it doesn't commute (\emph{sic}).''

 ``Conjugacy is where it's at, in the language of the 1960s.'' This is what the entire linear algebra lecture is about. For instance, the Jordan normal form gives you a unique representative of a conjugacy class. ``You can dig down here and spend years in this hole. Or you can go up. I'll go up, I'll let you dig down.''

$\matX\DefineAs\Helt 1 0 0$, $\matY\DefineAs\Helt 0 1 0$, $\matZ\DefineAs\Helt 0 0 1$ generate the finite Heisenberg group. With $\matZ=\matX\matY\matX^{-1}\matY^{-1}$, $\matX$, $\matY$ generate it. ``Jacobi?---no, no, no. No Jacobi identity.'' 

The free group on one element is isomorphic to $\tuple{\Z,+}$. The normal subgroup of $\FreeGroup X$ generated by $W$ is the smallest normal subgroup which contains $W$. This is well-defined, as $\FreeGroup X$ is such a normal subgroup, and the intersection of normal subgroups is a normal subgroup, so the normal subgroup generated by $W$ is
\begin{equation*}
  \IntersectionOver {
    \LongDomainSpec{
      \mathllap{N\NormalSubgroup} \mathrlap{\FreeGroup X}\\
      \mathllap{W\Subset} \mathrlap{N}
    }
  } N\text{.}
\end{equation*}
%\begin{supplemental}
Note that this is also equal to subgroup generated by the orbit of $W$ under the action of $\FreeGroup X$ on itself by conjugation, $\LeftConjugationAction g s \DefineAs g s g^{-1}$, so we can write
\begin{equation*}
\GroupPresentation{X}{W} = \QuotientGroup{\FreeGroup X}{\GroupGeneratedBy{\LeftConjugationAction {\FreeGroup X} W}} = \QuotientGroup{\FreeGroup X}{
  \IntersectionOver {
    \LongDomainSpec{
      \mathllap{N\NormalSubgroup} \mathrlap{\FreeGroup X}\\
      \mathllap{W\Subset} \mathrlap{N}
    }
  } N\text{.}
}
\end{equation*}
%\end{supplemental}

``Out of this trivial stuff comes something deep.''

``A faithful representation of a group is just changing variables.''
$\gr$ is a faithful complex irreducible representation of the finite group $G$ if and only if $\sum{g\in G} \abs{\Character[\gr]\of g}^2 = \Cardinality G$.

``I will see those of you who actually do come back next Tuesday.''

\NewLecture[date=2013-03-19, official=true]
The dual group only works for abelian groups. 
The way a bourgeois mathematician thinks of the classical phase space $\Rn\Cartesian\PontryaginDual\Rn$ is as the cotangent bundle $\Rn\Cartesian\Dual{\pa\Rn}$.
``If you would have the energy to learn ancient Greek, and you talked about the real housewives of Atlanta, it would be a waste.'' It is not sufficient to have a fancy language, you actually have to say something.

We started this lecture with the finite Fourier transform. This is a special case of what we discuss here, with $\IntegersModulo{n}$ as the abelian group $A$. This is perfect pedagogy: after only eight months, we come back to the beginning. Everything I've done so far is just that for various groups. When the group is infinite, e.g., the unit circle for Fourier series, you have to do analysis. ``[Claude] doesn't go there. I do.''

\begin{supplemental}
\subsection{\textsc{Supplemental: }\emph{\texorpdfstring{\textgreek{Ἡ φανταχτερή γλῶσσα τῶν ἀστικῶν μαθηματικῶν\footnote{The fancy language of bourgeois mathematicians.}}}{Ἡ φανταχτερή γλῶσσα τῶν ἀστικῶν μαθηματικῶν}}}
\begingroup
\newcommand{\VelocitySpace}{\LieAlgebraSymbol{g}}
\newcommand{\MomentumSpace}{\Dual\VelocitySpace}
\newcommand{\Lagrangian}{\mathscr{L}}
\newcommand{\Hamiltonian}{\mathscr{H}}
\newcommand{\eqrel}{\sim}
Assume the space $G$ of generalised coordinates of a physical system is a Lie group, i.e., a group which is also a differentiable manifold, and in which composition and inversion are smooth maps. Then if $\FunctionSpec\vq \R G$ describes the evolution of the generalised coordinates of the system with time, the generalised velocity at time $t$, $\TimeDerivative\vq\of t$, lies in the tangent space $\TangentSpace{\vq\of t}G$---recall that the tangent space of $g$ at $\vx$ is defined as the set of derivatives\footnote{One has to be careful here: unless $G$ is a submanifold of $\Rn$, the derivative is not well-defined. In that case however, $\TimeDerivative \vgg\of 0$ can be defined as the equivalence class of $\vgg$ in the set of curves through $\vx$ with $\vgg\of 0 = \vx$ under the relation $\vgg_1\eqrel\vgg_2 \Equivalent \pa{f\Compose\vgg_1}\der\of 0 = \pa{f\Compose\vgg_2}\der\of 0$, where $f$ is a differentiable chart of a neighbourhood of $\vx$ onto $\Rn$. It can then be shown that this is independent of $f$.} $\TimeDerivative \vgg\of 0$ at $0$ of smooth curves $\FunctionSpec\vgg\R G$ with $\vgg\of 0 = \vx$.

The ordered pair $\tuple{\vq\of t,\TimeDerivative\vq\of t}$ lies in the \emph{tangent bundle} $\TangentBundle G$, that is,
\begin{equation*}
\tuple{\vq\of t,\TimeDerivative\vq\of t} \in \TangentBundle G \DefineAs \UnionOver{\vx \in G} \set{\vx} \Cartesian \TangentSpace{\vx}G\text{.}
\end{equation*}
Informally, $\TangentBundle G$ is the disjoint union of all the tangent spaces of the manifold $G$. An element $\tuple{\vx,\vv}\in \TangentBundle G$  represents a tangent vector $\vv$ to $G$ at $\vx$. Physically, this is a state of the system, namely the one with coordinates $\vx$ and velocity $\vv$. 

As $G$ is a Lie group, composition on the left---or right---by any element is a diffeomorphism. In particular, for $q\in G$,
$\FunctionSpec {\gj_\vq} G G, \FunctionBody \vx {\vq^{-1} \vx}$ is a diffeomorphism, and so it induces a canonical linear isomorphism $\diffd \gj_\vq \of\vq$ between the tangent spaces $\TangentSpace{\vq}G$ and $\TangentSpace{\gj_\vq\of{\vq}}=\TangentSpace{\Identity}G$, by
\begin{equation*}
\diffd \gj_\vq \of\vq\TimeDerivative\vgg\of 0 = \diffd \gj_\vq \of{\vgg\of 0}\TimeDerivative\vgg\of 0 = \pa{\gj_\vq\Compose\vgg}\der\of 0 = \TimeDerivative\vgg_0\of 0\text{,}
\end{equation*}
where $\TimeDerivative\vgg\of{0}$ is an element of $\TangentSpace{\vq}G$, so $\vgg\of 0 = \vq$ and $\vgg_0 \DefineAs \gj_\vq\Compose\vgg$ verifies $\vgg_0\of 0 = \Identity$, therefore $\TimeDerivative\vgg_0\of0\in\TangentSpace\Identity G \DefinitionOf \VelocitySpace$. By definition, $\VelocitySpace$ is the Lie algebra $\LieAlgebra\of G$ associated with $G$.
It follows from this isomorphism that the tangent bundle of $G$ is \emph{parallelisable}, i.e.,
\begin{equation*}
\TangentBundle G = \UnionOver{\vx \in G} \set{\vx} \Cartesian \TangentSpace{\vx}G \Isomorphic \UnionOver{\vx \in G} \set{\vx} \Cartesian \TangentSpace\Identity G = G \Cartesian \TangentSpace\Identity G = G \Cartesian \VelocitySpace\text{,}
\end{equation*}
where the isomorphism is canonical.
Recall from Hamiltonian mechanics that for a Lagrangian
$\FunctionSpec \Lagrangian {\TangentBundle G \Cartesian \R}{\R}, \FunctionBody {\tuple{\vq,\vv,t}} {\Lagrangian\of{\vq,\vv,t}}$,
the Hamiltonian is defined as the Legendre transform of $\Lagrangian$,
\begin{equation*}
\Hamiltonian \DefineAs \sum i v_i \pderiv{v_i}\Lagrangian - \Lagrangian = \sum i v_i p_i - \Lagrangian\text{,}
\end{equation*}
where $p_i \DefineAs \pderiv{v_i}\Lagrangian$. The \emph{generalised momentum} $\FunctionNamedBody\vp \vv {\sum i v_i p_i}$ is therefore a linear form on the space $\VelocitySpace\owns\vv$. Using $\vq\of t$ as the evolution of the system as above, $\TimeDerivative\vq\of t \in \TangentSpace{\vq\of t}G$, and so $\vp\of t\in \Dual{\pa{\TangentSpace{\vq\of t}G}} \DefinitionOf \CotangentSpace{\vq\of t}G$, the \emph{cotangent space} of $G$ at $\vq$, and the tuples $\tuple{\vq,\vp}$ lie in the \emph{cotangent bundle} $\CotangentBundle G$,
\begin{equation*}
\CotangentBundle G \DefineAs \UnionOver{\vq\in G}\set\vq \Cartesian \CotangentSpace\vq G\text{.}
\end{equation*}
As the tangent spaces of $G$ are canonically isomorphic to each other, so are their duals, so we can write
\begin{equation*}
\CotangentBundle G = G \Cartesian \MomentumSpace\text{.}
\end{equation*}
The dual of the Lie algebra, $\MomentumSpace$, is the momentum space. The Hamiltonian is a function from the cotangent bundle (and time) to the reals, $\FunctionSpec\Hamiltonian {\CotangentBundle G \Cartesian \R = G \Cartesian \MomentumSpace \Cartesian \Reals} \R$.

The Pontryagin dual\footnote{The Pontryagin dual is named after %
\textrussian{Лев Семёнович Понтря́гин} (1908{\slash}1988).} $\PontryaginDual G$ is the set of homomorphisms from $G$ to $\UnitaryGroup\of 1$. Compare this with the vector space dual $\Dual V$, which is the set of linear maps from $V$ to the underlying field. One can see $\PontryaginDual G$ as a sort of non-linear analogue to $\Dual V$. We will see that for $G=\Rn$, we have a meaningful isomorphism between $\PontryaginDual G$ and $\MomentumSpace$. This will also give a motivation for defining the Hamiltonian on the \emph{cotangent} bundle and not on the tangent bundle, answering the question: \emph{why should the generalised momenta not lie in the same space as the velocities?}
\endgroup
\end{supplemental}

\begin{supplemental}

\subsection{\textsc{Supplemental: }Examples of representations from Raisa Galimova's exercise class}
\begingroup
\newcommand{\LeftAction}[2]{f_{#1} \of{#2}}
Let $G$ be a group acting on a measure space\footnote{Here $X$ is a set, $\FunctionSpec \gm X {\intclos 0 \infty}$ is a measure on $X$ and $\gS\Subset\PowerSet X$ is the $\mathrm{\sigma}$-algebra of $\gm$-measurable subsets.} $\tuple{X, \gS, \gm}$ by the left action $\FunctionSpec f G {\Functions X X}, \FunctionBody g {f_g}$, that is, we denote by $\LeftAction g x$ the action of $g\in G$ on $x\in X$, and $f_g \Compose f_h = f_{gh}$. Define the space of square-integrable functions on $X\text{, }
  \Lspace[2]\of X \DefineAs \setst{\gj\in\Functions X \C}{\Lnorm[2] \gj < \infty}$, where $
  \Lnorm[2] \gj \DefineAs \LTwoInner \gj \gj \text{, }
  \LTwoInner \gy \gj \DefineAs \int X \conj\gy \gj \diffd \gm
  $. Note that if $X$ is finite, and $\gm$ is the counting measure,
\begin{align*} 
  \LTwoInner\gy\gj &= \int X \conj\gy\gj \diffd \gm = \sum {x\in X} \conj\gy\of x \gj\of x \text{,}\\
  \forall\gj\in\Functions X \C \quad  \Lnorm[2]\gj &= \int X \abs{\gj}^2 \diffd \gm = \sum {x\in X} \abs{\gj}^2  < \infty \text{,}\\
\intertext{so this is consistent with the definitions from the lecture for a finite group $A$, namely $\Lspace[2]\of A = \Functions A \C$, and $\LTwoInner\gy\gj = \sum {g\in A} \conj\gy\of g \gj\of g$. If $X=\Rn$, and $\gm$ is the Lebesgue measure,}
   \LTwoInner\gy\gj &= \int X \conj\gy\gj \diffd \gm = \int \Rn \conj\gy\of \vx \gj\of\vx\diffd\vx\text{,}\\
   \Lnorm[2]\gj&=\int X \abs{\gj}^2 \diffd \gm = \int \Rn \abs{\gj\of \vx}^2 \diffd\vx \text{,}\\
\end{align*}
and again we recover the definition we gave for the inner product on $\Schwartz\StrictSubset\Lspace[2]\of\Rn$.

Now consider the representation $\FunctionSpec \gr G \GeneralLinearGroup\of{\Lspace[2]\of X}, \FunctionBody g {\gr_g}$ of $G$ on $\Lspace[2]\of X$ defined by $\pa{\gr_g\of\gj}\of x =  \gj\of{\LeftAction g x}$.
\paragraph{Fourier Series and $\UnitSphere 1$.}
If $G=\UnitSphere 1=X$ is the circle group, acting on itself by addition (here we use the definition $\UnitSphere 1 = \QuotientGroup\R {2\Pi\Z}$), $\Lspace[2]\of X = \Lspace[2]\of{\UnitSphere 1}$ decomposes under $\gr$ in a way which is dictated by Fourier series. Indeed, we have for $\gj\in \Lspace[2]\of{\UnitSphere 1}$, $j\in\Z$---see \emph{The Discrete Fourier Transform}, page~8, though note that the inner product was antilinear in the second argument back then, whereas it is antilinear in the first one this semester:
\begin{equation*}
  \ft\gj\of j = \frac{1}{2\Pi}\int 0[2\Pi] \gj\of \gq \E^{-\I j \gq}\diffd \gq = \int {\UnitSphere 1} \gj \E^{-\I j \placeholder}\diffd \gm = \LTwoInner {\E^{\I j \placeholder}} \gj\text,
\end{equation*}
where $\E^{\I j \placeholder}\DefineAs \FunctionBody \gq {\E^{\I j \gq}}$, $\E^{\I j \placeholder} \in \PontryaginDual{\UnitSphere 1} \Isomorphic \Z$ and where we take a normed $\gm$, i.e., $\gm\of{\UnitSphere 1} = 1$. In other words, $\ft{\gj}\of j =  \LTwoInner {\E^{\I j \placeholder}} \gj$ is the projection of $\gj$ onto the subspace $V_j\DefineAs\LinearSpan\set{\E^{\I j \placeholder}}$ of $\Lspace[2]\of{\UnitSphere 1}$. Note that the $\E^{\I j \placeholder}$ are exactly the elements of $\PontryaginDual{\UnitSphere 1}$.
Moreover, $V_j$ is $\gr$-invariant, as $\gr$ acts by translation on $\gj\in \Lspace[2]\of{\UnitSphere 1}$, namely $\pa{\gr_\ga\of\gj}\of \gq = \gj\of{\ga+\gq}$, and hence
$
\forall \gl\E^{\I j \placeholder} \in V_j,\quad
\pa{\gr_\ga\of{\gl\E^{\I j \placeholder}}} =\gl\E^{\I j \pa{\ga+\placeholder}}=\gl\E^{\I j \ga} \E^{\I j \placeholder} \in V_j$.
As it is one-dimensional, $V_j$ is irreducible. We also have, with $\LTwoInner {\E^{\I j \placeholder}}{\E^{\I k \placeholder}}= \ft{\E^{\I k \placeholder}}\of j  = \KroneckerDelta j k$, $V_j \Orthogonal V_k$ for $k\neq j$.
By Parseval's identity, $\sum {j\in\Z}\ft\gj\of j \E^{\I j \placeholder}$ converges absolutely to $\gj$ for all $\gj\in\Lspace[2]\of{\UnitSphere 1}$, so
\begin{equation*} \Lspace[2]\of{\UnitSphere 1}=\DirectSumOver{j\in\Z}
V_j=\DirectSumOver{\gc\in\PontryaginDual{\UnitSphere 1}} \LinearSpan \gc \text{.}
\end{equation*}
To summarise, the translation action $\gr$ is a unitary representation of $\UnitSphere 1$ on $\Lspace[2]\of{\UnitSphere 1}$ with respect to the inner product $\LTwoInner \placeholder\placeholder$, it decomposes in the irreducible representations $V_j$ which are the spans of individual elements $\E^{\I j \placeholder}$ of $\PontryaginDual{\UnitSphere 1}$ and $\ft{\placeholder}\of{j}$ is the projector onto $V_j$. Since $\PontryaginDual {\UnitSphere 1} \Isomorphic \Z$, and the elements of $\PontryaginDual {\UnitSphere 1}$ are the basis onto which functions are projected by the Fourier transform, it makes sense to think of the projector as $\ft{\placeholder}\of{\E^{\I j \placeholder}}$ instead, with $\ft\gj\of{\gc}\DefineAs\LTwoInner\gc\gj$ for $\gc\in\PontryaginDual {\UnitSphere 1}$.
 
\paragraph{The Fourier transform and $\R$.}
If $G=\Rn=X$ acts by addition on itself, the decomposition of $\Lspace[2]\of X = \Lspace[2]\of{\Rn}$ under $\gr$ is described by the Fourier transform. As above, we have for $\gj\in\Lspace[2]\of\Rn, \vk\in\Rn,$
\begin{equation*}
 \ft\gj\of \vk = \ftnrm\int \Rn \gj\of \vx \E^{-\I \scal\vk \vx}\diffd \vx = \int \Rn \gj \E^{-\I \scal\vk{}}\diffd \gm = \LTwoInner {\E^{\I \scal\vk{}}} \gj
\end{equation*}
by taking $\gm = \ftnrm\LebesgueMeasure n$, where $\LebesgueMeasure n$ is the Lebesgue measure on $\Rn$. Again, we see that the $\E^{\I \scal\vk{}}\DefineAs \FunctionBody \vx \E^{\I \scal\vk{\vx}}$ are exactly the elements of $\PontryaginDual\Rn\Isomorphic\Rn$, and that the $V_\vk\DefineAs\LinearSpan\set{\E^{\I \scal\vk{}}}$ are $\gr$-invariant, where $\gr$ acts by translation and are orthogonal. The projector onto $V_\vk$ is  $\ft{\placeholder}\of \vk$. As for $\gj\in\Lspace[2]\of\Rn$,
\begin{equation*} 
\rft{\ft\gj}\of\vx = \ftnrm\int\Rn\ft\gj\of\vk\E^{\I\scal\vk\vx}\diffd\vk \gj\of\vx=
\int\Rn\ft\gj\E^{\I\scal\vx{}}\diffd \PontryaginDual\gm = \gj\of\vx
\end{equation*}
for almost all $x$---$\PontryaginDual\gm$ is the dual measure\footnote{The dual measure $\PontryaginDual\gm$ is the measure we need to get the
  expressions to look the same when written in terms of measures. We chose $\gm$ in such
  a way that $\gm = \PontryaginDual\gm$, but if we had defined a non-unitary version, e.g.
  $\ft\gj\of \vk \DefineAs \int \Rn \gj\of x \E^{-\I \scal\vk \vx}\diffd \vx$, we would  
  need to define the inverse Fourier transform accordingly as
  $\rft\gy\of\vx\DefineAs\sqftnrm \int\Rn\gy\of\vk\E^{\I\scal\vk\vx}\diffd\vx$, i.e., for
  $\gm=\LebesgueMeasure n$, $\PontryaginDual \gm = \sqftnrm \LebesgueMeasure n$.}
---the representation $\gr$ of $\R$ on $\Lspace[2]\of\R$ by translation decomposes in irreducible representations as
\begin{equation*}
  \Lspace[2]\of{\UnitSphere 1}=\DirectSumOver{\vk\in\Rn}
  V_\vk=\DirectSumOver{\gc\in\PontryaginDual{\Rn}} \LinearSpan \gc \text{.}
\end{equation*}
Once again, it makes sense to think of the projectors as  $\ft{\placeholder}\of{\E^{\I \scal\vk{}}}$, with $\ft\gj\of\gc \DefineAs \LTwoInner \gc \gj$ for $\gc\in\PontryaginDual\Rn$, as the isomorphism $\PontryaginDual\Rn\Isomorphic\Rn$ is not unique. Actually, the isomorphism $\FunctionBody \vk {\E^{2 \Pi \I \scal{\vk}{} }}$ is sometimes used, as it allows for $\gm = \PontryaginDual\gm = \LebesgueMeasure n$---there are no constant factors before the integrals. Using this isomorphism, the Gaussian $\E^{-\frac{1}{2}\norm\vx^2}$ is no longer self-dual, i.e., the same as its Fourier transform, but $\E^{-\Pi\norm\vx^2}$ is instead.

This also gives us an isomorphism between the momentum space $\CotangentSpace 0 \Rn = \Dual{\pa\Rn}$ and the Pontryagin dual $\PontryaginDual \Rn$, namely $\FunctionBody \vgl {\E^{2 \Pi \I \vgl }}$, with $\PontryaginDual \Rn\owns\FunctionNamedBody{\E^{2 \Pi \I \vgl}} \vk {\E^{2 \Pi \I \vgl\vk}}$, which, in contrast to the isomorphism from $\Rn$ to $\PontryaginDual\Rn$, does not depend on the choice of an inner product. It therefore makes sense to think of momenta as lying in the dual of the tangent space, and not in the tangent space. Thus the phase space should indeed be $\CotangentBundle \Rn$, and not $\TangentBundle \Rn$.

\paragraph{Spherical harmonics and $\SpecialOrthogonalGroup\of 3$.} 
\newcommand{\degL}{\ell}
\newcommand{\Harmonic}[1]{\mathbf{H}_{#1}}
If $G=\SpecialOrthogonalGroup\of 3$ acts by rotations on $X=\UnitSphere 2$, the representation $\gr$ decomposes as 
\begin{equation*}
\Lspace[2]\of{\UnitSphere 2} = \DirectSumOver{\degL=1}[\infty] \Harmonic\degL\text{,}
\end{equation*}
where the irreducible $\Harmonic\degL$ are defined as follows: $\Harmonic\degL$ is the set of all homogeneous harmonic polynomials of degree $\degL$ on $\UnitSphere 2$, i.e.,
\begin{equation*}
\Harmonic\degL = \setst{\FunctionSpec f {\UnitSphere 2} \C, \FunctionBody {\Transpose{\tuple{x,y,z}}} {f\of{x,y,z}}}{f\of{x,y,z} = \sum {\ga+\gb+\gg = \degL} c_{\ga\,\gb\,\gg} x^{\ga}y^{\gb}z^{\gg} \And \Laplacian f = 0  }\text{.}
\end{equation*}
\endgroup
\end{supplemental}
\NewLecture[date=2013-03-21, official=true]
\begingroup
\newcommand{\vsqrtninv}{\frac{1}{\sqrt{\vn}}}
The dual of a group is a sort of nonlinear version of the dual of a vector space.

We switched from an additive to a multiplicative notation. ``Algebraists... you have to be careful. They're sensitive.''

``Representation theory is like a bubble on Wall Street.'' We have, obviously, $\matT  \vsqrtninv = \vsqrtninv$. ``Does nobody know about Elmo?''  Now, by taking this eigenvector and applying $\matM$ over and over again---in a perfectly legal manner---we get all the other eigenvectors! All the representation theory, all the modules, all the things they talk about... it's just this. ``[Claude's] entire thesis is just a super-duper version of this.'' You don't have to do anything, you just talk. This is the French influence.

``We're going to come to the point where you see the difference between [Claude] and me.'' We will need not one, but \emph{two} proofs for the Stone-Von Neumann theorem.

``Why am I not happy [about Proof \textsc{i}] whereas he is happy? [...] I'm only going to talk about mathematical reasons.'' Man cannot live on algebra alone. What happens as $n$ goes to infinity? The proof doesn't converge. He doesn't care. 

The problem is phase transitions: the symmetries change, and you cannot see that with algebra. For instance, for superconductivity, you have to have enough stuff. You cannot just have an explicit $n$. 
``Suddenly, we're going to go back to the Fourier transform.'' 

\noindent
``That's what's good about these algebraists. They're tenacious.''

``We have a new Pope now. You cannot confuse the Father, the Son and the Holy Ghost. If you do, you are going to get in trouble. If you confuse eigenvectors and eigenvalues, you're also going to get into trouble. Not the same kind of trouble though.''

``If you don't come back next week I understand.''
\endgroup

\section{All the eigenfunctions of the Fourier transform}
\NewLecture[date=2013-03-26]
Recall: 
\begin{align*}
\FT\of\gj\of\vk\DefineAs\ft\gj\of\vk&\DefineAs\ftnrm\int\Rn\gj\of\vx\E^{-\I\scal{\vk}{\vx}}\diffd\vx\text{,} &&\gj\in\Schwartz\text,\\
\RFT\of\gy\of\vx\DefineAs\rft\gy\of\vx&\DefineAs\ftnrm\int\Rn\gy\of\vx\E^{\I\scal{\vk}{\vx}}\diffd\vk\text{,} &&\gy\in\Schwartz\text,\\
\FT\RFT &=\RFT\FT=\Identity\text,\\
\FT{\pa{\derivop[2]x + x^2}}\of{k}&=\pa{\derivop[2]x + x^2}\FT\of\gj\of{k}\text.
\end{align*}
So we have this very nice guy here: $\derivop[2]x + x^2$, and having gone to \foreign{\textgerman{Elementarschule}}, you'd think \[\derivop[2]x + x^2 \stackrel{?}= \pa{x-\derivop x}\pa{x+\derivop x}\text{.}\] But the \emph{real} identity they should teach you at school---life is short, the sooner you get going, the better---is \[\pa{a-b}\pa{a+b}=a^2-b^2+\commutator a b\text{.}\] That's the difference between childhood and adulthood: $ab$ is no longer equal to $ba$, and it causes a lot of problems.
Here $\commutator {\derivop x} x = \Identity$. We now know  $ \pa{x-\derivop x}\pa{x+\derivop x} = -\derivop[2]x + x^2 -1$.
This will be around long after strings are seen as a strange cult of the 21st century.

We defined:
\begin{align*}
\opA&\DefineAs\frac{1}{\sqrt{2}}\pa{x+\derivop x}\text,\\
\opAdag&\DefineAs\frac{1}{\sqrt{2}}\pa{x-\derivop x}\text.
\end{align*}
We wrote ``dagger''. This makes sense, as;
\begin{align*}
\LTwoInner \gy{\opA\gj} &= \frac{1}{\sqrt{2}}\int\R\conj{\gy}\of x \pa{\derivop x+x}\gj\of x \diffd x\\
&= \frac{1}{\sqrt{2}}\int\R\conj{\gy}\of x \derivop x\gj\of x \diffd x +
\frac{1}{\sqrt{2}}\int\R\conj{\pa{x\gy\of x}} \gj\of x \diffd x\\
&=\int\R\conj{\pa{\frac{1}{\sqrt{2}}\pa{-\derivop x + x}\gy\of x}} \gj\of x \diffd x \\
&=\LTwoInner {\opAdag \gy}\gj\text.
\end{align*}
Write 
\begin{align*}
\opA_x&\DefineAs\frac{1}{\sqrt{2}}\pa{x+\derivop x}\text,\\
\adj{\opA_x}&\DefineAs\frac{1}{\sqrt{2}}\pa{x-\derivop x}\text{,}
\end{align*}
and similarly for $k$ instead of $x$.
Observe:
\begin{align*}
\FT\of {\opA_x \gj}\of k &= \I \opA_k \ft\gj\of k\text,\\
\FT\of {\adj{\opA_x} \gj}\of k &= \I \adj{\opA_k} \ft\gj\of k\text,\\
\opA_x\E^{-\frac{1}{2}x^2}&=\pa{\derivop x+x}\E^{-\frac{1}{2}x^2}= 0\text{.}\\
\intertext{define}
h_0\of x &\DefineAs \frac{\E^{-\frac{1}{2}x^2}}{\sqrt[4]{\Pi}}\\
\end{align*}
so that $\LTwoInner {h_0}{h_0} = 1$.
We can now perform the following induction:
\begin{align*}
\opA h_0&=0\text, \\
\ft{h_0}&= h_0\text,\\
\ft{\adj {\opA_x} h_0}\of k &=\pa{-\I}\adj{\opA_k} h_0\of k\text{ using the vital lemma,}\\
\FT\of{\pa{\adj{\opA_x}}^n h_0}\of k &= \pa{-\I} \adj{\opA_k}\FT\of{\pa{\adj \opA_x}^{n-1}h_0}\of k
= \pa{-\I}^n \pa{\adj{\opA_k}}^n h_0\of k\text.
\end{align*}
In other words, the $\pa{\opAdag}^n h_0$---with an $\opAdag$, not an $\opA$: ``a dagger can be the difference between life and death!''---are eigenfunctions of the the Fourier transform. We will need to prove that these are orthogonal and form a complete set, and we will have an orthogonal basis of $\SchwartzSpace\of\R$.
``I'll do some coherent states. I like coherent states.'' Next time we shall prove:
\begin{align*}
\commutator A {\adj A} &= \Identity \text,\\
\commutator A {\pa{\adj A}^n} &= n \pa{\adj A}^{n-1} \text.
\end{align*}
This is the only real idea in group representations: you find some commutation relations, and you generate all the eigenvectors and eigenvalues.

\NewLecture[date=2013-03-28]
We also define $\opH \DefineAs \opAdag \opA$, $h_n\of  x \DefineAs \pa{\opAdag}^n h_0$ for $n\geq 1$. We have $h_n\in \SchwartzSpace\of\R$ for $n\geq 0$. $\opA$, $\opAdag$ and $\opH$ all map $\SchwartzSpace\of\R$ to itself.
Now we compute:
\begin{align*}
2\commutator \opA \opAdag \gj &= 2 \opA\opAdag\gj- 2 \opAdag\opA\gj\\
&=\pa{\derivop x + x}\pa{-\derivop x + x}\gj - \pa{-\derivop x + x}\pa{\derivop x + x}\gj\\
&=\pa{\derivop x + x}\pa{\gj\der + x\gj} - \pa{-\derivop x + x}\pa{\gj\der + x\gj}\\
&= \pa{\gj\dder + \pa{x\gj}\der + \pa{-x\gj\der}+x^2\gj}
 - \pa{\gj\dder - \pa{x\gj}\der + x\gj\der+x^2\gj}\\
 &= 2\pa{x\gj}\der - 2 x\gj\der = 2\gj
\end{align*}
This yields the following proposition.
\begin{proposition}
\begin{align}
\commutator \opA \opAdag &= \Identity \\
\label{comm2}
\commutator \opA {\pa{\opAdag}^n} &= n\pa{\opAdag}^{n-1}\text, &&n\geq 1\text,\\
\ft{h_n}&=\pa{-\I}^n h_n\text,&&n\geq 0\text,\\
\label{orth4}
\LTwoInner {h_m}{h_n} &= \KroneckerDelta m n\text,\\
\label{creation5}
\opA h_n &= \sqrt n h_{n-1}\text,\\
\opAdag h_n &= \sqrt{n+1} h_{n+1} \text, \\
\label{Hh07}
\opH h_n &= n h_n\text, && n\geq 0\text,\\
h_n&\in\SchwartzSpace\of\R\text, && n\geq 0\text,\\
\ft{\opA_x \gj}&= \I \opA_k \ft\gj\text,\\
\ft{\adj{\opA_x} \gj}&= \pa{-\I} \adj{\opA_k} \ft\gj\text.
\end{align}
\begin{proof}
Proof of (\ref{comm2}) by induction:
\begin{align*}
  \commutator \opA \opAdag &= \Identity\text, && (n=1)\text,\\
  \commutator \opA {\pa{\opAdag}^{n+1}} &= \opA \pa{\opAdag}^{n+1} - \pa{\opAdag}^{n+1} \opA\\
  &= \opA \pa{\opAdag}^{n+1} - \opAdag \pa{\opA \pa{\opAdag}^n - n \pa{\opAdag}^{n-1}} \\
  &= \opA \pa{\opAdag}^{n+1} - \opAdag \opA \pa{\opAdag}^n + n \pa{\opAdag}^n
\end{align*}
  ``Ah, there must be an easier way to show this, it can't be that hard. Let's just reboot this argument:''
\begin{alignat*}{2}
&& \opA\pa{\opAdag}^n - \pa{\opAdag}^n\opA &= n \pa{\opAdag}^{n-1}\\
\Implies&& \opA\pa{\opAdag}^{n+1} - \pa{\opAdag}^n \opA \opAdag &= n \pa{\opAdag}^n \\
\Implies&& \opA \pa{\opAdag}^{n+1} - \pa{\opAdag}^n \pa{\Identity + \opAdag\opA}&= n \pa{\opAdag}^n \\
\Implies&& \opA \pa{\opAdag}^{n+1} - \pa{\opAdag}^{n+1} \opA - \pa{\opAdag}^{n} &= n \pa{\opAdag}^n \text.
\end{alignat*}
Proof of (\ref{orth4}). Now we are going to do something quickly, where algebra really shines: you do nothing but apply the same basic identities over and over, yet get something useful out. Let us assume $m>n\geq 0$, because we already did the case where both are $0$.
\begin{align*}
\LTwoInner{h_m}{h_n} &= \frac{1}{\sqrt{\Factorial m}} \frac{1}{\sqrt{\Factorial n}} \LTwoInner{\pa{\opAdag}^{m} h_0}{\pa{\opAdag}^{n} h_0} \\
&= \frac{1}{\sqrt{\Factorial m}} \frac{1}{\sqrt{\Factorial n}}\LTwoInner{\pa{\opAdag}^{m-1} h_0}{\opA\pa{\opAdag}^{n} h_0}\\
&= \frac{1}{\sqrt{\Factorial m}} \frac{1}{\sqrt{\Factorial n}}\LTwoInner{\pa{\opAdag}^{m-1} h_0}{\pa{\pa{\opAdag}^{n}\opA + n\pa{\opAdag}^{n-1}} h_0 }\\
&= \frac{1}{\sqrt{\Factorial m}} \frac{n}{\sqrt{\Factorial n}} 
  \LTwoInner{\pa{\opAdag}^{m-1} h_0}{\pa{\opAdag}^{n-1} h_0}\\
&= \frac{1}{\sqrt{\Factorial m}} 
  \frac{n\pa{n-1}}{\sqrt{\Factorial n}} 
  \LTwoInner{\pa{\opAdag}^{m-2} h_0}{\pa{\opAdag}^{n-2} h_0}\\
  &= \dotsb = \frac{1}{\sqrt{\Factorial m}} 
  \frac{n\pa{n-1}\dotsm 1}{\sqrt{\Factorial n}} 
  \LTwoInner{\pa{\opAdag}^{m-n} h_0}{h_0}\text.
\end{align*}
As $\opAdag$ kills $h_0$---it is called the \emph{annihilation} operator, in German the \foreign{\textgerman{Vernichtungsoperator}}---we get $
  \LTwoInner{h_m}{h_n} =
  \frac{1}{\sqrt{\Factorial m}} 
  \frac{n\pa{n-1}\dotsm 1}{\sqrt{\Factorial n}} 
  \LTwoInner{0}{h_0}=0$ if $m>n$. If $m=n$, we have $
  \LTwoInner{h_n}{h_n} =
  \frac{1}{\sqrt{\Factorial n}} 
  \frac{n\pa{n-1}\dotsm 1}{\sqrt{\Factorial n}} 
  \LTwoInner{h_0}{h_0}=1$.  
  In other words, $\set{h_n}$ is an orthogonal family.

  Proof of (\ref{creation5}):
  \begin{align*}
  \opA h_n &= \frac{1}{\sqrt{\Factorial n}} \opA\pa{\opAdag}^n h_0 \\
  &= \frac{1}{\sqrt{\Factorial n}}\pa{\pa{\opAdag}^n\opA + n \pa{\opAdag}^{n-1}}h_0\\
  &= \frac{n}{\sqrt{\Factorial n}} \pa{\opAdag}^{n-1} h_0 = \sqrt{n} h_{n-1}\text.
  \end{align*}
  The other ones are even easier; I'm not doing them, you are getting bored.

  Now for something fun, the proof of (\ref{Hh07}). First, note that
  \begin{align*}
    \opH h_0 &= \opAdag \opA h_0 = 0\text.
  \end{align*}
  With what we already have, we can complete the proof elegantly---but then, I have always maintained that elegance is for tailors:
 \begin{align*}
   \opH h_n &= \opAdag \pa{\opA h_n} = \opAdag \sqrt{n} h_{n-1} = \sqrt{n} \sqrt{n} h_n = n h_n\text.
 \end{align*}
\end{proof}
\end{proposition}
Now, ladies and gentlemen, your jaws should be dropping. We have these beautiful identities:
\begin{align*}
  \ft h_n &= \pa{-\I}^n h_n \text, && n \geq 0 \text,\\
  \opH h_n &= n\, h_n\text, && n \geq 0 \text.
\end{align*}
So, ``out of nowhere'' we now have eigenvectors with eigenvalues $0$, $1$, $2$, etc.!

``Whenever I think of algebra, I think about Lawrence of Arabia. It's clean.'' \emph{The professor asks if anyone in the audience has actually heard about T. E. Lawrence, and goes on to discuss how the David Lean film features a scene widely known in the film world as an example for the match cut technique.} ``If you are really interested, I can just tell you about film and skip this [points to the blackboard] boring crap. I know something about it.'' \emph{At this point the lecture has extended five minutes into the break, and people are beginning to leave.}
``You should leave the moment you feel lied to. That's a basic thing in all human relationships.''

Now, what would make this even better? Let us show that we are done, that we have all the eigenvectors---in other words, that our orthogonal family is a basis. ``That, is, you can't get away from it. You can run, but you can't get away.''

\begin{proposition}
If $\gj\in\SchwartzSpace\of\R$ and $\LTwoInner \gj {h_n} = 0$, $n\geq 0$, then $\gj\Identically 0$.
\begin{proof}
We have $x=\frac{\opA + \opAdag}{\sqrt{2}}$.
We have:
\begin{align*}
\LTwoInner \gj {x h_0} 
&= \frac{1}{\sqrt{2}}\LTwoInner \gj {\opA h_0} + \frac{1}{\sqrt 2} \LTwoInner \gj {\opAdag h_0}\\
&=\frac{1}{\sqrt{2}}\LTwoInner \gj {h_1} = 0 \text.
\end{align*}
What do we do? We only need one word: Repeat.
\begin{align*}
\LTwoInner \gj {x^2 h_0} 
&= \frac{1}{2}\LTwoInner \gj {\pa{\opA+\opAdag}^2 h_0} \\
&= \frac{1}{2}\LTwoInner \gj {\pa{\opA \opA + \opAdag \opA + \opA  \opAdag + \opAdag \opAdag}h_0}\\
&= \frac{1}{2}\LTwoInner \gj {\opA \opAdag h_0 + \opAdag \opAdag h_0 }\\
&= \frac{1}{2}\LTwoInner \gj {\pa{\opAdag \opA +\Identity} h_0 + \opAdag h_1}\\
&= \frac{1}{2} \LTwoInner \gj {h_0 + \sqrt{2} h_2} = 0\text.
\end{align*}
And now? Again: Repeat.
\begin{align*}
\LTwoInner \gj {x^3 h_0} 
&= \pa{\frac{1}{\sqrt 2}}^3 \LTwoInner \gj {\pa{\opA+\opAdag}^3 h_0} \\
&= \dotsb = 0\text.
\end{align*}
By induction, we can show $\LTwoInner \gj {x^n h_0} = 0 $ for $n\geq0$. It follows 
\begin{align*}
\sum{n=0}[\infty] \frac{\pa{-\I k}^n}{\Factorial n} \LTwoInner\gj {x^n h_0} &=0\\
\Implies \LTwoInner \gj {\pa{\sum{n=0}[\infty]\frac{\pa{-\I k}^n}{\Factorial n}}h_0} &=0\text,&& \forall k\in\R\text.
\end{align*}
which, if we look carefully, we can recognize as a Fourier transform:
\begin{align*}
0 &= \LTwoInner\gj {\E^{-\I k x}h_0}\\
  &= \sqrt{2\Pi}\frac{1}{\sqrt{2\Pi}} \int\R \conj \gj \of x h_0\of x \E^{-\I k x}\diffd x\text, && \forall k\in\R\text.
\end{align*}
Note that $\conj \gj \of x h_0\of x\in \SchwartzSpace\of\R$. It follows:
\begin{alignat*}{3}
         & \forall k\in \R \quad & \ft{\conj \gj  h_0}\of k &= 0\\
\Implies & \forall x\in \R \quad &\conj \gj\of x  h_0\of x &= 0 && \quad \text{(as shown previously)}\\
\Implies & \forall x\in \R \quad & \gj \of x &=0 \text. && \quad \text{($h_0(x) \neq 0\text, \forall x\in\R$)}
\end{alignat*}
So $\gj\Identically 0$.
\end{proof}
\end{proposition}

We now know that the $h_n$ form a basis of $\SchwartzSpace\of\R$. But we are greedy. We really want to write
\[
\gy\in\SchwartzSpace\of\R \stackrel{?}{=} \sum{n=0}[\infty] \LTwoInner\gy {h_n} h_n\of x \DefineAs \lim_{\conv N \infty} \sum{n=0}[N] \LTwoInner\gy {h_n} h_n\of x\text.
\]
It is exceedingly boring to prove this, I'm sure you don't have the patience. But there is something I \emph{can} do:
\begin{align*}
\abs{\LTwoInner \gy {h_n}}
&= \abs{\int\R \conj\gy\of x h_n\of x \diffd x}\\
&\leq \int\R\abs{\conj\gy\of x} \abs{h_n\of x}\diffd x\\
&= \int\R\abs{\conj\gy\of x} h_n\of x \diffd x\\
&\leq \Lnorm[2]{\gy} \underbrace{\Lnorm[2]{h_n}}_{= 1}\text.
\end{align*}
But in fact, we can do better:
\begin{align*}
\LTwoInner\gy{h_n}
&= \frac{1}{n^p}\LTwoInner \gy {\opH^p h_n}\\
&= \frac{1}{n^p}\LTwoInner {\opH^p \gy} {h_n}\text.\\
\end{align*}
So,
\begin{align*}
\abs{\LTwoInner \gy {h_n}}
&\leq \frac{1}{n^p} \abs{\LTwoInner \gy {\opH^p h_n}}\\
&\leq \frac{1}{n^p} \Lnorm[2]{\opH^p \gy}\text.
\end{align*}

Concluding, we have the following
\begin{lemma}
  If $\gy \in \SchwartzSpace\of\R$, then $\abs{\LTwoInner \gy {h_n}} \leq \frac{\Lnorm[2]{\opH^p \gy}}{n^p}$, $n \geq 1$, $p \geq 0$.
\end{lemma}

\newcommand{\selt}{\mathscr{s}}
\newcommand{\vecs}{\mathbfscr{s}}
\NewLecture*[date=2013-04-09]
\begin{proposition}\LectureStartsHere The $h_n\of x$, $n\geq 0$ form an orthonormal basis of $\SchwartzSpace\of\R$. More precisely, let \[\SchwartzSpace\of\N \DefineAs \setst{\tuple{\selt_0,\selt_1,\selt_2,\dotsc}}{\selt_j \in \C \text{ and } \forall p, \exists c_p, \abs{\selt_j} \leq c_p \frac{1}{\pa{1 + j}^p}}\text.\]
Then,
\begin{align*}
    \MapSpecBody
    {\SchwartzSpace\of \N}
    {\SchwartzSpace\of\R}
    {\vecs}%TODO:use script s, that's what he did.
    {\sum{n=0}[\infty] \selt_n h_n\of x}
\end{align*}
is a continuous, bijective linear map with a continuous inverse.
\end{proposition}
``Whatever somebody means by an orthonormal basis for an infinite dimensional vetor space, this is about as good as it gets.'' In proving the theorem, which is an exercise, we will have to do something like \[ \abs{\sum{n \geq 0} \selt_n h_n\of x} \leq \sum{n \geq 0} \abs{\selt_n} \abs{h_n\of x} \]---and remember, the macho waits as long as possible before bringing the $\abs{\placeholder}$ in. Compare this to what we had for Fourier series:
\[
  \abs{\sum{j=-\infty}[\infty] \ft{\gj}\of j \E^{\I j x}} \leq \sum{j=-\infty}[\infty] \abs{\ft{\gj}\of j} \underbrace{\abs{\E^{\I j x}}}_{=1}
\]
For the $h_n$, we are not so lucky, and actually have to estimate $sup_{x\in \R} \abs{h_n\of x}$.
\begin{align*}
\abs{h_n\of x} &= \abs{\pa{-\I}^{-n} \ft{h_n}\of x} \\
           &\leq \frac{1}{\sqrt{2\Pi}}\int{-\infty}[\infty] 1 \abs{h_n\of y}\diffd  y \\
           &= \frac{1}{\sqrt{2\Pi}}\int{-\infty}[\infty] \frac{1}{\sqrt{1+y^2}} \sqrt{1+y^2} \abs{h_n\of y}\diffd  y \\
           &\leq \frac{1}{\sqrt{2\Pi}}\pa{\int{-\infty}[\infty]{\frac{1}{1+y^2}\diffd y}}^{\frac{1}{2}}\pa{\int{-\infty}[\infty]{\pa{1+y^2}h_n^2\of y\diffd y}}^{\frac{1}{2}} \\
           &= \frac{1}{\sqrt{2\Pi}}\pa{\int{-\infty}[\infty]{\frac{1}{1+y^2}\diffd y}}^{\frac{1}{2}}\pa{\LTwoInner{h_n}{\pa{1+y^2}h_n}}^{\frac{1}{2}} \\
           &=\frac{1}{\sqrt{2\Pi}}\pa{\int{-\infty}[\infty]{\frac{1}{1+y^2}\diffd y}}^{\frac{1}{2}}\pa{\LTwoInner{h_n}{h_n} + \frac{1}{2} \LTwoInner{h_n}{\pa{\opA+\opAdag}^2 h_n}}^{\frac{1}{2}}\\
           &\leq \frac{1}{\sqrt{2\Pi}}\underbrace{\pa{\int{-\infty}[\infty]{\frac{1}{1+y^2}\diffd y}}^{\frac{1}{2}}}_{\mathclap{< \infty \text{, indepedent of }n}} \sqrt{2\pa{n+1}}
\end{align*}
\section{Quantising the harmonic oscillator}
``Now, we come into the good stuff''. Once upon a time, there was a classical Hamiltonian:
\[
  h\of{q,p} \DefineAs \frac{1}{2} \pa{\frac{p^2}{m}+m\gw^2 q^2}
\]
Do you remember anything from classical mechanics?
``So, Thursday: from the classical to the quantum world and beyond.''

\subsection{Classical Mechanics}
\NewLecture[date=2013-04-11]
What's classical mechanics? We start with a number $n\geq 1$, the number of \emph{degrees of freedom}. It does not change while you keep discussing that system. Then, we have a set $\gW \Subset \Rn$, open and connected, called the \emph{classical configuration space}.
For instance, we can consider a pendulum, with $q\in\gW = \R$ the angle. Or we could have a joint in the pendulum, yielding $\tuple{q_1,q_2}\in\gW=\R^2$.
\marginfig[The configuration space $\gW=\R$ of a simple pendulum. Here $q\in\R$, as the pendulum can do a full turn---or several---around its pivot.]{\FigureSimplePendulumConfigurationSpace}
%\marginpar{\vspace*{1.5cm}}
\marginfig[The configuration space $\gW=\R^2$ of a double pendulum, where $\vq=\tuple{q_1,q_2}\in\R^2$.]{\FigureDoublePendulumConfigurationSpace}
\marginpar{\vspace*{3cm}}
\begin{definition}[Classical phase space] The \emph{classical phase space} is defined as $\gW \Cartesian \Rn$, where $\tuple{\vq,\vp}\in\gW \Cartesian \Rn$.
\end{definition}
``If you don't measure anything, then you're doing string theory, and your argument is: there must be supersymmetries, because it's so beautiful.''
\begin{definition}[Classical observables] The set of \emph{classical observables} is the algebra of smooth, real valued functions on $\gW\Cartesian\Rn$, namely $\Continf\of{\gW\Cartesian\Rn, \R} \text.$
\end{definition}
An example of a classical observable is
\begin{align*}
q_i\in\Continf\of{\gW\Cartesian\Rn, \R}\\
\FunctionBody{\gW\Cartesian\Rn\owns\tuple{\vq,\vp}}{q_i\in\R}\text.
\end{align*}
Somebody complained about complex-valued functions not being real enough, so we use real-valued functions.
``You could ask me: why smooth?---Why smooth?'' You can't measure something that isn't regular. ``No human being can work with something other than a polynomial anyway.''
\begin{definition}[Poisson bracket]
Finally, define the \emph{Poisson bracket} of $\gy\text{, }\gj$ as
\[
\Poisson{\gy}{\gj}\of{\vq,\vp} \DefineAs \sum{j=1}[n]{\pderiv {q_j}\gy \pderiv {p_j} \gj-\pderiv {p_j}\gy \pderiv {q_j}\gj}
\text,
\]
for $\gy\text{, }\gj\in\Continf\of{\gW\Cartesian\Rn}$.
\end{definition}

\begin{proposition}[\foreign{\textgerman{Ur}}-something of classical mechanics]
$\tuple{\Continf\of{\gW,\Rn},\Poisson{\placeholder}{\placeholder}}$ is a Lie algebra,
\foreign{\textgerman{das heißt, }} $\Poisson{\placeholder}{\placeholder}$ is bilinear, skew-symmetric and satisfies the Jacobi identity.
\end{proposition}
\begin{definition}[Classical mechanical system]
A \emph{classical mechanical system} on $\gW \Cartesian \Rn$ is a (Hamiltonian) total energy function \[h\of{\vq,\vp}\in \Continf\of{\gW\Cartesian\Rn}\text.\]
\end{definition}
\begin{definition}[Evolution of a classical observable] The evolution in time $f$ of an observable $\gy\in\Continf\of{\gW\Cartesian\Rn}$, given at $t=0$ by $f\of{\vq,\vp,0}=\gy\of{\vq,\vp}$, is determined by the \emph{Liouville equation} \[\derivop t f = \Poisson f h\text.\]
\end{definition}
The conservation of energy immediately follows:\[
\derivop t h = \Poisson h h = 0\text.
\]
This is all of classical mechanics. We can certainly ``pimp'' the theoretical structure behind it further by introducing symplectic manifolds, Poisson algebras, and so on. But so far, this could have all been done by mathematicians. Where does the physicist come in? The art and the genius lies in finding the correct $h$. In doing so, at least when describing a real system you see around you, you more or less establish a law of nature.

Starting from this theoretical perspective, as opposed to doing a history course and starting with Newton and $F = ma$, could be described as the modern view, because it brings the symmetries of the theory to the front. From what we have, we can now derive Hamilton's equations of motion:
\begin{align*}
\derivop t q_j &= \Poisson{q_j}{h} \\
&=\sum{k=1}[n]\pa{\pderivop{q_k} q_j}\pderiv {p_k} h - \pa{\pderivop{q_k} p_j}\pderiv {q_k} h \\
&=\pderiv {p_j} h\\
\pderivop t p_j &= - \pderiv {q_j} h\text.
\end{align*}

How can we now describe a ``natural'' transformation between two phase spaces? If the map
\begin{align*}
\FunctionActionSpecBody
{\vs}
{\gW\Cartesian\Rn}
{\Closure\gW\Cartesian\Rn}
{\pa{\Pullback \vs \gy}\of{\vq,\vp}}
{\gy\of{\vs\of{\vq,\vp}}\in\Continf\of{\gW\Cartesian\Rn}}
\end{align*} verifies
\begin{align*}
\gy\in\Continf\of{\Closure\gW\Cartesian\Rn}\\
\Poisson{\Pullback s \gy}{\Pullback s \gj}\of{\vq, \vp} = \Poisson{\gy}{\gj}\of{s\of{\vq, \vp}} = \Pullback s \Poisson{\gy}{\gj} && \forall \gy, \gj \in \Continf\of{\Closure\gW\Cartesian\Rn}\text,
\end{align*}
then it is called \emph{symplectic}---or ``canonical'' in 19th century math slang. Why is this important? It allows us to define when two systems, i.e., two hamiltonians and the associated phase spaces, are physically equivalent.

Let me just show you one thing. $\vs$ is a symplectic isomorphism if and only if $\diffd \vs = \tuple{\pderiv {x_k} {s_j}}$ satisfies
\[
\scal{
\pascal{\diffd \vs \of{\vq,\vp}\vv}}{\pascal{\matJ\diffd \vs \of{\vq,\vp}\vw }} = 
\scal{\vv}{\pascal{\matJ \vw}}\text{, where }
\matJ\DefineAs\begin{pmatrix}
\nullmat & \Identity_n\\
-\Identity_n & \nullmat
\end{pmatrix}\text,
\]
for all $\vv\in\Rn$, $\vw\in\Rn$ and $\tuple{\vq,\vp}\in\gW\Cartesian\Rn$.
\begin{definition}[Symplectic group]
And if we really want to, we can now also define the group of \emph{symplectic matrices} $\SymplecticGroup\of{2n, \R} \DefineAs \setst{\matA \in \GeneralLinearGroup\of{2n, \R}}{\Transpose{\matA} = - \matJ \matA^{-1} \matJ}$.
\end{definition}
% The standard notation is Sp(2n, R). What do we do here?
Note that ``it's even in the Heisenberg stuff'', as $\SymplecticGroup\of{2, \R} = \SpecialLinearGroup\of{2, \R}$. % This is true, but what does it have to do with the Heisenberg group? There seems to be something in Wikipedia about Heisenberg groups and symplectic vector spaces.
With this definition, the above characterisation of a symplectic isomorphism can be reformulated to $\diffd \vs \in \SymplecticGroup\of{2n, \R}$.

What is the simplest possible motion? Let's take an $h_0$ that only depends on $\vp$, not on $\vq$. In that case we have
\begin{alignat*}{3}
\TimeDerivative q_j &= \pderiv{p_j}{h_0}\of\vp &\Implies&& \vq &= \deriv{\vp}{h_0}t + \vq\of{0} \\
\TimeDerivative p_j &= \pderiv{q_j}{h_0}\of\vp = 0 &\Implies && \vp &= \vp\of{0} \\
\vv &\DefineAs \TimeDerivative \vq = \pderiv p h
\end{alignat*}
That's a straight line. There's nothing simpler than a straight line, except no line at all---but I'm not going to get into discussions of nothingness, though it's fashionable these days. There are entire books written about nothingness, and how string theory explains everything about nothingness...

Now suppose somebody over there in Oberwinterthur has a fancy Hamiltonian $h\of{\vq,\vp}$. How do you know it's not a straight line, and that you are not being charged non-straight line prices for a straight line? You want to pull back, to find $\vs$ such that $\Pullback \vs h = h_0$. You heard about the Hamilton-Jacobi equation, right? And probably your mind went fuzzy at this time, and you heard about maps, and maybe generating functions, and this and that... 
There is an $\vs$ if and only if $\setst{\gj}{\Poisson\gj h = 0}$ has an Abelian subalgebra $\LieAlgebraSymbol A$ of dimension $n$ such that
\begin{align*}
\LieAlgebraSymbol A &= \set{I_1,\dotsc,I_n}\text, \\
\Poisson{I_j}{I_k}&= 0\text,\\
\Poisson{I_j}{h}&=0\text.
\end{align*}
\
Setting things up like this, the logical structure is crystal clear. This is something a mathematician could do. You may say, as a physicist, ``frankly, my dear, I don't give a damn''\footnote{\emph{Gone with the Wind}, 1939, directed by Victor Fleming.}, but the reason this structure is interesting is because we have beautiful examples. So, if mathematicians are coming up with all different kinds of beautiful structures, the question is---and I am putting this in very low terms---``Are they not just playing with themselves?'' With this you can predict where Pluto or Neptune is. String theory has predicted nothing, and it may never.

In the case $h\of{q,p}=\frac{1}{2}\pa{\frac{p^2}{n} + m\gw^2 q^2}$, where $\tuple{q,p}\in\R\Cartesian\R$, we have the harmonic oscillator.
And that's classical mechanics. Now on to quantum mechanics.

\subsection{Quantum Mechanics}
\begingroup%QM
\newcommand{\QuantumObservables}{\StandardSymbol{QO}}
How do we start? There is a number $n\geq 1$ of \emph{quantum degrees of freedom}.
Then, $\gW\Subset \Rn$ is the \emph{quantum configuration space}, like the classical configuration space above. If you had fallen asleep at the beginning of the previous hour, and you thought \textsc{q.m.} was just a misspelling of classical mechanics, you might think the lecture had just started. Now it becomes different.
\begin{definition}[Quantum phase space]
The \emph{quantum phase space} is $\Schwartz$.
\end{definition}
Now we need an algebra of quantum observables.
\begin{definition}[Quantum observables] The \emph{quantum observables} are the elements of \[
\QuantumObservables\DefineAs\setst{\opA\in\Endomorphisms\of\Schwartz}{
\begin{array}{l}
\text{$A$ is a continuous linear map such that}\\
 \forall\gy, \gj \in \Schwartz, \LTwoInner{\opA\gy}{\gj} = \LTwoInner{\gy}{\opA\gj}
\end{array}
}\text.
\]
\end{definition}
Let me give you two examples of observables: 
\begin{align*}
\pa{\opQ_j\gy}\of\vq &\DefineAs q_j\gy\of\vq\text, \\
\pa{\opP_j\gy}\of\vq &\DefineAs \frac{\ReducedPlanck}{\I} \pderivop{q_j}\text.
\end{align*}
\begin{proposition}
$\tuple{\QuantumObservables, \frac  \I \ReducedPlanck \commutator \opA \opB}$ is a Lie algebra.
\begin{proof} We check that $\QuantumObservables$ is closed under the bracket $\frac \I \ReducedPlanck \commutator\placeholder\placeholder$. Let $\opA${, }$\opB$ be self-adjoint. Then
\begin{align*}
\adj{\pa{\frac \I \ReducedPlanck \commutator\opA\opB}}
&= -\frac \I \ReducedPlanck\adj{\pa{\opA\opB-\opB\opA}}\\
&= -\frac \I \ReducedPlanck\pa{\adj\opB\adj\opA-\adj\opA\adj\opB}\\
&= -\frac \I \ReducedPlanck\pa{\opB\opA-\opA\opB} = \frac \I \ReducedPlanck \commutator\opA\opB\text.
\end{align*}
You can check the Jacobi identity and all that.
\end{proof}
\end{proposition}
\begin{definition}[Quantum system]
A \emph{quantum system} is an $\opH\in\QuantumObservables$.
\end{definition}
\begin{definition}[Evolution of a quantum observable] The evolution of the observable $\opA$ is determined by \[\TimeDerivative A= \frac \I \ReducedPlanck \commutator\opA\opH\text.\] % TODO(eggrobin): this should be an operator, but accents appear in weird places over operators...
\end{definition}
``That's it. That's quantum mechanics.''

\NewLecture[date=2013-04-16]
Okay! So where were we, ladies and gentlemen? When I last saw you, we talked about quantum mechanics. We have: 
\begin{gather*}
\tuple{
\underbrace{n}_{\mathclap{\substack{\uparrow\\\text{degrees of}\\\text{freedom}}}},
\overbrace{\Rn}^{\mathclap{\substack{\text{configuration space}\\\downarrow}}},
\underbrace{\tuple{\Schwartz, \LTwoInner\placeholder\placeholder}}_{\text{classical phase space}},
\overbrace{\HermitianOperators\of{\Schwartz}}^{\text{quantum observables}},
\underbrace{\frac{\I}{\ReducedPlanck}\commutator\placeholder\placeholder}_{\mathclap{\text{Lie algebra structure}}}
}\text, \\
\derivop t \opA = \frac{\I}{\ReducedPlanck}\commutator \opA \opH
\end{gather*}
We're doing this the French way. This is a 5-tuple, the way Bourbaki would write it. To them, mathematics is just long strings of symbols.

Such a logical structure is of no interest if you can't find $\opH$, unless there is a way of picking $\opH$s that describe quantum systems, not just because you believe they do, but because they do actually predict things that you can verify. It's like finding Waldo\footnote{This is a reference to Martin Handford's \emph{Where's Wally?}, a series of children's books in which a character (Wally) is hidden on every page, published in the United Kingdom from 1987 onwards. It was published as \emph{Where's Waldo?} in the United States, and as \emph{\textgerman{Wo ist Walter?}} in Germany.}. Unless you can find Waldo, it's just a meaningless complicated picture. Finding Waldo gives it its meaning. Actually let's call the Hamiltonian $\opW$\footnote{This notation was---fortunately---not actually used during the lecture, so we stick to $\opH$.}. Finding Waldo is \emph{not} the Bourbaki way of doing things. I'll show you two books any physicist will tell you about: \emph{Find classical Waldo}, and \emph{Find quantum Waldo}. \emph{Find classical Waldo} you already had a lecture about last semester.

Recall the two quantum observables from the last lecture.
\begin{definition}[Momentum and position operators]
\begin{alignat*}{2}
\pa{\opP_k \gj}\of \vq &\DefineAs \frac{\ReducedPlanck}{\I} \pderivop {q_k} \gj\of\vq && \text{ is the \emph{momentum operator},} \\
\pa{\opQ_k \gj}\of \vq &\DefineAs q_k \gj\of\vq &&\text{ is the \emph{position operator}.}
\end{alignat*}
\begin{lemma}These operators have the following properties:
\begin{align*}
\opP_k\text{, }\opQ_k&\in\HermitianOperators\of\Schwartz \\
\frac{\I}{\ReducedPlanck}\commutator{\opP_k}{\opP_l} &= 0\text, \\
\frac{\I}{\ReducedPlanck}\commutator{\opQ_k}{\opQ_l} &= 0\text, \\
\frac{\I}{\ReducedPlanck}\commutator{\opP_k}{\opQ_l} &= \KroneckerDelta k l \Identity\text.
\end{align*}
\begin{proof}
The cases where the commutator is $0$ are obvious. We prove the last equality,
\begin{align*}
\frac{\I}{\ReducedPlanck}\commutator
{\frac\ReducedPlanck\I\derivop q}
{q} \gj
&= \derivop q \of{q\gj} - q \derivop q \gj \\
&= \gj = \Identity \gj\text.
\end{align*}
\end{proof}
\end{lemma}
\end{definition}
Now we do something \emph{really} weird. We find Waldo in the classical case---this may be hard, but we find Waldo hiding amidst the electric and magnetic fields---and then we quantise him. Namely, for a classical Waldo $h\of{\vq,\vp}$, replace any $q_k$ by $Q_k$ and any $p_k$ by $P_k$.
\[
h\of{\vq,\vp} \xrightarrow{\text{Quantise Waldo}} \opH\of{\opQ_1,\dotsc,\opQ_n, \opP_1, \dotsc, \opP_n}
\]
If you believe that... you'll believe anything.
As an example of quantisation, consider the following:
\[
h\of{\vq,\vp} = \frac{1}{2m} \vp^2 + V\of\vq 
\Implies
\opH\of{\opvQ, \opvP} = \frac{1}{2m} \sum{j=1}[n]\opP_j^2 + V\of{\opQ_1,\dotsc,\opQ_n}\text,
\]
where we have for a continuous function $f$ on $\R$ and an operator $\opA$ with spectral decomposition $\opA = \sum{k=1}[r]\gl_k \opgP_k$,
\[
f\of\opA = \sum{k=1}[r] f\of{\gl_k} \opgP_k\text.
\]
Why? This is obvious if $f$ is a polynomial, because $\opgP_k\opgP_l = \KroneckerDelta k l \opgP_k$. Recall the Weierstrass approximation theorem: any continuous function can be approximated by polynomials, and the approximation is uniform, so by taking the limit, $f\of\opA = \sum{k=1}[r] f\of{\gl_k} \opgP_k$.

This works because we have a strict Apartheid between the $\opP$s and $\opQ$s. They have to sit on different sides of the bus.
For instance, look at the harmonic oscillator:
\begin{align*}
h\of{p,q}&= \frac{1}{2}\pa{\frac{p^2}{m}+m\gw^2 q^2}\\
\Implies
\opH\of{Q, P} &= \frac{1}{2}\pa{-\frac{\ReducedPlanck^2}{m}\derivop[2] q + m^2 \gw^2 q^2}\text.
\end{align*}
Note that there are no $qp$ terms in the classical Hamiltonian.
But if we do not have this Apartheid in the classical Hamiltonian, if Waldo lives in the United States after the civil rights movement---or even after Obama was elected---we may have $q_i p_j$ in the Hamiltonian---say with $\vq \Exterior \vp = \smash{\Transpose{\tuple{q_2 p_3 - q_3 p_2, q_3 p_1 - q_1 p_3, q_1 p_2 - q_2 p_1}}}$. And then this commutes in the classical case, but not in the quantum case, so we have a problem: do we write $\opQ_i \opP_j$ or $\opP_j \opQ_i$? Somehow, if you have a physically interesting Hamiltonian, it just happens that you can fiddle with it to get rid of those $q_i p_j$. This may sound like a ridiculous story like \foreign{\textgerman{Samichlaus}}, but it works. That's all I can tell you. Do you know about Pauling? He got the Nobel Peace Prize---and also the Nobel Prize in Chemistry for something he did on the side. Well Pauling used that sort of bullshit to calculate the angle in the water molecule, and he got the right answer.

So do you understand the point? There is a logical structure which is neat, and there is this weird recipe to quantise Waldo, and it works. I can't tell you more, and nobody can. Actually, there is a slightly more satisfying recipe, using these $\opW$s---hey, more Waldos!---these Weyl operators we talked about. We'll come back to this later. It's a messy story, and it doesn't even have a good end. Somehow or other, quantum mechanics and gravity don't mix. There was a time when string theory was supposed to quantise gravity. Twenty years ago there was a long list of all the thing string theory was supposed to do. Now the list looks very different, and it's much shorter; none of the things that used to be on that list are on it anymore. String theory has changed its \foreign{\textlatin{curriculum vitae}}.


\NewLecture[date=2013-04-18]
\newcommand{\redH}{\operatorname{\mathit{\textcolor{red}{H}}}}
The last time I saw you we were looking for Waldo. We're going to look for Waldo some more. We were looking at
\begin{align*}
h\of{p,q} &\DefineAs \frac{1}{2}\pa{\frac{p^2}{m}+m\gw^2q^2}\text,\\
\opH=h\of{\frac{\ReducedPlanck}{\I}\derivop q, q} &= \frac{1}{2}\pa{\frac{1}{m}\pa{\frac{\ReducedPlanck}{\I}\derivop q}^2+m\gw^2 q^2}\\
&=\frac{1}{2}\pa{-\frac{\ReducedPlanck^2}{m}\derivop[2] q + m\gw^2 q^2}\text.
\end{align*}
Now we look for the eigenvalues $E$, $\opH\gy = E \gy \text{, for }\gy\in\SchwartzSpace\of\R$.

The easiest case is when we have this strict Apartheid between $q$ and $p$, and of course this does not happen all that much in real life---thank God for that.
Write
\begin{align*}
x&\DefineAs \pa{\frac{m\gw}{\ReducedPlanck}}^{\frac{1}{2}} q\text,
\intertext{we get}
\derivop x = \deriv x q \derivop q &= \pa{\frac{\ReducedPlanck}{m\gw}}^{\frac{1}{2}} \derivop q\text.
\intertext{We now expand the eigenvalue equation from above,}
\pa{\opH \gy}\of q = E\gy\of q  &\Equivalent -\frac{\ReducedPlanck^2}{2m} \derivop[2] q \gy\of q + \frac{m\gw^2}{2}q^2 \gy\of q
=E\gy\of q\text.
\intertext{Let}
f\of x \DefineAs \gy\of{\pa{\frac{\ReducedPlanck}{m\gw}}^{\frac{1}{2}}x}\text.
\intertext{The eigenvalue equation becomes}
\frac{1}{2}\pa{-\gw\ReducedPlanck\derivop[2] x + \gw \ReducedPlanck x^2}f\of x
&= E f\of x\text,\\
\frac{1}{2} \pa{-\derivop[2] x + x^2}f\of x &= \frac{E}{\ReducedPlanck\gw}f\of x\text.
\end{align*}
``There is one more thing that I should do to bring us back to the nostalgic world of our childhood. There is one more thing that I should do, and all will be right in the world. What do I need to do?'' \emph{Somebody\footnote{He will henceforth be referred to as `the poet'.} answers with a complicated circumlocution for `subtract one'.} You should write novels. This is like Percy Bysshe Shelley when he talks about a skylark:
\renewcommand{\poemtoc}{paragraph}
\poemtitle{To a Skylark}
\settowidth{\versewidth}{That from heaven or near it}
\begin{verse}[\versewidth]
{\large\textinitial{H}}\textsc{ail} to thee, blithe spirit!\\
Bird thou never wert---\\
That from heaven or near it\\
Pourest thy full heart\\
In profuse strains of unpremeditated art.

[...]
\begin{flushright}
---Percy Bysshe Shelley.
\end{flushright}
\end{verse}
I just say `it's a skylark'. Here I just want the formula.
We add a $-1$ in the last expression:
\begin{align*}
\frac{1}{2} \pa{-\derivop[2] x + x^2 - 1}f\of x &= \pa{\frac{E}{\ReducedPlanck\gw}-\frac{1}{2}}f\of x\text.
\intertext{We rewrite that as}
\redH f \of x &= \gl f \of x\text{, where }\gl = \frac{E}{\ReducedPlanck\gw}-\frac{1}{2}\text.
\end{align*}
The red $\redH$ is the one you know---I ran out of $\opH$s. The fact that I picked red has no political overtones whatsoever. This is Switzerland, red's okay. It's on the flag. %TODO: novel stuff. This was hilarious.
Recall:
\begin{align*}
\redH h_n &= n h_n\text{, where}\\
h_n &= \frac{1}{\sqrt{\Factorial n}}\pa{\opAdag}^n h_0\text,\\
h_0\of x &= \frac{1}{\sqrt[4]{\Pi}} \E^{-\frac{1}{2}x^2}\text.
\intertext{So here,}
n &= \frac{E}{\ReducedPlanck\gw}-\frac{1}{2}\text,\\
E_n &= \ReducedPlanck\gw \pa{n+\frac{1}{2}}\text,
\end{align*}
and we have found the energy levels of the quantum harmonic oscillator. \emph{The poet asks whether this has something to do with blackbody radiation}. ``Most physicists aren't even aware that they're lying'', but you \emph{really} need quantum field theory for that.

Let us look at the wavefunction $\gj$. It's normalised so that
\begin{align*}
\int\R \abs{\gy\of q}^2 \diffd q &= 1\text,
\intertext{and we interpret this as the probability}
\Probability[\gy]{a<q<b}&=\int{a}[b] \abs{\gy\of q}^2 \diffd q\text.
\intertext{There's no reason why it should work, but it does.
The other thing is}
\Probability[\gy]{a<k<b}&=\int{a}[b] \abs{\ft\gy\of k}^2 \diffd q\text.
\intertext{What is the Heisenberg uncertainty principle? Define}
\ExpectedValue[\gy]{\opA}&\DefineAs\LTwoInner{\gy}{\opA\gy}\text,\\
\Variance[\gy]{\opA}&\DefineAs\ExpectedValue[\gy]{\pa{\opA-\ExpectedValue[\gy]{\opA}\Identity}^2}\text.
\intertext{Here's the punchline:}
\Variance[\gy]{\opQ}\Variance[\gy]{\opP}&\geq\frac{\ReducedPlanck^2}{4}\text.
\end{align*} %TODO:CHECK
What does this mean? The tinier the variance, the more the expected value is expected, so the more you know about $\opQ$, the less you know about $\opP$. I'm not going to prove this, you'll do it in quantum mechanics next year.

I'm now going to show you a more rational way of quantising Waldo. This is still a story, but while---unless you're really gullible---you stopped believing in \foreign{\textgerman{Samichlaus}} at the age of 4. This you might believe until 10.

\marginnote{\emph{From here onwards there are official notes for the material covered in the lecture.}}
\emph{Somebody asks about the dominated convergence theorem, which is used in the proof of irreducibility of $\gr_h$.}
It's just you, right? No, he wants to do that too. So next Tuesday, I'll bite the bullet and we'll talk about the Lebesgue dominated convergence theorem.

The biggest mistake people do when they decide to do research is to read a book. That's just as bad as jumping into a crevice, except if you fall into a crevice, some Austrian might find you in 3000 years and you'll end up in a museum. If you read a book, you're really finished.
\emph{The professor being extremely sleep-deprived, he ends the lecture 25~minutes early, leading to widespread cheering and applause.}


\section{Matrix Lie groups and Lie algebras}
\newcommand{\LAsutwo}{\LieAlgebraSymbol{su}\of 2}%
\newcommand{\LAgl}{\LieAlgebraSymbol{gl}}
\newcommand{\Field}{\mathbb{K}}
\NewLecture[date=2013-04-23]
You've talked at length about $\smash{\Matrices n \R \Isomorphic \R^{n^2}}$, $\smash{\Matrices n \C \Isomorphic \C^{n^2}}$. $n\times n$ matrices on $\Field$ are just elements of $\smash{\Field^{n^2}}$. The only reason we write them as square arrays is to equip these spaces with a---polynomial---multiplication,\[
\tuplespec{\matA \matB}{ij} \DefineAs \sum{k=1}[n]A_{ij}B_{ij}\text.
\]
Manifolds are just $\R^d$ or $\C^d$ if what you are doing has anything to do with reality.

Let $\Field\in\set{\R,\C}$. We have the continuous \emph{determinant} function,
\[
\FunctionSpec{\Determinant}{\Matrices n \Field}{\Field}\text.
\]
Define\begin{align*}
\GeneralLinearGroup\of{n,\Field} &\DefineAs \setst{\matg\in\Matrices n \Field}{\Determinant \matg \neq 0}\\
&=\Matrices n \Field \setminus \underbrace{\setst{\matg\in\Matrices n \Field}{\Determinant \matg = 0}}_{\mathclap{\text{closed as $\Determinant$ is continuous and $\set{0}$ is closed.}}}\\
\end{align*}
$\GeneralLinearGroup\of{n,\Field}$ is an open subset of $\smash{\Field^{n^2}}$. Now let us look at the multiplication and inversion.
\begin{align*}
\MapSpecBody[\Continf\text{ map}]
{\GeneralLinearGroup\of{n,\Field}\Cartesian\GeneralLinearGroup\of{n,\Field}}
{\GeneralLinearGroup\of{n,\Field}}
{\tuple{\matg,\matrh}}
{\matg\matrh}\\
\MapSpecBody[\Continf\text{ map}]
{\GeneralLinearGroup\of{n,\Field}}
{\GeneralLinearGroup\of{n,\Field}}
{\matg}
{\tuplespec{\frac{1}{\Determinant \matg}\pa{\pa{-1}^{i+j}\matA_{ij}\of{\matg}}}{\mathrlap{\tuple{i,j}\in\set{1,\dotsc,n}}}}\text,
\end{align*}
where $\matA_{ij}\of{\matg}$ is the determinant  of $\matg$ with the $i$th row and $j$th column cut out.
Thus $\tuple{\GeneralLinearGroup\of{n,\Field},\text{matrix multiplication}}$ is a group with smooth operations. What you do when you talk about Lie groups is the same, except you start with completely abstract manifolds. As physicists, it's unlikely that you'll work with Lie groups that are not very closely related to the following concept.
\begin{definition}[Matrix Lie group] A matrix Lie group $G$ is a closed subgroup of $\GeneralLinearGroup\of{n,\Field}$ under the topology generated by the norm\[
\FrobeniusNorm\matg \DefineAs \sqrt{\Trace\matg\adj\matg} = \pa{\sum{i,j=1}[n]\abs{g_{ij}}^2}^{\frac{1}{2}}\text.
\]
\end{definition}
You already know an example of one from last week, the continuous Heisenberg group\[
\HeisenbergGroup[n]=\setst{\tuple{
\begin{array}{c|ccc|c}
1&r_1&\dotsc&r_n&t\\
\hline
&&&&s_1\\
\nullvec&&\Identity_n&&\vdots\\
&&&&s_n\\
\hline
0&&\nullvec&&1
\end{array}
}}{\begin{array}{l}\vr,\vs\in\Rn,\\ t\in\R\end{array}}\text.
\]
Why is it closed? Look at the map\[
\FunctionBody{\R^{\pa{n+2}^2}\Superset\Helt \vr \vs t} {\tuple{r_1,\dotsc,r_n,s_1,\dotsc,s_n,t}\in\R^{2n+1}}\text.
\]
Do I have to prove that it's continuous? 
\emph{The poet:}---Isn't it open?---``Well, that's the problem with education. Does anyone remember the definition of connected?'' \emph{The poet answers with the definition of path-connected}.---``There was another definition, one with open sets and so on. What are the subsets of $\Rn$ that are both open and closed?'' Somebody else:---The half open, half closed intervals. \emph{The professor whistles ``La Vie en rose''}.---``Sorry, nothing personal. I had to wait for my blood pressure to go down.''

An open set $\gW$ of $\Rn$ is a set such that every point is contained in an open ball inside $\gW$. A closed set of  $\Rn$ is a set in which every Cauchy sequence converges. Here our group verifies both, so it is both open and closed in $\R^{\pa{n+2}^2}$, and thus it is connected.

Now we look at a Lie group at the very bottom of the food chain. This is the plankton in the sea of algebraic objects. The reason we talk about it though is that to understand spin, you have to eat plankton sandwiches with $\SpecialUnitaryGroup\of{2}$ inside. And you have to look at the representations. That's what we need this representation crap for.
\begin{definition}[Special unitary group of degree $2$] We define the group of unitary $2\times2$ matrices with determinant $1$,
\[
\GeneralLinearGroup\of{2,\C}\Superset\SpecialUnitaryGroup\of{2}\DefineAs\setst{\matg \in \GeneralLinearGroup\of{2,\C}}{\adj\matg\matg=\Identity, \Determinant\of\matg = 1}\text.
\]
\end{definition}
It is a subgroup of $\GeneralLinearGroup\of{2,\C}$, as 
\begin{align*}
\matg\matrh\in\SpecialUnitaryGroup\of 2 &\Implies
\adj{\pa{\matg\matrh}}\matg\matrh = \adj\matrh\adj\matg\matg\matrh=\Identity\\
&\Implies\matg\matrh\in\SpecialUnitaryGroup\of 2\text,\\
\matg\in\SpecialUnitaryGroup\of 2 &\Implies \adj\matg\matg = \Identity\\
&\Implies \matg^{-1} = \adj\matg\\
&\Implies \adj{\pa{\matg^{-1}}}=\matg\\
&\Implies \adj{\pa{\matg^{-1}}}\matg^{-1}=\Identity\\
&\Implies \matg^{-1}\in\SpecialUnitaryGroup\of{2}\text.
\end{align*}
It is closed, as $\FunctionBody \matrh {\adj\matrh}$ is a continuous function, and thus 
\begin{alignat*}{2}
&&\lim_{\conv j \infty} \matg_j &= \matg\\
\Implies&& \lim_{\conv j \infty} \adj{\matg_j} &= \adj\matg\\
\Implies&& \lim_{\conv j \infty} \adj{\matg_j}\matg_j &= \adj\matg\matg = \Identity\text,
\end{alignat*}
and the determinant is a continuous function too.
So you have a nice closed subgroup of $\GeneralLinearGroup\of{2,\C}$ looking at you. Now I don't know how you feel about things looking at you, but it's looking at you.

``Do you remember what a compact set is?''---Any cover by open sets has a finite subcover.---``This is correct, but with this definition you have to look at all open covers, and check whether they have a finite subcover, and we don't have time for that. There is a nicer property, named after two Germans---actually it was one German and one Frenchman\footnote{This is the Heine-Borel theorem, after Heinrich Eduard Heine (1828/1881) and Félix Édouard Justin Émile Borel (1871/1956). Note that in France this is called the \foreign{\textfrench{théorème de Borel-Lebesgue}} after the aforementioned Borel and an additional Frenchman, Henri Léon Lebesgue (1875/1941). We have found no Russians associated with this theorem, but interestingly, the theorem is called \foreign{\textrussian{лемма Гейне --- Бореля}} there, so it is only a lemma. Many mathematicians have actually contributed to this theorem, so taking those mentioned by \emph{Wikipedia}, it could be called the Borel-Cousin-Dirichlet-Heine-Pincherle-Lebesgue-Schoenflies-Weierstrass theorem---that's 1 Italian, 3 French, 3 Prussians and Dirichlet, who was born in Napoleonic France in 1805 and became \foreign{\textlatin{de facto}} Prussian in 1815. Do not confuse this Heine with Christian Johann Heinrich Heine (1797/1856), another poet.}.'' The poet answers ``closed and bounded''\footnote{This is true in $\Rn$. For a general metric space, replace ``closed and bounded'' by ``complete and totally bounded''.}, though he got the answer from someone else.---``You should be a novelist. You know how to incorporate ideas from others''. So $\SpecialUnitaryGroup\of{2}$ is a compact matrix Lie group, as \[
\matg\in\SpecialUnitaryGroup\of 2 \Implies \FrobeniusNorm\matg = \sqrt{\Trace \matg\adj\matg} = \sqrt{\Trace \Identity_2} = \sqrt{2}\text.
\]

The fundamental reason we study compact sets is that it's like being finite without being finite. For instance you can extract a finite subcover from any open cover.

Now, after an hour of being yelled at and patronised, and all sorts of other wonderful things,  we now have a compact matrix Lie group. We could have picked the circle group, but this has a nice property that the circle group doesn't have. \emph{The poet:}---The circle group is abelian, this one isn't.---``You see? leave it to a poet''.

\NewLecture[date=2013-04-25]
Okay, so we have $\SpecialUnitaryGroup\of 2 \Subset \GeneralLinearGroup\of{2,\C}$, which is a compact matrix group. We inflicted upon you all sorts of things about %WHAT
finite groups. What is it about these groups that make them richer? Let's look at $\GeneralLinearGroup\of{n,\Field}\Subset\Matrices n \Field \Isomorphic \Field^{n^2}$. Now one thing that you've learned about is paths.
\begin{definition}[Smooth path]
A smooth path in $\GeneralLinearGroup\of{n,\Field}$ is a smooth map\[\FunctionSpec\matgg{\tuple{a,b}}{\GeneralLinearGroup\of{n,\Field}}\text.\]
\end{definition}
\begin{definition}[Tangent vector]
A tangent vector $\matX$ to the matrix Lie group $G$  at $\Identity\in G$ is \[\matX=\Evaluate{\derivop t \matgg}{t=0}\] for some smooth path $\matgg\of t\in G$, $a<t<b$ and $\matgg\of 0 = \Identity$. %TODO appropriate symbol types.
\end{definition}
\marginfig[A complex thing that you can't visualise.\label{ComplexThing}]{\FigureComplexThingThatYouCantVisualise}
The idea is the following. The group is a complex thing that you can't visualise, so I draw a complex thing that you can't visualise (Figure~\ref{ComplexThing}). You study it locally instead, by looking at the tangent space. You pick a curve lying in $G$, and as it lies in $G$---which it can do---it lies in $\GeneralLinearGroup\of{n,\Field}$, so you can differentiate it,\[
\derivop t \matg = \tuple{\derivop t \gg_{ij}\of t}\in\Matrices n \Field \text.
\]
We now define the Lie algebra of $G$.
\begin{definition}[Lie algebra of a matrix Lie group] Lie algebra of $G$ is its tangent space at the identity,
\[
\LieAlgebra\of G \DefineAs \setst{\matX\in\Matrices n \Field}{\text{$\matX$ tangent to $G$ at $\Identity$}}\text.
\]
\end{definition}
The reason mathematicians like this is that everything you know, linear algebra, geometry, analysis---actually you don't need much analysis for this---conspire, legally, to make this object.
Now we have a definition, so we see how it applies to our example $\SpecialUnitaryGroup \of 2$.
Consider a path in $\SpecialUnitaryGroup \of 2$,
%TODO \Identity_2 should be \Identity[2].
\begin{align*}
\FunctionSpec*
{\matgg}
{\intopen{-\ge}{\ge}}
{\SpecialUnitaryGroup\of 2}\text,\\
\matgg\of 0 &= \Identity_2\text,\numberthis\label{NonlinearCondition}\\
\matgg\of t &=
\begin{pmatrix}
\gg_{11}\of t & \gg_{12}\of t \\
\gg_{21}\of t & \gg_{22}\of t
\end{pmatrix}&&\text{for $-\ge<t<\ge$\text.}
\intertext{As the path lies in $\SpecialUnitaryGroup \of 2$,}
\Identity_2 &= \matgg\of{t}\adj{\matgg\of{t}}\text,\\
\nullmat &= \derivop t\of{\matgg\of t \adj{\matgg\of t}} && \text{---`Who do you gotta call?'---Leibniz.}\\
&= \pa{\derivop t \matgg\of t}\adj{\matgg\of t} + \matgg\of t \pa{\derivop t\of{\adj{\matgg\of t}}}\span\omit\span\omit\\
&= \TimeDerivative\matgg\of t \adj{\matgg\of t} + \matgg\of t \adj{\pa{\derivop t \matgg\of t}}\text.\span\omit\span\omit
\intertext{In particular, at $t=0$,}
\begin{split}
\nullmat &= \TimeDerivative\matgg\of 0 \adj{\matgg\of 0} + \matgg\of 0 \adj{\TimeDerivative\matgg\of 0}\\
&=\TimeDerivative\matgg\of 0 + \adj{\TimeDerivative\matgg\of 0}\text.
\end{split}&&\parbox{.4\textwidth}
{---`What's $\matgg\of 0$?'---Zero.---`Zero! I understand poetic license, I like poetic license, but saying a one is a zero is too much poetic license.'}
\end{align*}
So we have shown
\begin{equation}
\matX\in\LieAlgebra\of{\SpecialUnitaryGroup\of 2} \Implies \matX + \adj\matX = 0 \label{InfinitesimalVersion}
\end{equation}
\emph{The professor scrambles for a pointing stick in order to show the equalities he wants to refer to.} ``They removed it, they didn't want me to threaten you with it---ah! here it is.'' This (\ref{NonlinearCondition}) is not a linear condition, \emph{this} (\ref{InfinitesimalVersion}) is the infinitesimal version of that---and that's the fundamental idea in calculus: you take a nonlinear condition and you approximate it linearly. We have another nonlinear condition:
\begin{equation}
1 = \Determinant \matgg\of t \Implies 0 = \derivop t \Determinant \matgg\of t\text.\label{DeterminantZero}
\end{equation}
For the moment, don't be a mathematician. You don't need to know how to differentiate determinants. You just differentiate this one, like a physicist---or a not so bourgeois and pedantic mathematician. A mathematician, at least one with a certain kind of education, would stop dead here. They'd develop a whole new theory of differentiating matrices, with their own journal, and they'd have conjectures, and prove them, and go on to prove things nobody wants to hear about.
\begin{align*}
0 &= \derivop t \Determinant \matgg\of t\\
&=\derivop t \of{\gg_{11}\of t \gg_{22}\of t - \gg_{21}\of t \gg_{12}\of t} && \text{---`Who do you gotta call?'}\\
&=\TimeDerivative\gg_{11}\of t \gg_{22}\of t +\gg_{11}\of t \TimeDerivative\gg_{22}\of t && \text{---Leibniz.}\\
&\quad - \TimeDerivative\gg_{12}\of t \gg_{21}\of t - \gg_{12}\of t \TimeDerivative\gg_{21}\of t\text.
\intertext{At $t=0$,}
0 &= \Evaluate{\derivop t \Determinant \matgg\of t}{t=0} = \TimeDerivative\gg_{11}\of t +\TimeDerivative\gg_{22}\of t\text,\span\omit
\end{align*}
So the infinitesimal version of (\ref{DeterminantZero})is \[
\Trace \TimeDerivative\matgg\of 0 = 0\text.
\]
These two conditions on the tangent vectors motivate the following definition.
\begin{definition}We call $\LAsutwo$ the set of skew-Hermitian $2\times2$ complex matrices with trace $0$,\[
\LAsutwo\DefineAs\setst{\matX\in\Matrices 2 \C}{\matX+\adj\matX=0, \Trace\matX=0}\text.
\]
\end{definition}
So far we have shown
\[
\matX\in\LieAlgebra\of{\SpecialUnitaryGroup\of 2}\Implies
\matX\in\LAsutwo\text.
\]
But we don't know if there \emph{are} any tangent vectors yet. We haven't looked at any actual smooth curves.
---``How do we find smooth curves?''---Exponentiate.

If you tell me you have never heard about this I'll call you all liars, because when I tried to do the properties of the exponential map last semester you told me you had already seen that. I know, last semester was a long time ago, you don't remember anything.

Speaking of not remembering anything. Two Harvard professors---there are lots of Harvard professors, but I'm only going to talk about two of them---published a paper\footnote{Carmen Reinhart and Kenneth Rogoff, Growth in a Time of Debt, \emph{in American Economic
Review: Papers \& Proceedings}, 100.} saying that if your debt reaches 90~\% of your \textsc{gdp}, your economy would slow down by 0.1~\%---to show that there is no gender bias, one was of the masculine persuasion, and the other one was of the feminine persuasion. This has been used to justify austerity policies all over. A graduate student\footnote{Thomas Herndon, Political Economy Research Institute, University of Massachusetts Amherst.} in econometrics---to economists, that's like street sweeping---couldn't reproduce the results, and asked them some of their data. So they sent their Excel spreadsheet---that's mathematics for economists---and the student noticed basic arithmetic mistakes, and that Australia, Tasmania and New Zealand\footnote{The list of excluded countries is actually Denmark, Canada, Belgium, Austria and Australia.} were left out---and minus 0.1~\% became plus 2.2~\%. \emph{You} could have done that too. Things are the same in mathematics and physics. Maybe I'll give you a list of danger signs when picking who to write a paper with. If they have been doing the same things for the past 15~years, \foreign{\textgerman{isch nöt guet!}}  Every time you publish a paper in one of the top journals in economics---up to 4 papers---your salary goes up by 25000~Swiss~francs. It doesn't matter whether it's wrong. The science behind that is ``not worth a tub of hot spit''\footnote{This is a reference to a quote by John Nance Garner on the vice-presidency of the United States. The original quote is: ``not worth a bucket of warm piss''.}.---Do we have to take a break?

As the gentleman said here, ``exponentiate''. Let $\matX\in \LAsutwo$. We have\begin{align*}
\matX+\adj\matX &= \nullmat\text,\\
\Trace\matX&=0\text.
\end{align*}
\emph{Pointing to the poet}---``You told me you had done the properties of the exponential map.''---So it's my fault.---``We've got to have a fall guy. It's like \emph{the Maltese Falcon}\footnote{\emph{The Maltese Falcon}, 1941, directed by John Huston.}.''
\begin{align*}
\E^\matA&=\sum{k=0}[\infty]\frac{\matA^k}{\Factorial k}\text, \\
\E^{-\matA}&=\pa{\E^\matA}^{-1}\text.
\intertext{Define the path}
\matgg_\matX\of t &\DefineAs \E^{t\matX}\text.
\intertext{We have, with $\matX\in\LAsutwo$,}
\adj{\matgg_\matX\of t} &= \adj{\pa{\E^{t\matX}}}=\E^{\adj{\pa{t\matX}}}\\
&=\E^{-t\matX}=\pa{\E^{t\matX}}^{-1}=\pa{\matgg_\matX\of t}^{-1}\text,\\
\Determinant \matgg_\matX\of t &=\Determinant \E^{t\matX}= \E^{\Trace t\matX}\\
&= \E^{t\Trace\matX}=1\text.
\end{align*}
So we have the following \textsc{fact}:$
\forall t \in \R\quad
\forall \matX \in \LAsutwo\quad
\matgg_\matX\of t \in \SpecialUnitaryGroup\of 2$.
Moreover, $\matgg_\matX\of 0 = \Identity$, so\[
\matX = \Evaluate{\derivop t \matgg_\matX}{t=0}\in \LieAlgebra\of{\SpecialUnitaryGroup\of 2}\text.\]
We have proven the following proposition.
\begin{proposition}The Lie algebra of $\SpecialUnitaryGroup\of 2$ is the set of skew-Hermitian $2\times 2$ complex matrices with trace $0$,\[
\LieAlgebra\of{\SpecialUnitaryGroup\of 2} = \LAsutwo\text.
\]
\end{proposition}
I have called $\LAsutwo$ a ``Lie algebra''. Where's the bracket? It's the commutator.
\begin{proposition}
$\tuple{\LAsutwo,\commutator\placeholder\placeholder}$ is a Lie algebra.
\begin{proof}
 Let $\matX$, $\matY\in\LAsutwo$.
\begin{align*}
\Trace\commutator \matX \matY &= \Trace \matX\matY - \Trace \matY\matX = 0\text.\\
\adj{\commutator\matX\matY} &= \adj{\pa{\matX\matY-\matY\matX}}\\
&= \adj\matY\adj\matX-\adj\matX\adj\matY\\
&=\pa{-\matY}\pa{-\matX}-\pa{-\matX}\pa{-\matY}\\
&=\commutator\matY\matX=-\commutator\matX\matY\text.
\end{align*}
\end{proof}
\end{proposition} 

[The poet] just proved it's a real vector space, without even knowing it. Consider two tangent vectors $\TimeDerivative\matgg\of 0$ and $\TimeDerivative\matgg'\of 0$ at $\Identity$. Then we can construct a smooth path whose tangent vector at $\Identity$ is any real linear combination of those. $\matgg\of{at}$ just means that we go slower or faster along the path $\matgg$. ---`Who do you gotta call?'---Chain rule; but it's not a name.---`Just call it Chain's rule.'
\[
\Evaluate{\derivop t \of{\matgg\of{at} \matgg'\of{bt}}}{t=0}
=a\TimeDerivative\matgg\of 0 + b\TimeDerivative\matgg'\of 0\text.
\]

You may ask, ``why is the commutator, $\commutator{\TimeDerivative\matgg\of 0}{\TimeDerivative\matgg'\of 0} = {\TimeDerivative\matgg\of 0} {\TimeDerivative\matgg'\of 0} - {\TimeDerivative\matgg'\of 0}{\TimeDerivative\matgg\of 0}$, the Lie bracket?''. How do we get a minus in the derivative? \emph{The poet:}---The guy behind me whispers `quotient'.---``It's a girl.''---Oh! I'm sorry. \emph{He turns around to see who was whispering.} Evidently.---``You're a real poet. You had me worried for a moment. Poets are supposed to know that stuff.'' So, the derivative of a quotient is \[
\derivop x f\of x ^{-1} = (-1) \frac{f'\of x}{f\of x^2}\text. \quad
\parbox{.5\textwidth}{`There's no analogy, just differentiate the damn function!'}
\]
The \emph{group} commutator of $\matgg\of t$ and $\matgg'\of t$ is $\matgg\of t\matgg'\of t\matgg\of t^{-1}\matgg'\of t^{-1}$. If we differentiate it, we get\[
\Evaluate{\derivop t \matgg\of t\matgg'\of t\matgg\of t^{-1}\matgg'\of t^{-1}}{t=0}=\TimeDerivative\matgg\of 0 + \TimeDerivative\matgg'\of 0 - \TimeDerivative\matgg\of 0 -\TimeDerivative\matgg'\of 0 = 0\text,
\]
so that doesn't work. We have to differentiate twice. The reason is that we're just taking the first term in the expansion of $\log\of{\matgg\of t\matgg'\of t\matgg\of t^{-1}\matgg'\of t^{-1}}$, and we want more terms.
\begin{align*}
\log\of{\E^{s\matA}\E^{s\matB}}&=\log\of{\Identity+\pa{\E^{s\matA}\E^{s\matB}-\Identity}}
\intertext{Now we plug this into the Taylor series of the logarithm, which is---\emph{after several attempts at getting the signs right}---}
\log\of{1+x}&=-\sum{k=1}[\infty]\frac{\pa{-x}^n}{n}.
\intertext{The moment you memorise you're dead, unless you're an actor like him, or you're counting cards in Vegas. But if you count cards and they catch you, you're not in good shape either. We get, for $\matA$, $\matB$ in the Lie algebra,}
\log\of{\E^\matA\E^\matB}&=\matA+\matB+\frac{1}{2}\commutator\matA\matB+\dotsb\\
\log\of{\E^\matA\E^\matB\E^{-\matA}\E^{-\matB}}&=\commutator\matA\matB+\dotsb
\end{align*}

Here's the question, ladies and gentlemen.
\begin{proposition}
$\FunctionSpec{\exp}{\LAsutwo}{\SpecialUnitaryGroup\of 2}$ is\begin{itemize}
\item[(\textsc{a})]\st{injective;}
\item[(\textsc{b})]surjective;
\item[(\textsc{c})]\st{bijective;}
\item[(\textsc{d})]\st{all of the above.}
\end{itemize}
What do you pick? What can you hit with the exponential map? Quiet man:---$\NonZero\C$. So it's surjective. It can't be bijective, so it's (\textsc{b}). 
\begin{proof}{}[...]
``Where do the eigenvalues of an unitary operator lie? Please, after a whole year''---On the unit circle.---``Thank God.''
\end{proof}
\end{proposition}

Next time we'll do something more fun. We'll talk about spin, and we'll talk about quarks.

\section{Spin and flavour}
\NewLecture[date=2013-04-30, official=true]
This will involve a lot of calculations, you're not going to like it. I'll tell you why you'll love it.

---``Common, you keep mixing real and complex. If $\matX$ is skew-Hermitian, $\adj\matX=-\matX$. What is $\adj{\pa{\I\matX}}$?'' \emph{The professor whistles ``la Vie en rose''}.\[\adj{\pa{\I\matX}}=\I\matX\]

Now these groups should appeal to physicists. $\R^3$ is both \emph{what} and \emph{what}? It's both homogeneous, the laws of physics are the same in Boston and in Zürich, and isotropic. What does this have to do with groups? What it looks like at the origin is what it looks like everywhere: multiplication by $\matg$ is an invertible smooth map.

The first thing you do with $\SpecialUnitaryGroup\of 2$ is we look at the electron. Let's just have an electron sitting there in the Universe. We turn everything else off. How many degrees of freedom do we have? Two, I've yelled at you last semester about this, trying to get \emph{some} response.
\begin{align*}
\begin{pmatrix}
\gy_\uparrow\\
\gy_\downarrow
\end{pmatrix}
&\in\C^2
\intertext{---``Is that how you write it?''---Everybody has their own notation.---``Okay, so I'll use my own.''}
\begin{pmatrix}
a\\
b
\end{pmatrix}
&\in\C^2\text,
\intertext{where the up and down states are respectively}
\begin{pmatrix}
1\\0
\end{pmatrix}\text{ and }\begin{pmatrix}
0\\1
\end{pmatrix} \text.\span\omit
\end{align*}
If you rotate the spin space---by applying an element of $\SpecialUnitaryGroup\of 2$ to it---it doesn't change anything. It's a law of physics. ---``Have you ever heard of spin one in physics?''---We only talked about spin one half, although we mentioned that some exotic particles had other spins.---``Has anyone heard of Helium 3?''---It's a liquid.---``It depends on the temperature. You may be a poet, but the temperature has to come into the equation. Do you know about Percy Bysshe Shelley? Has anyone heard of \emph{Frankenstein}? It was written by Mary Wollstonecraft Shelley. This Mary Shelley had a husband, Percy Bysshe Shelley, who was a poet, and who wrote about the Moon:
\renewcommand{\poemtoc}{paragraph}
\poemtitle{The Moon}
\settowidth{\versewidth}{And feeble wanderings of her fading brain}
\begin{verse}[\versewidth]
{\large\textinitial{A}}\textsc{nd}, like a dying lady lean and pale,\\
Who totters forth, wrapp'd in a gauzy veil,\\
Out of her chamber, led by the insane\\
And feeble wanderings of her fading brain,\\
The moon arose up in the murky east\\
A white and shapeless mass.
 
Art thou pale for weariness \\
Of climbing heaven and gazing on the earth,\\
Wandering companionless\\
Among the stars that have a different birth,\\
And ever changing, like a joyless eye\\
That finds no object worth its constancy?
\begin{flushright}
---Percy Bysshe Shelley.
\end{flushright}
\end{verse}
So you can write poetry about modern science---`the stars that have a different birth' was modern science at the time---but you've got to have temperature as part of the equation.''---I'm not here for poetry.---``You may not be here for that, but maybe that's what you are, like this basketball player\footnote{This is a reference to \textsc{nba} player Jason Collins, who recently revealed he is gay. ``I didn't set out to be the first openly gay athlete playing in a major American team sport. But since I am, I'm happy to start the conversation.''}.''

Spin one has three dimensions,\[
\begin{pmatrix}
a\\ b\\ c
\end{pmatrix}\in\C^3\text.
\] The fact of the matter is, what you want is an irreducible representation $\gr_n$ of $\SpecialUnitaryGroup\of 2$ of dimension $n\geq 2$. Now there are two things we need. We need that there \emph{is} an irreducible representation, and we need to combine them. In economics, they have equilibria. The problem is, they have too many, so they don't know which one they're talking about. And that's the problem with string theory. The trouble with string theory is, it's like economics: they have too many ground states.

---``You go to that guy, who's half Swiss and half German and who likes to calculate''---Well, only sometimes.---``You're only half Swiss and half German part of the time? So we have some sort of quantum European here.'' 

Has anybody seen \emph{the four hundred blows}\footnote{\emph{Les quatre cents coups}, 1959, directed by François Truffaut.}? Well they dress with scarfs like him---it's not a happy film, by the way.

Now if we only have one quark in the Universe---we turn everything else off---it has inside it \emph{three} degrees of freedom, \emph{up}, \emph{down} and \emph{strange}.
\[
\begin{pmatrix}
u\\ d\\ s
\end{pmatrix}\in \C^3 \text{, the flavour space.}
\]
\emph{The poet:}---Why up, down and strange? Why not give them greek names, so it looks important and fancy?---``Gell-Mann came up with these names. He was---is he still alive?''---He is still alive.---``he is an obstreperous guy. Do you know what obstreperous means? If you don't, look it up.'' 

Heisenberg---\emph{he}'s dead, like Franco---considered a part of that, and he called it \emph{isospin}. It described the symmetry between up and down, and so it used $\SpecialUnitaryGroup\of 2$. Indeed, if you have $\matU\in\SpecialUnitaryGroup\of 2$, then\[
\tuple{\begin{array}{c|c}
\matU & \begin{array}{c}
0\\0
\end{array} \\
\hline
 \begin{array}{cc}0&0\end{array} & 1
\end{array}}
\] is in $\SpecialUnitaryGroup\of 3$.

\newcommand{\three}{\mathbf 3}
\newcommand{\threebar}{\overline{\mathbf 3}}
\newcommand{\eight}{\mathbf 8}
\newcommand{\one}{\mathbf 1}

The law of physics is that the antiquark lives in the dual flavour space $\Dual{{\C^3}}$. The flavour space is a representation $\three$ of $\SpecialUnitaryGroup\of 3$, and the dual flavour space is the dual representation $\threebar$. So if we have a quark-antiquark pair---a meson---, it lies in the tensor product $\C^3\Tensor\Dual{{\C^3}}$, or $\three\Tensor\threebar$. Now the dimension of this representation is $3\times3=9$, and it decomposes into the trivial---one-dimensional---representation $\one$, and another---eight-dimensional---irreducible representation, which we call $\eight$, that is, $\three\Tensor\threebar=\one\DirectSum\eight$. Gell-Mann came up with this, and he called it the eightfold way. I hope I have convinced you we need to look at this stuff. This is not some bourgeois pedantic mathematician stuff. When I do it next Friday you cannot complain. You can leave the room, but you cannot complain.

\NewLecture[date=2013-05-02]
Alright, here we are. I'm going to talk to you about $\LAsutwo$ and $\SpecialUnitaryGroup\of 2$ at a rate at which you can absorb the information---I know that's a slow rate. I'm going to give you a lecture today, just using the blackboard and talking.
\begin{align*}
\SpecialUnitaryGroup\of 2 &\DefineAs \setst{\matg\in\GeneralLinearGroup\of{2,\C}}{\matg\adj\matg = 1, \Determinant \matg = 1}\\
\LAsutwo\of 2 &\DefineAs \setst{\matx\in\LAgl\of{2,\C}}{\adj\matx+\matx=0, \Trace \matx = 0}
\end{align*}
We have a little lemma.
\begin{lemma}
\[
\frac{\I}{\sqrt 2} \PauliMatrix 1 = \frac{\I}{\sqrt2}\begin{pmatrix}
0&1\\
1&0
\end{pmatrix}\text,
\frac{\I}{\sqrt 2} \PauliMatrix 2 = \frac{\I}{\sqrt2}\begin{pmatrix}
0&-\I\\
\I&0
\end{pmatrix}\text,
\frac{\I}{\sqrt 2} \PauliMatrix 3 = \frac{\I}{\sqrt2}\begin{pmatrix}
1&0\\
0&-1
\end{pmatrix}
\]  is an orthonormal basis for $\tuple{\LAsutwo, \FrobeniusInner\placeholder\placeholder}$, where $\FrobeniusInner\matA\matB\DefineAs\Trace\of{\matA\adj\matB}$.
\begin{proof}
Calculate\begin{align*}
\FrobeniusInner{\frac{\I\PauliMatrix 1}{\sqrt2}}{\frac{\I\PauliMatrix 1}{\sqrt2}}
&= \Trace\of{\frac{\I}{\sqrt2}\PauliMatrix 1\frac{-\I}{\sqrt2}\adj{\PauliMatrix 1}}\\
&=\frac{1}{2}\Trace\PauliMatrix 1^2 = \frac{1}{2} \Trace\begin{pmatrix}
1&0\\
0&1
\end{pmatrix} = 1\text,
\end{align*}
etc.
\end{proof}
\end{lemma}
So you have an orthonormal basis. It's nice to have an orthonormal basis. 
\begin{definition}
$\AdjointRep\of\matg \matx \DefineAs \matg\matx\matg^{-1}$, for $\matg\in\SpecialUnitaryGroup\of 2$ and $\matx\in\LAsutwo$.
Two by two matrix, two by two matrix, two by two matrix. It's well defined.
\end{definition}
\begin{proposition}
$\FunctionSpec\AdjointRep{\SpecialUnitaryGroup\of 2}{\UnitaryGroup\of{\LAsutwo}}\DefineAs\UnitaryGroup\of{\tuple{\LAsutwo,\FrobeniusInner\placeholder\placeholder}}$ is an irreducible representation of $\SpecialOrthogonalGroup\of2$.
\begin{proof}
Let us prove $\AdjointRep\of\matg\in \UnitaryGroup\of{\LAsutwo}$:
\begin{align*}
\begin{split}
\FrobeniusInner{\AdjointRep\of\matg\matx}{\AdjointRep\of\matg\maty}
&= \Trace\of{\pa{\AdjointRep\of\matg\matx}\adj{\pa{\AdjointRep\of\matg\maty}}}\\
&= \Trace\of{\matg\matx\matg^{-1}\adj{\pa{\matg\maty\matg^{-1}}}}\\
&= \Trace\of{\matg\matx\matg^{-1}\adj{\pa{\matg^{-1}}}\adj\maty\adj\matg}\\
&= \Trace\of{\matg\matx\matg^{-1}\matg\adj\maty\matg^{-1}}\\
&= \Trace\of{\matx\adj\maty} = \FrobeniusInner\matx\maty\text.
\end{split}\quad\parbox{.4\textwidth}{$\matg$ and $\matg^{-1}$ kill each other off---no, let's use a more poetic term. They \emph{merge} into the identity. Then we bring the $\matg^{-1}$ to the front, and again, they \emph{merge} into the identity.}
\end{align*}
I hope you can handle all that merging.
There's something I didn't do! I need to show $\AdjointRep\of\matg\matx\in\LAsutwo$. ---``I have to \emph{show} that it somehow \emph{metamorphoses} into $\LAsutwo$.''---Maybe \emph{mirrors} would be a good metaphor.---``No. I don't like mirrors.''
\begin{align*}
\adj{\pa{\AdjointRep\of\matg\matx}}&=\adj{\pa{\matg\matx\matg^{-1}}}\\
&=\adj{\pa{\matg^{-1}}}\adj\matx\adj\matg\\
&=\matg\adj\matx\matg^{-1}=-\matg\matx\matg^{-1}\text.
\end{align*}

The trace is \foreign{\textgerman{ewig entlang Konjugationsklassen}}---it's \emph{much} easier to be a bad poet than a bad mathematician. This is a basic fact, as our algebraic conscience would tell us. It's as basic as ``you don't eat soup with a fork''. Isn't it? \emph{Claude nods}. It's good to have an algebraic conscience, because I don't have one.

Now we prove that it's a representation,
$\AdjointRep\of\matg\AdjointRep\of\matrh\matx
= \AdjointRep\of{\matg\matrh}$ for all $\matg$, $\matrh\in\SpecialUnitaryGroup\of 2$ and $\matx\in\LAsutwo$.
\begin{align*}
\AdjointRep\of\matg\pa{\AdjointRep\of\matrh\matx}&=\matg\pa{\matrh\matx\matrh^{-1}}\matg^{-1}\\
&=\pa{\matg\matrh}\matx\pa{\matg\matrh}^{-1} = \AdjointRep\of{\matg\matrh}\matx\text,\\
\AdjointRep\of\Identity\matx&=\Identity\text.
\end{align*}
\end{proof}
\end{proposition}
\begin{proposition}
$\tuple{\LAsutwo,\FrobeniusInner\placeholder\placeholder}\Isomorphic\tuple{\R^3,\scal{\:}{\:}}$. The isomorphism is given by
\begin{multline*}
\FunctionBody
{\R^3\owns\vgx}
{
\gx_1\frac{\I}{\sqrt2}
\begin{pmatrix}
1&0\\
0&-1
\end{pmatrix}
+\gx_3\frac{\I}{\sqrt2}
\begin{pmatrix}
0&-\I\\
\I&0
\end{pmatrix}
+\gx_2\frac{\I}{\sqrt2}
\begin{pmatrix}
0&1\\
1&0
\end{pmatrix}}\\
=\frac{\I}{\sqrt2}
\begin{pmatrix}
\gx_1&\gx_2-\I\gx_3\\
\gx_2+\I\gx_3&-\gx_1
\end{pmatrix}
\text.
\end{multline*}
\begin{proof}
\emph{The poet:}---We already encountered this formula last year.---``That's right, but I certainly didn't expect you to remember it.'' \emph{the professor holds on to a table and breathes deeply.} ``Sorry. I was beginning to hyperventilate.  I certainly didn't expect you to remember this.''---Isn't this also the measurement operator of a Stern-Gerlach box?---``A what? [...] Yes, that's right.''
\[
\FrobeniusInner{\frac{\I}{\sqrt2}\scal\vgx\PauliTensor}{\frac{\I}{\sqrt2}\scal\vgl\PauliTensor}
=\gx_1\gl_1+\gx_2\gl_2+\gx_3\gl_3\text.
\]
\end{proof}
\end{proposition}
\[
\tuplespec{\AdjointRep\of\matg}{kl} = \FrobeniusInner{\AdjointRep\of\matg\frac{\I}{\sqrt2}\PauliMatrix k}{\frac{\I}{\sqrt2}\PauliMatrix l}
\]
\emph{The poet:}---I'm not sure about the order of the $k$ and $l$ I dictated you.---``Let's just check! Here's how you do linear algebra.''
\[
\AdjointRep\of\matg\matx = \AdjointRep\of\matg\sum{k=1}[3]\FrobeniusInner\matx{\ReducedPauli k}\ReducedPauli k\text.
\]
I don't want to write $\frac\I{\sqrt2}$ all the time, so I defined
\[
\ReducedPauli k \DefineAs \frac{\I}{\sqrt2}\PauliMatrix k\text.
\]
---``Algebraic conscience, do you agree? I'm just projecting onto an orthogonal basis.'' \emph{Claude looks confused} ``I'll just give him a few seconds to give us the go-ahead.'' \emph{Claude points out a mistake.} ``It's good to have an algebraic conscience. I don't have an algebraic conscience---I don't have a conscience at all, but that's another matter.''
\begin{align*}
\AdjointRep\of\matg\matx &= \sum{k=1}[3]\FrobeniusInner\matx{\ReducedPauli k}\AdjointRep\of\matg\ReducedPauli k\text.
\intertext{Let us look at $\AdjointRep\of\matg\ReducedPauli k$.}
\AdjointRep\of\matg\ReducedPauli k &= \sum{k=1}[3]\FrobeniusInner{\AdjointRep\of\matg\ReducedPauli k}{\ReducedPauli l}\text.
\intertext{Substituting,}
\AdjointRep\of\matg\matx&=\sum{k,l=1}[3]\FrobeniusInner{\AdjointRep\matg\ReducedPauli k}{\ReducedPauli l}\FrobeniusInner{\matx}{\ReducedPauli k} \ReducedPauli l\text.
\end{align*}
Algebraic conscience, do you agree? You see, I'm doing this to show that you don't have to memorise this. If you memorise, you always make a mistake. We have proven:
\begin{align*}
\AdjointRep\of\matg & = \tuple{\FrobeniusInner{\AdjointRep\of\matg\ReducedPauli l}{\ReducedPauli k}}\text.
\intertext{Moreover,}
\AdjointRep\of\matg\adj{\AdjointRep\of\matg} &= \Identity_{\LAsutwo}\text.\\
\FrobeniusInner{\AdjointRep\of\matg\ReducedPauli l}{\ReducedPauli k} &= \FrobeniusInner{\ReducedPauli k}{\AdjointRep\of\matg\ReducedPauli l}\\
&=\FrobeniusInner{\adj{\pa{\AdjointRep\of\matg}}\ReducedPauli k}{\ReducedPauli l}\\
&= \FrobeniusInner{\AdjointRep\of\matg^{-1}\ReducedPauli k}{\ReducedPauli l}\\
&= \FrobeniusInner{\AdjointRep\of{\matg^{-1}}\ReducedPauli k}{\ReducedPauli l}\text{...}
\end{align*}
---``I want to show it's real. It should be real, right?'' \emph{Claude:}---The vector space $\LAsutwo$ is a real vector space. $\AdjointRep\of\matg$ is a transformation of it.---``We have to take a break, right?''

So what I want to show is that $\AdjointRep\in\SpecialOrthogonalGroup\of{\tuple{\LAsutwo,\FrobeniusInner\placeholder\placeholder},3}$, where $\tuple{\LAsutwo,\FrobeniusInner\placeholder\placeholder}$ is a real inner product space.
$\FrobeniusInner \matx\maty = \Trace\of{\matx\adj\maty}=-\Trace\of{\matx\maty}$ is real linear in both variables.
$\AdjointRep\of\matg\matx\DefineAs\matg\matx\matg^{-1}$ is a real linear transformation of the real inner product space $\tuple{\LAsutwo,\FrobeniusInner\placeholder\placeholder}$.

\emph{Claude:}---We checked that it's an orthogonal matrix. So we \emph{know} that.---``Yes. We checked $\FrobeniusInner{\AdjointRep\of\matg\matx}{\AdjointRep\of\matg\maty}=\FrobeniusInner\matx\maty$ for $\matg\in\SpecialUnitaryGroup\of 2$, $\matx$, $\maty\in\LAsutwo$, in other words, $\AdjointRep\of\matg\in\OrthogonalGroup\of{\tuple{\LAsutwo,\FrobeniusInner\placeholder\placeholder}}$. We checked that in complete detail.'' 

The poet is right, it's irreducible because it has no invariant subspace. We now know---hey, how do you like that, `we now know', all you have to do is to add a `k' in front of the `n'--- that $\FunctionBody{\SpecialUnitaryGroup\of 2 \owns\matg}{\AdjointRep\of\matg\in\OrthogonalGroup\of{\tuple{\LAsutwo, \FrobeniusInner\placeholder\placeholder}}}$.

%TODO:QRCODE
We still have to prove that it's in $\SpecialOrthogonalGroup\of{\tuple{\LAsutwo,\FrobeniusInner\placeholder\placeholder}}$. We already know that it's orthogonal, so we only have to prove that its determinant is $1$. As it's orthogonal, its determinant can only be $1$ or $-1$. Now the determinant is a continuous function. Can a continuous function take both the values $1$ and $-1$, and nothing in between?---No.---``Well, that's not quite true. The domain has to have a property...''---Connected.---``Exactly. Is $\SpecialUnitaryGroup\of 3$ connected? In the notes from last semester...'' \emph{The poet:}---They are not available online.---``Yes they are\footnote{The lecture notes from last semester are still available online at \url{http://www.math.ethz.ch/~gruppe5/group5/lectures/mmp/hs12/Files/}.}. In \emph{Finite Dimensional Quantum Mechanics}, on page eight, there is a section called ``Unitary Groups'', and there, you have the fact that it's connected. Namely, you have---in an exercise---the fact that $\SpecialUnitaryGroup\of 2$ is homeomorphic to the $3$-sphere $\UnitSphere 3$. As physicists, you don't quite know what the $3$-sphere is, but you know what the $2$-sphere looks like: it's round, and it's connected.''

We have proven the following.
\begin{proposition}
The map $\FunctionSpec\AdjointRep{\SpecialUnitaryGroup\of{2}}{\SpecialOrthogonalGroup\of{\tuple{\LAsutwo,\FrobeniusInner\placeholder\placeholder}}\Isomorphic\SpecialOrthogonalGroup\of 3}$ is a surjective homomorphism, and\begin{align*}
\matg\in\AdjointRep^{-1}\of\matR &\Implies -\matg\in\AdjointRep^{-1}\of\matR\text, \\
\forall\matR\in\SpecialOrthogonalGroup\of 3\quad \Cardinality{\AdjointRep^{-1}\of\matR}&=2\text,
\end{align*}
that is, it is a double covering.
\begin{proof}
We have done the fact that the determinant is one.
How do we prove it's surjective? When we have proven that, we want to know how many people go to each rotation, \foreign{\textgerman{wieviele blätter diese Überlagerung hat}}. So we have to go door-to-door and ask. How the hell are we going to prove that? Or how are we bloody well going to prove that?---you see, in the thirties, `bloody well' was really vulgar. Now if you say that, you just sound like a bad British actor. Look at the maps\begin{align*}
\FunctionBody*{\mata\in\LAsutwo}{\E^\mata\in\SpecialUnitaryGroup\of 2}\text,\\
\FunctionBody*{\mata\in\LAsutwo}{\AdjointRep\of{\E^\mata}\in\SpecialOrthogonalGroup\of 3}\text.
\end{align*}
---``How do you characterise a rotation?''---You have to have an axis and angle.---``Ex\-act\-ly. It can be an axis of evil, but you need an axis and an angle. Now this will amuse you. What is the definition of $\AdjointRep\of{\E^\mata}\mata$?'' \emph{The poet:}---$\E^\mata\mata\pa{\E^\mata}^{-1}=\mata\E^\mata\E^{-\mata}=\mata$---``Does this commute? The poet tells me it commutes. Is there an anti-poet?''---I have the reason why it commutes.---``You are the reason why it commutes?''---No, I \emph{have} the reason.

$\mata$ is the axis of the special orthogonal linear transformation $\AdjointRep\of{\E^\mata}$. 
$\AdjointRep\of{\E^{\gf\mata}}$ has axis $\mata$. As $\gf$ changes, the angle has to change.
\marginfig[$\AdjointRep\of{\E^{\gf\mata}}$ is a rotation around $\mata$ of angle $\ga$. What is $\ga$ as a function of $\gf$?]{\FigureRotationAdjointRep}%
Pick a $\matb\in\LAsutwo$ perpendicular to $\mata$ such that $\norm\matb = 1$. We have
\begin{align*}
\FrobeniusInner\mata\matb&=0\text,\\
\FrobeniusInner\matb\matb&=1\text,\\
\FrobeniusInner\matb{\AdjointRep\of{\E^{\gf\mata}}\matb}&=\cos\ga\text.
\end{align*}
---``What is $\ga$ as a function of $\gf$?'' \emph{The poet:}---Traditionally, $\gf$ is the angle, so the angle must be $\gf$.---``You're making the \emph{Candide} argument. Pangloss said ``the reason we have noses is to keep up our eyeglasses''\footnote{\textfrench{<<~Il est démontré, disait-il, que les choses ne peuvent être autrement: car tout étant fait pour une fin, tout est nécessairement pour la meilleure fin. Remarquez bien que les nez ont été faits pour porter des lunettes; aussi avons-nous des lunettes.~[...]~>>}---Voltaire, \emph{Candide, ou l’Optimisme}.}. But speaking \emph{culturally} $\ga$ could be $\gf$ or $2\gf$. It's a little proposition that it's just $\gf$.'' \emph{Claude:}---I think it's $2\gf$.---``I think this is an excellent exercise.\footnote{The angle is actually $\gf\sqrt 2$, as was stated in exercise sheet 10.}'' So there is a beautiful map from the beautiful poetic space $\SpecialUnitaryGroup\of 2$ onto the space $\SpecialOrthogonalGroup\of 3$ where we, as sinners, all live.
\end{proof}
\end{proposition}

\newcommand{\LAso}{\LieAlgebraSymbol{so}}%
\newcommand{\vbracket}[2]{\lsquareCommaRsquare{#1}{#2}_{\R^3}}%
\NewLecture[date=2013-05-07]
I learned yesterday that there was no lecture on Thursday, so I'll try to do something that fits in the hour.
Let \[\matX\in\LAso\of3\DefineAs\setst{\matX\in\LAgl\of{3,\R}}{\Transpose\matX+\matX=0}\text.\]
We have $x_{ji}=-x_{ij}$, and thus $x_{ii}=0$, so
\begin{align*}
\Transpose{\pa{\E^\matX}}&=\E^{\Transpose\matX}=\E^{-\matX}=\pa{\E^\matX}^{-1}\text,\\
\Determinant\of{\E^\matX}&=\E^{\Trace\matX}=\E^0=1\text{, so we have proven}\\
\FunctionBody*{\LAso\of3\owns\matX}{\E^\matX\in\SpecialOrthogonalGroup\of3}\text.
\end{align*}
You were probably told that this is an infinitesimal rotation---\foreign{\textgerman{das ist eine infinitesimale Drehung}}---but we are going to prove it. For $\vx$, $\vy\in\R^3$, define
\begin{align*}
\vbracket\vx\vy&\DefineAs
\begin{pmatrix}
x_2y_3-x_3y_2\\
x_3y_1-x_1y_3\\
x_1y_2-x_2y_1
\end{pmatrix}\text,\\
{\matL_\vx}&\DefineAs\begin{pmatrix}
0&-x_3&x_2\\
x_3&0&-x_1\\
-x_2&x_1&0
\end{pmatrix}\text.
\intertext{We have}
\Transpose{{\matL_\vx}}&=-{\matL_\vx}\text.
\end{align*}
\begin{lemma}
$\FunctionBody{\R^3\owns\vx}{{\matL_\vx}\in\LAso\of3}$ is an isomorphism of Lie algebras, $\smash{\matL_{\vbracket\vx\vy}=\commutator{{\matL_\vx}}{\matL_\vy}}$, and you can easily check that it's surjective and blah blah blah blah.
\begin{proof}You just have to prove that the kernel is empty. You had something about that last year, when you were younger and things were still partially commutative. ---``They did that last year, didn't they?'' \emph{Claude:}---Yes, yes.---``Should we do that again?''---No, no!---``I hope that this rings a bell, or at least a---what's it called, the thing from Proust, \emph{À la recherche du temps perdu}''---A \foreign{\textfrench{madeleine}}.---``Yes, exactly, a \foreign{\textfrench{madeleine}}. That's in \emph{Du côté de chez Swann}.\footnote{``\textfrench{Et dès que j’eus reconnu le goût du morceau de madeleine trempé dans le tilleul que me donnait ma tante (quoique je ne susse pas encore et dusse remettre à bien plus tard de découvrir pourquoi ce souvenir me rendait si heureux), aussitôt la vieille maison grise sur la rue, où était sa chambre, vint comme un décor de théâtre s’appliquer au petit pavillon donnant sur le jardin, qu’on avait construit pour mes parents sur ses derrières (ce pan tronqué que seul j’avais revu jusque-là) ; et avec la maison, la ville, la Place où on m’envoyait avant déjeuner, les rues où j’allais faire des courses depuis le matin jusqu’au soir et par tous les temps, les chemins qu’on prenait si le temps était beau.}''---Marcel Proust, \emph{À la recherche du temps perdu}.}'' 
\end{proof}
\end{lemma}
---``Do you agree? You look like you don't agree.''---It's morning.---``It depends on your job. I see you're not a nocturnal animal. If you want, you can come back at two, I'll be there. I don't think [Claude] will come though. Will you?'' \emph{Claude:}---Nooo...
\begin{align*}
\matL_{\vx}\vy&=\vbracket\vx\vy\\
\matL_{\vx}\vx&=0\\
\E^{{\matL_\vx}}&=\sum{l\geq0}\frac{{\matL_\vx}^l}{\Factorial l}\\
\E^{{\matL_\vx}}\vx&=\sum{l\geq 0}\frac{{\matL_\vx}^l}{\Factorial l}\vx=\Identity\vx=\vx\text{, that is, $\E^{{\matL_\vx}}$ is a rotation around $\vx$.}\span\omit\span\omit\\
{\matL_\vx}^2&=\begin{pmatrix}
-\pa{{x_3}^2+{x_2}^2} & x_1x_2 & x_1x_3\\
x_1x_2 & -\pa{{x_1}^2+{x_3}^2} & x_2x_3\\
x_1x_3 & -x_2x_3 & -\pa{{x_1}^2 + {x_2}^2}\\
\end{pmatrix}\span\omit\span\omit\\
{\matL_\vx}^3&=-\norm\vx^2{\matL_\vx}
\intertext{and by induction}
{\matL_\vx}^{2m+1} &= \pa{-\norm\vx^2}^m{\matL_\vx} &&\text{for $m\geq 0$,}\\
{\matL_\vx}^{2m} &= \pa{-\norm\vx^2}^{m-1}{\matL_\vx}^2 &&\text{for $m\geq 1$.}
\end{align*}
So, we sum.
\begin{align*}
\E^{{\matL_\vx}} &= \Identity 
+ \sum{m\geq 1} \frac{1}{\Factorial{\pa{2m}}} {\matL_\vx}^{2m} 
+ \sum{m\geq 0} \frac{1}{\Factorial{\pa{2m+1}}}{\matL_\vx}^{2m+1}\\
&=\Identity 
+ \sum{m\geq 1}\frac{\pa{-\norm\vx^2}^{m-1}}{\Factorial{\pa{2m}}} {\matL_\vx}^2 
+ \sum{m\geq 0}\frac{\pa{-\norm\vx^2}^{m}}{\Factorial{\pa{2m+1}}} {\matL_\vx}
\end{align*}
What is the Taylor series of the sine? You think it's not worth knowing that stuff, but it is. \emph{Somebody says $\smash{x - \frac{x^3}{\Factorial 3} + \frac{x^5}{\Factorial 5} - \dotsb}$.} Just give me the series!
\begin{align*}
\sin x &= x - \frac{x^3}{\Factorial 3} +  \frac{x^5}{\Factorial 5} - \dotsb\\
&= \sum{m=0}[\infty] \pa{-1}^m \frac{x^{2m+1}}{\Factorial{\pa{2m+1}}}
\end{align*} 
So we get\[
\E^{{\matL_\vx}} = \Identity - \frac{1}{\norm\vx^2}\sum{m\geq1}\frac{\pa{-\norm\vx^2}^m}{\Factorial{\pa{2m}}}{\matL_\vx}^2+\frac{1}{\norm\vx}{\matL_\vx}\sin\norm\vx\text.
\]
Now what is the Taylor series of the cosine? You don't remember the series, but you seem to be pretty good at remembering the first terms. \emph{The professor whistles ``La Vie en rose'' while writing the series.}
\begin{align*}
\cos x &= 1 - \frac{x^2}{\Factorial 2} + \dotsb\\
&= \sum{m=0}[\infty] \pa{-1}^m \frac{x^{2m}}{\Factorial{2m}}
\end{align*}
We get\[
\E^{{\matL_\vx}} = \Identity + \frac{1-\cos \norm\vx}{\norm\vx^2}{\matL_\vx}^2+\frac{\sin \norm\vx}{\norm\vx}{\matL_\vx}\text.\]
So we have this very nice formula---and we don't need Euler angles. The calculation is so simple that going to the North pole doesn't bring you anything.
\marginfig[$\E^{{\matL_\vx}}\in\SpecialOrthogonalGroup\of3$ is a rotation around $\vx$. What is the angle $\ga$?]{\FigureRotationRThree}
We have $\smash{\scal\vx{\Transpose{\tuple{x_2,-x_1,0}}}}$, and $\smash{
\scal{\pascal{\E^{{\matL_\vx}}\tuple{x_2,-x_1,0}}}{\tuple{x_2,-x_1,0}}=\norm{\tuple{x_2,-x_1,0}}^2\cos\ga}$.
Once we have the angle $\ga$, we have the axis and the angle, so we have a rotation.
Why didn't I take the vector product with an arbitrary vector? I want to actually calculate with this. We will need the following vectors:
\begin{align*}
{\matL_\vx}\begin{pmatrix}
x_2\\-x_1\\0
\end{pmatrix} &= \begin{pmatrix}
x_3x_1\\ x_2x_3\\ -{x_1}^2-{x_2}^2
\end{pmatrix}\text,\\
{\matL_\vx}^2\begin{pmatrix}
x_2\\-x_1\\0
\end{pmatrix} &= \begin{pmatrix}
-x_2{x_3}^2 - x_2\pa{{x_1}^2+{x_2}^2}\\
{x_3}^2x_1+x_1\pa{{x_1}^2+{x_2}^2}\\
-x_1x_2x_3+x_1x_2x_3
\end{pmatrix}\text,\\
\E^{{\matL_\vx}}\begin{pmatrix}
x_2\\-x_1\\0
\end{pmatrix} &=\begin{pmatrix}
x_2\\-x_1\\0
\end{pmatrix} + \frac{\sin\norm\vx}{\norm\vx}\begin{pmatrix}
x_3x_1\\ x_2x_3\\ -{x_1}^2-{x_2}^2
\end{pmatrix} + \frac{1-\cos\norm\vx}{\norm\vx^2}
\begin{pmatrix}
-x_2\norm\vx^2\\
x_1\norm\vx^2\\
0
\end{pmatrix}\text.
\end{align*}
\emph{The poet:}---Couldn't you just give us the result? It's a $3\times3$ matrix, we can do it ourselves, if we want.---``If you \emph{want}. But you won't do it. So once in your life, do some calculations. I know it's painful. No pain, no gain.''
\begin{align*}
\scal{
\pascal{\E^{{\matL_\vx}}\begin{pmatrix}
x_2\\-x_1\\0
\end{pmatrix}}
}{\begin{pmatrix}
x_2\\-x_1\\0
\end{pmatrix}} &= {x_1}^2 +{x_2}^2 + \frac{1-\cos\norm\vx}{\norm\vx^2}\pa{-{x_2}^2\norm\vx^2-{x_1}^2\norm\vx^2}\\
&={x_1}^2+{x_2}^2 + \pa{1-\cos\norm\vx}\pa{-\pa{{x_1}^2+{x_2}^2}}\\
&=\pa{{x_1}^2+{x_2}^2}\cos\norm\vx
\end{align*}
---``You see, the point is I didn't prepare for this calculation.''---Evidently.---``We now know that $\norm\vx$ is the angle, goodbye, farewell, have a nice holiday.''

\NewLecture[date=2013-05-14]
Let \[\begin{pmatrix}
\ga & \gb\\
-\conj\gb & \conj\ga
\end{pmatrix}\in\SpecialUnitaryGroup\of 2\text.
\]
What is the inverse of this matrix? Can somebody invert a $2\times 2$ matrix? \emph{Somebody laboriously answers $\smash{\tuple{\begin{smallmatrix}
\conj\ga &\conj\gb\\
-\gb & \ga
\end{smallmatrix}}}$.} How did you get that? You used minors. That works, but there is an easier way to do that. What is the definition of $\SpecialUnitaryGroup\of2$? $\adj\matU=\matU^{-1}$, so you just need to dagger the matrix, \[\begin{matrix}
\begin{pmatrix}
\ga & \gb\\
-\conj\gb & \conj\ga
\end{pmatrix}
=\begin{pmatrix}
\conj\ga & -\gb\\
\conj\gb & \ga
\end{pmatrix}
\end{matrix}\text,
\]
and the inverse you gave me was wrong. If you end up working in the aeronautic industry, or anywhere where you could do harm---actually I should encourage you to do string theory. There it doesn't matter.

We could take representations of $\LAsutwo$ and sort of exponentiate them, and get representations of $\SpecialUnitaryGroup\of 2$, but we are going to construct them by hand. Define
\[
V_n\DefineAs\setst{p\of{z_1,z_2}\DefineAs\sum{j=0}[n]c_j{z_1}^{n-j}{z_2}^j}{c_j\in\Z, j=0,\dotsc,n}\text.
\]
---What is the dimension of this vector space? Can \emph{somebody} find a basis of this space? \emph{The poet:}---``The monomials.''---And how many monomials are there?---``$n$.''---I understand that when $n$ is sufficiently large, the difference between $n$ and $n+1$ looks as small as a daffodil seen from the space station.
\DeclareDocumentCommand{\irrep}{m O{}}{\mathop{{}\gr_{#1}\ifstrempty{#2}{}{\of{#2}}}}
\renewcommand{\poemtoc}{paragraph}
%\poemtitle{I Wandered Lonely as a Cloud}
\settowidth{\versewidth}{That floats on high o'er vales and hills,}
\begin{verse}[\versewidth]
{\large\textinitial{I}} wandered lonely as a cloud\\
That floats on high o'er vales and hills,\\
When all at once I saw a crowd,\\
A host of golden daffodils;\\
Beside the lake, beneath the trees,\\
Fluttering and dancing in the breeze.

[...]
\begin{flushright}
---William Wordsworth.
\end{flushright}
\end{verse}
But it's not $n$.---``Is it $n+1$?''---Yes. $\Dimension[\C]V_n=n+1$. Now we define a representation of $\SpecialUnitaryGroup\of 2$ onto $V_n$,
\begin{align*}
\pa{\irrep{n+1}[\matg] p}\of{z1,z2}&\DefineAs p\of{\matg^{-1}\begin{pmatrix}
z_1\\z_2
\end{pmatrix}}\\
&=p\of{\conj\ga z_1 -\gb z_2, \conj\gb z_1 + \ga z_2}\text.
\end{align*}
We check that $\irrep{n+1}[\matg] p\in V_n$ for all $\matg\in\SpecialUnitaryGroup\of 2$, $p\in V_n$,
\begin{align*}
\pa{\irrep{n+1}[\matg] p}\of{z_1,z_2} &= \sum{j=1}[n]c_j\pa{\conj\ga z_1 -\gb z_2}^{n-j}\pa{\conj\gb z_1 + \ga z_2}^j\\
&=\sum{j=0}[n]\sum{k=0}[n-j]\sum{l=0}[j]\pa{c_j\binom{n-j}{k}\binom{j}{l}\conj\ga^{n-j-k}\gb^k\conj\gb^{j-l}\ga^l}{z_1}^{n-\pa{k+l}}{z_2}^{k+l}\in V_n\text.
\end{align*}
\begin{theorem}
$\irrep{n+1}$ is an irrrep.---How many `r's? It's \textsc{irr}educible \textsc{rep}resentation, so three. Should it be ``irrep.''? ``Irrep.'' sounds like \emph{Irma Vep}.\footnote{\emph{Irma Vep}, 1996, directed by Olivier Assayas, a film about a French film director attempting to remake Louis Feuillade's 1915/1916 serial film \emph{Les Vampires}.}---of $\SpecialUnitaryGroup\of 2$ on $V_n$ for all $n\geq 1$.
\begin{proof}
Define
\begin{align*}
\matR_t&\DefineAs \begin{pmatrix}
\E^{\I t} & 0\\
0 & \E^{-\I t}
\end{pmatrix}\in\SpecialUnitaryGroup\of 2\text,\\
\cz_{n,j}&\DefineAs {z_1}^{n-j}{z_2}^j\text{ for }j=0,\dotsc,n\text.
\end{align*}
We have
\begin{align*}
\irrep{n+1}[\matR_t]\cz_{n,j}
&=\pa{\conj\ga z_1 - \gb z_2}^{n-j}\pa{\conj\gb z_1 + \ga z_2}^j\\
&=\pa{\E^{-\I t} z_1}^{n-j}\pa{\E^{\I t} z_2}^j\\
&=\E^{\I jt}\E^{-\I\pa{n-j}t}\cz_{n,j}\text.
\end{align*}
Let
\begin{align*}
\matT_t&\DefineAs\begin{pmatrix}
\cos t & \sin t\\
-\sin t & \cos t
\end{pmatrix}
\in \SpecialUnitaryGroup\of 2\text.
\intertext{Then}
\irrep{n+1}[\matT_t]\cz_{n,j}&=\sum{k=0}[n-j]\sum{l=0}[j]\binom{n-j}{k}\binom{j}{l}\pa{\cos t}^{n-j-k+l}\pa{-1}^k\pa{\sin t}^{j+k-l}{z_1}^{j-k-l}{z_2}^{k+l}\text.
\end{align*}
We check this,% TODO: Spacing (or reorder)
\begin{align*}
&\pa{z_1 \cos t  - z_2\sin t }^{n-j}\pa{z_1 \sin t  + z_2 \cos t}^j\span\omit\span\omit
\\ &&&= \pa{\sum{k=0}[n-j]\binom{n-j}{k}\pa{\cos t}^{n-j-k}{z_1}^{n-j-k}\pa{-\sin t}^k{z_2}^k}
\\ &&&\phantom{=}\Multiply
\pa{\sum{l=0}[j]\binom{j}{l}\pa{\sin t}^{j-l}{z_1}^{j-l}\pa{\cos t}^l{z_2}^l}\text.
\end{align*}
Now somebody who is familiar with this knows that I have taken the Cartan, the maximal toral blah blah.

---How much time is left?---``One minute.''---One minute?---``Yes.''
\paragraph*{Strategy} Suppose that for some $\opA\in\Endomorphisms\of{V_n}$, $\irrep{n+1}[\matg]\opA = \opA\irrep{n+1}[\matg]$ for all $\matg\in\SpecialUnitaryGroup\of 2$. Show $\opA=\gl\Identity$.

\NewLecture[date=2013-05-16]
Last time we had computed $\irrep{n+1}[\matT_t]\cz_{n,j}$, but we will only need \[
\irrep{n+1}[\matT_t]\cz_{n,n}=\pa{-\conj\gb z_1 + \ga z_2}^n = \sum{k=0}[n] \binom{n}{k} \conj\gb^{n-k}\ga^k {z_1}^{n-k} {z_2}^k\text.\]
If $\opA$ commutes with $\irrep{n+1}[\matg]$,
\begin{align*}
\irrep{n+1}[\matR_t]\pa{\opA\cz_{n,j}}
&= \opA\irrep{n+1}[\matR_t]\cz_{n,j}\\
&= \opA\E^{-\I\pa{n-2j}t}\cz_{n,j}\\
&= \E^{-\I\pa{n-2j}t}\opA\cz_{n,j}\text.
\end{align*}
\emph{To Claude}:---Okay? Oh, you just got here.---``Er... yes''.

How do you interpret this? Should I speed up? You don't care anyway.
There are only a couple of weeks of school, and you are still here just by inertia.
\foreign{\textgerman{Das heißt}}, $\opA\cz_{n,j}$ is an eigenvector of $\irrep{n+1}[\matR_t]$ with eigenvalue $\E^{-\I\pa{n-2j}t}$.

---Can somebody find a $t$ such that $\E^{-\I\pa{n-2j}t}=\E^{-\I\pa{n-2k}t}\Equivalent 0\leq j=k\leq n$, \foreign{\textlatin{id est}}, \foreign{\textgerman{das heißt}}, $\E^{\I\pa{n-2j}t}$ for $j=0,\dotsc, n$ are all different?---``It has to be a multiple of $\Pi$.''---I'm hyperventilating! You're running in the wrong direction, \foreign{\textgerman{das ist nicht guet!}}
Let's pick $t$ irrational, for instance $t = \sqrt{2}$, \begin{alignat*}{4}
 \E^{-\I\pa{n-2j}t} &= \E^{-\I\pa{n-2k}t}
 &\Equivalent && \pa{n-2j}t &= \pa{n-2k}t + 2m\Pi\\
&& \Equivalent && t &= \frac{2m\Pi}{2\pa{k-j}}\text.
\end{alignat*} I need something more. I need that you can't multiply $\Pi$ by a rational number and get $\sqrt 2$. \emph{Claude:}---``If you stick with the square root of two.''---Yes. Let's pick $t=\sqrt 2 \Pi$. Then we can stop because you all know about that poor Greek guy who talked too much, so they took him in a row boat on the lake of Zürich and drowned him. This was before the Greeks had an economic crisis.
$\irrep{n+1}[\matR_{\sqrt2 \Pi}]$ is diagonalisable, its eigenvectors are the $\opA\cz_{n,j}$ and its  eigenvalues are the $\E^{-\I\pa{n-2j}\sqrt2\Pi}$, and are distinct.
Recall that
\[
\irrep{n+1}[\matR_t]\cz_{n,j}=\E^{-\I\pa{n-2j}t}\cz_{n,j}\text,
\]
and thus the $\opA\cz{n,j}$ and the $\cz_{n,j}$ are eigenvectors of $\smash{\irrep{n+1}[\matR_{\Pi\sqrt{2}}]}$ with the distinct eigenvalues $\smash{\E^{-\I\pa{n-2j}\Pi\sqrt{2}}}$. As there are $n+1$ distinct eigenvalues and the space is $n+1$-dimensional, the eigenspaces have to be one-dimensional. Do you believe that? Can you look at me in the eyes and say, ``if there are $n+1$ distinct eigenvalues and the space is $n+1$-dimensional, then the eigenspaces have to be one-dimensional''? It's like Seinfeld: ``It's not a lie if you believe it''.\footnote{\emph{Seinfeld}, season 6, episode 16 ``The Beard'', directed by Andy Ackerman.}

So if $\smash{\irrep{n+1}[\matR_{\sqrt 2\Pi}]}$ commutes with $\opA$, then $\opA$ is very special: the basis of the vector space diagonalises it.
We have \[
\exists\tuple{\gm_0,\dotsc,\gm_n}\in\C^{n+1}\quad\opA\cz_{n,j}=\gm_j\cz_{n,j}
\text.\]
Now we do the same with $\matT_t$. Using Binomial's theorem,
\begin{align*}
\opA\irrep{n+1}[\matT_t]\cz_{n,n}
&= \opA\sum{j=0}[n]\binom{n}{j}\pa{\sin t}^n-j\pa{\cos t}^j\cz_{n,j}\\
&= \sum{j=0}[n]\binom{n}{j}\pa{\sin t}^n-j\pa{\cos t}^j\opA\cz_{n,j}\\
&= \sum{j=0}[n]\binom{n}{j}\pa{\sin t}^n-j\pa{\cos t}^j\gm_j\cz_{n,j}\text.
\intertext{As $\opA$ commutes with $\irrep{n+1}[\matT_t]$, this is equal to}
\irrep{n+1}[\matT_t]\opA\cz_{n,n} &= \gm_n\irrep{n+1}[\matT_t]\cz_{n,n}\\
&=\sum{j=0}[n]\binom{n}{j}\pa{\sin t}^n-j\pa{\cos t}^j\gm_n\cz_{n,j}
\end{align*}
Now we use logic: if $a=b$ and $b=c$, then we conclude that $a=c$. Namely,
\[
\sum{j=0}[n]\binom{n}{j}\pa{\sin t}^n-j\pa{\cos t}^j\gm_j\cz_{n,j}
=
\sum{j=0}[n]\binom{n}{j}\pa{\sin t}^n-j\pa{\cos t}^j\gm_n\cz_{n,j}\text.
\]
Now go back to your first year of linear algebra. Go back to those happy times of your childhood. \emph{The poet:}---``The coefficients have to be equal.''
\[
\binom{n}{j}\pa{\sin t}^n-j\pa{\cos t}^j\gm_j = \binom{n}{j}\pa{\sin t}^n-j\pa{\cos t}^j\gm_n
\]
Pick $t$ such that $\pa{\sin t}\pa{\cos t}\neq 0$. It follows $\gm_j=\gm_n$, so $\opA\cz_{n,j}=\gm_n\gz_{n,j}$, i.e., $\opA=\gm_n\Identity$. How do you use this to prove that $\irrep{n+1}$ is irreducible? Here is the painful part. You have climbed the mountain, and you are looking at the valley where raspberries grow---I like raspberries---and watermelons. How do you prove it is irreducible? I bet someone that nobody would answer this question. It looks like I'm going to make some money. \emph{Claude:}---``How much money?''---You don't count. 
%TODO: Where does this come from? [It could, or it does?---``It does.''---It's the only psychologically sound answer.]
MISSING TEXT MISSING TEXT MISSING TEXT
Suppose $\opgP$ is an orthogonal projection from $V_n$ onto $W$ \emph{Claude:}---``It has to be unitary.''---That's why you don't count.

When my son was born, I took  all the algebra books out of my house. You don't want to take chances.
\begin{theorem}
$\irrep{n+1}$ is an irreducible representation of $\SpecialUnitaryGroup\of2$ on $V_n$ for all $n\geq 0$ and $\gc_n\of{\matR_t}\DefineAs\Trace\irrep{n+1}[\matR_t]=\frac{\sin\of{\pa{n+1}t}}{\sin t}$.
\begin{proof}
What are the conjugacy classes of $\SpecialUnitaryGroup\of 2$?
$\ConjugacyClass\of{\matg}=\setst{\matrh\matg\matrh^{-1}}{\matrh\in\SpecialUnitaryGroup\of 2}$.
The set \[
\setst{\begin{pmatrix}
\E^{\I t} & 0 \\ 0 & \E^{-\I t}
\end{pmatrix}
}{0\leq t\leq \Pi}
\]
contains a representative from each class.

---Did we call it a class function?---``In the solutions.''
[...]
After an entire year of me making fun of you, one of you got it.
\NewLecture*[date=2013-05-21]
We have shown that $\irrep{n+1}$ defines a representation of $\SpecialUnitaryGroup\of 2$ on $V_n$ such that if $\opA\in\Endomorphisms\of{V_n}$ and $\irrep{n+1}[\matg]\opA=\opA\irrep{n+1}[\matg]$ for all $\matg\in\SpecialUnitaryGroup\of 2$, then $\opA = c\Identity$.
\begin{claim}There is a Hermitian inner product $\InvariantInnerVn\placeholder\placeholder$ on $V_n$ such that
\[\InvariantInnerVn{\irrep{n+1}[\matg] v}{\irrep{n+1}[\matg] w} = \InvariantInnerVn v w\]
for all $v$, $w\in V_n$ and $\matg\in\SpecialUnitaryGroup\of 2$, \foreign{\textgerman{das heißt}}, $\irrep{n+1}$ is a unitary representation of $\SpecialUnitaryGroup\of 2$ on $\tuple{V_n, \InvariantInnerVn\placeholder\placeholder}$.
\begin{proof}
Consider the finite case. Let $\FunctionSpec\matgr G {\GeneralLinearGroup\of V}$ be a homomorphism from the finite group $G$ to the finite-dimensional vector space $V$. Let $\vv_1,\dotsc, \vv_n\in V$ be a basis of $V$. For $\vw\in V$ with $\vw = \sum{j=1}[n] c_j \vv_j$, let $c_j\of\vw\DefineAs c_j$. Define a Hermitian inner product by $\InnerVn{\vv_i}{\vv_j}\DefineAs\KroneckerDelta i j$. We have \[\InnerVn \vv\vw = \sum{j=1}[n]\conj{c_j}\of\vv c_j\of\vw\text.\]
Now define $\InvariantInnerVn\vv\vw\DefineAs\sum{g\in G}\InnerVn{\matgr\of g \vv}{\matgr\of g \vw}$. Then for all $g \in G$, $\vv$, $\vw\in V$,\[
\InvariantInnerVn{\matgr\of g \vv}{\matgr\of g \vw}=\InvariantInnerVn\vv\vw
\text.\]

Let us do the same for $\SpecialUnitaryGroup\of 2$. Pick a Hermitian inner product $\InnerVn\placeholder\placeholder$ on $V_n$, for instance, for
\begin{align*}
p &= \sum{j=0}[n] p_j {z_1}^{n-j} {z_2}^j\text,\\
q &= \sum{j=0}[n] q_j {z_1}^{n-j} {z_2}^j\text,
\intertext{let}
\InnerVn p q &\DefineAs \sum{j=0}[n]\conj{p_j}q_j\text.
\end{align*}
---What do you do now?
\emph{The poet:}---``In physics talk, instead of summing, you integrate over the group, but I don't know if that makes sense.''---Never admit that. You're a physicist. Your stomach tells you that you have to integrate,
\[\InvariantInnerVn p q \DefineAs \int{\SpecialUnitaryGroup\of 2} \InnerVn {\irrep{n+1}[\matg] p} {\irrep{n+1}[\matg] q} \diffd \matg\text.\]
And that's exactly what a physicist should write. What you used here is that
\[
\int{\SpecialUnitaryGroup\of 2}f\of{\matg\matrh} \diffd \matg = \int{\SpecialUnitaryGroup\of 2}f\of\mata \diffd \mata\text.
\]
I'm sorry it's boring, but what the hell.
This is good enough for a physicist, but a mathematician needs more than their stomach to integrate. I'm going to do it for a mathematician who doesn't mind a formula. $\SpecialUnitaryGroup\of2$ is the three-sphere $\UnitSphere 3$. Let us consider an $\vx$ in $\R^4$.
\[
\R^4\owns\vx=r\begin{pmatrix}
\cos \gf_1                     \\
\sin \gf_1 \cos \gf_2          \\
\sin \gf_1 \sin \gf_2 \cos \gq \\
\sin \gf_1 \sin \gf_2 \sin \gq
\end{pmatrix}\text,
\]
where $r=\sqrt{{x_1}^2+{x_2}^2+{x_3}^2+{x_4}^2}$, $\gf_1$, $\gf_2 \in \intclos 0 \Pi$ and $\gq\in\intclop 0 {2\Pi}$.
We have
\[
\abs{\Determinant\diffd \vx} = r^3 \sin^2 \gf_1 \sin \gf_2 \diffd \gf_1 \diffd \gf_2 \diffd \gq 
\text.
\]
The map
\begin{align*}
\MapSpecBody
{\intclos 0 \Pi \Cartesian \intclos 0 \Pi \Cartesian \intclop 0 {2\Pi}}
{\SpecialUnitaryGroup\of 2}
{\tuple{\gf_1,\gf_2,\gq}}
{
  \begin{pmatrix}
    \cos \gf_1 + \I\sin \gf_1 \cos \gf_2 & \sin \gf_1\sin \gf_2 \E^{\I\gq} \\
    -\sin \gf_1 \sin \gf_2 \E^{-\I\gq} & \cos \gf_1 - \I\sin \gf_1 \cos \gf_2
  \end{pmatrix}
}
\end{align*}
is surjective. We define
\[
\int{\SpecialUnitaryGroup\of 2} f\of \matg \diffd\matg \DefineAs \int{0}[2\Pi]\int{0}[\Pi]\int{0}[\Pi]f\of{\gf_1,\gf_2,\gq} \sin^2\gf_1 \sin\gf_2 \diffd\gf_1\diffd\gf_2\diffd\gq\text.
\]
\end{proof}
\end{claim}
We now have a Hermitian inner product, so we have proved it's irreducible. Now what I've got to do is calculate the characters.
\end{proof}
\end{theorem}
\end{proof}
\end{theorem}

\NewLecture[date=2013-05-23]
We proved the following proposition last time.
\begin{proposition}
$\irrep n$ is an irreducible representation of $\SpecialUnitaryGroup\of 2$ on $V_{n-1}$, that is, we have found an irreducible representation of each dimension $n\geq 1$.
\end{proposition}
Are there any others? We have found one, but there could be 17 irreducible representations of dimension eleven! But we live in the best possible of all worlds, as Voltaire told us, and there are no others.

I think my algebraic conscience is not here today... No, he's nowhere in sight. So today, if I act algebraically unethically, I apologise, because I have no algebraic conscience.
Define
\begin{align*}
\Character[n]\of\matg &\DefineAs \Trace \irrep{n}[\matg]\text.
\intertext{We have}
\Character[n]\of{\matrh\matg\matrh^{-1}} &\DefineAs \Character[n]\of\matg \text.
\intertext{Recall}
\matR_t&\DefineAs \begin{pmatrix}
\E^{\I t} & 0 \\
0         & \E^{-\I t}
\end{pmatrix}\text.
\end{align*}
---For $\matg\in\SpecialUnitaryGroup\of 2$, is there an $\matrh\in\SpecialUnitaryGroup\of 2$ and a $t \in \intclop 0 \Pi$ such that $\matrh\matg\matrh^{-1}=\matR_t$? \emph{A~mathematician:}---``Yes. Special unitary matrices are diagonalisable by special unitary matrices, and as the determinant is $1$, the diagonal matrix has to have this form.''

\emph{Enter Claude.}---Oh hi!
$\Character[n]\of{\matR_t}$ is a function of $t\in\intclos 0 \Pi$, the conjugacy classes of $\SpecialUnitaryGroup\of 2$.
\begin{align*}
\Character[n]\of{\matR_t}&=\Trace\irrep{n}\of{\matR_t}\text,\\
\irrep{n}\of{\matR_t}\cz_{n-1,j}&=\E^{-\I t\pa{n-2j}}\cz_{n-1,j}\text{ for }j=0, \dotsc, n-1\text,
\end{align*}
where $\cz_{n,j}\DefineAs {z_1}^{n-j}{z_2}^j$ is a basis for $V_n$. It follows
\begin{align*}
\Character[n]\of{\matR_t}&=\Trace\irrep{n}\of{\matR_t}=\sum{j=0}[n-1]\E^{-\I t\pa{n-1-2j}}\\
&=\E^{-\I\pa{n-1}t}\sum{j=0}[n]\E^{2\I t j}&&\parbox{.4\textwidth}{In this course there are only three things you can do.}\\
&=\E^{-\I\pa{n-1}t}\frac{1-\E^{2\I t n}}{1-\E^{2\I t}}&&\parbox{.4\textwidth}{That's not beautiful enough.}\\
&=\frac{\E^{-\I\pa{n-1}t}}{\E^{\I t}}\frac{1-\E^{2\I t n}}{\E^{\I t}-\E^{\I t}}\\
&=\E^{-\I n t}\frac{1-\E^{2\I t n}}{\E^{\I t}-\E^{\I t}}=\frac{\sin nt}{\sin t}\text.
\end{align*}
So we have proven a nice little proposition.
\begin{proposition}
$\Character[n]\of{\matR_t}=\frac{\sin nt}{\sin t}$, for $n\geq 1$.
\end{proposition}
Let us check that this is consistent with what we know. $\matR_0=\Identity$, so\[
\Character[n]\of\Identity=\lim_{\conv t 0} \frac{\sin nt}{\sin t} = n\text,\]
which is the dimension of the representation.

I'm going to write down some formulae from last time.
\begin{align*}
\vx &= r \begin{pmatrix}
\cos \gf_1\\
\sin \gf_1 \cos \gf_2\\
\sin \gf_1 \sin \gf_2 \cos \gq\\
\sin \gf_1 \sin \gf_2 \sin \gq
\end{pmatrix}\text{ for some }\gf_1,\gf_2\in{\intclos 0 \Pi}, \gq\in\intclop{0}{2\Pi}\\
\abs{\Determinant \diffd \vx} &= r^3\sin^2\gf_1\sin\gf_2\diffd\gf_1\diffd\gf_2\diffd\gq\text.
\end{align*}
The elements of $\SpecialUnitaryGroup\of 2$ are the
\[
\begin{pmatrix}
\ga&\gb\\
-\conj\gb&\conj\ga
\end{pmatrix}\text,
\]
---What do I need $\ga$ and $\gb$ to verify? You can answer in German--``-\foreign{\textgerman{$\ga$ Betrag im Quadrat plus $\gb$ Betrag im Quadrat}} equals one.''---That's English. `\foreign{\textgerman{ist gleich Eins}}'. %TODO: Check capitalisation.
Define
\begin{align*}
\ga&\DefineAs \cos\gf_1 + \I \sin\gf_1 \cos \gf_2\\
\gb&\DefineAs \sin\gf_1 \sin\gf_2 \cos\gq
+\I \sin\gf_1\sin\gf_2\sin\gq\\
&=\sin\gf_1\sin\gf_2\E^{\I\gq}\text.
\end{align*}
We have $\abs{\ga}^2+\abs{\gb}^2=1$, so\[
\matg\of{\gf_1,\gf_2,\gq}\DefineAs\begin{pmatrix}
\cos\gf_1+\I \sin\gf_1\cos\gf_2 & \sin\gf_1\sin\gf_2\E^{\I\gq}\\
-\sin\gf_1\sin\gf_2\E^{-\I\gq}  & \cos\gf_1-\I\sin\gf_1\cos\gf_2
\end{pmatrix}\in\SpecialUnitaryGroup\of 2\text.
\]
This is all of $\SpecialUnitaryGroup\of 2$.
Now define
\begin{align*}
\int{\SpecialUnitaryGroup\of 2}f\of\matg\diffd\matg
&\DefineAs\int{\UnitSphere 3}f\of{\matg\of{\gf_1,\gf_2,\gq}}\diffd\vx\\
&=\int{0}[\Pi]\int{0}[\Pi]\int{0}[2\Pi]f\of{\matg\of{\gf_1,\gf_2,\gq}}\sin^2\gf_1\sin\gf_2\diffd\gf_1\diffd\gf_2\diffd\gq\text.
\end{align*}
---Can somebody do this integral for $f=\Character[k]\conj{\Character[l]}$?---``Probably a Kronecker delta.''---You're a physicist.---``No.''---You're actually a mathematician? You've been influenced by some physicists then, because for a mathematician at \textsc{ethz} to say `probably' without a whole theory of integration takes some serious behaviour modification. But can one of the mathematicians here do it, or do we have to ask a physicist? Now it comes to a question of pride. Consider a class function $f$. Then $f\of{\matg\of{\gf_1,\gf_2,\gq}}=f\of{\gf_1}$, as $\matR_t=\matg\of{t,0,0}$.
\begin{align*}
\int{\SpecialUnitaryGroup\of 2}f\of\matg\diffd\matg
&=\int{0}[\Pi]\int{0}[\Pi]\int{0}[2\Pi]f\of{\gf_1}\sin^2\gf_1\sin\gf_2\diffd\gf_1\diffd\gf_2\diffd\gq\\
&=\underbrace{\int{0}[\Pi]\sin\gf_2\diffd\gf_2 \int{0}[2\Pi]\diffd\gq}_{C} \int{0}[\Pi]f\of{\gf_1}\sin^2\gf_1\diffd\gf_1\text.
\end{align*}
We don't really care about the constant $C$. \emph{Claude}:---``It's $4\Pi$.''---For $f=\Character[k]\conj{\Character[l]}$,
\begin{align*}
\int{\SpecialUnitaryGroup\of 2}\Character[k]\of\matg\conj{\Character[l]\of\matg}\diffd\matg&=C\int{0}[\Pi]\frac{\sin k\gf}{\sin \gf}\frac{\sin l \gf}{\sin \gf}\sin^2 \gf \diffd \gf\\
&=C\int{0}[\Pi]\sin k\gf \sin l\gf \diffd\gf\\
&=C\int{0}[\Pi]\cos\of{k-l}\gf\diffd\gf - C\int{0}[\Pi]\cos\pa{k+l}\gf\diffd\gf\text,
\end{align*}
where $k$, $l\geq1$. So we have a proposition.
\begin{proposition}
$\displaystyle\int{\SpecialUnitaryGroup\of 2}\Character[k]\of\matg\conj{\Character[l]\of\matg}\diffd\matg=C\KroneckerDelta k l$.
\end{proposition}
%Suppose EQ12 %This was unfinished, will typeset after Tuesday so it looks seamless
\endgroup%QM

\appendix
\newpage
\section*{Colophon}
This document is available online at \url{http://goo.gl/m6YNz} \marginnote{\includegraphics[width=.5\marginparwidth]{QR-MMP-II}}(\textsc{qr}-code beside).

It was typeset using the \XeTeX\ engine by Jonathan Kew and Khaled Hosny, version 0.9998, published under the MIT
%Should MIT be small caps?
License, and written in the \XeLaTeX\ markup language.

The serif text is typeset in the Linux Libertine font family, version 5.3, and the sans serif text in the \textsf{Linux Biolinum} font family, version 5.3, both published by the Libertine Open Fonts Project under the GNU
%Should GNU be small caps?
General Public License with the font exception and under the Open Font License. The mathematical expressions are typeset in the $\operatorname{Cambria~Math}$ font family, version 5.93, designed under the direction of Jelle Bosma (Monotype Imaging Incorporated) and Ross Mills (Tiro Typeworks), and licensed by Microsoft Corporation.

Version control was done using the Git source code management system by Linus Torvalds \foreign{\textlatin{et~alii}}. The document is hosted on the GitHub hosting service provided by GitHub Incorporated.

\subsection*{Bug reporting}
The reader is kindly invited to report any falsehoods, typos, unclarities, etc. For the sake of everybody's sanity, please follow the following guidelines when reporting an issue:
\begin{itemize}
\item check whether the issue still is present in the latest version of the document, and whether it has already been reported on the ``issues'' page of the repository \url{http://goo.gl/XNQRk} \marginnote{\includegraphics[width=.5\marginparwidth]{QR-MMP-II-issues-page}}[-0.3\marginparwidth](\textsc{qr}-code beside);
\item if you have a GitHub account, file an issue at the above page and properly label it as
\begin{itemize}
\item a \emph{bug} if it is a typo, falsehood or any other issue which is unambiguously an error;
\item an \emph{enhancement} if it is a request for clarification, a missing part of a lecture or any other addition.
\end{itemize}
Please file separate issues for unrelated issues found in the document, and include a concise yet descriptive title;
\item if you do not have a GitHub account---and are not willing to create one---, send the report to \hyperref{mailto:egg.robin.leroy+MMPII@gmail.com}{}{}{\nolinkurl{egg.robin.leroy+MMPII@gmail.com}} \marginnote{\includegraphics[width=.5\marginparwidth]{QR-MMP-II-issues-email}}[-0.3\marginparwidth](see \textsc{qr}-code beside). Note that bugs reported by email will take longer to process than those reported via GitHub;
\item the report should include:
\begin{itemize}
\item the relevant page(s) and lecture number(s);
\item the build date and time found at the bottom of this page;
\item a description of the erroneous version, with enough context to find the issue;
\item a clear description of how the issue should be fixed.
\end{itemize}
\end{itemize}
An example of a bug report could be the following:
\begin{quote}
\textsf{%
\textbf{Typo: ``them macho wait'' p.~19}\\
Page 19, lecture 14, build date 2013-04-18T08:47:21+02:00, ``remember, them macho wait as long as possible'' should read ``remember, the macho waits as long as possible''.
}
\end{quote}
\vfill
\begin{flushright}
Build date: \texttt{\today T\currenttime+02:00}.
\end{flushright}

\end{document}